{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EleonoraBaim/NPS_Dialogue_system/blob/main/bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF7HrVEX-GBS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "import sys\n",
        "import tqdm\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/Colab_Notebooks/BERT/\"\n",
        "\n",
        "dataset_folder = \"ready_datasets\"\n",
        "tokenizer_model_path = \"BERT_models/rubert-base-cased-conversational\"\n",
        "classification_model_path = \"BERT_models/rubert-base-cased-conversational2\"\n",
        "ckpt_path = str(project_path + \"model_parts/curr_ckpt.pt\")\n",
        "best_model_path = str(project_path + \"model_parts/best_model.pt\")\n",
        "labels_path = str(project_path + \"model_parts/categories.json\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO0I7_Bjg9ql",
        "outputId": "369e6ed7-fc97-4aa4-fd20-638904882631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fymNh2_DhF-A",
        "outputId": "a2a5bcfb-101c-49b0-f80d-0289e4062b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec 11 04:35:10 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    72W / 149W |   4184MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPqPyaOmhK4j",
        "outputId": "9f15185e-8f2e-47a4-c89b-9f155243053f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 39.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 511 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Importing"
      ],
      "metadata": {
        "id": "aZ8qcL4QQtS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqIJdRPdg3he",
        "outputId": "a1c3c1e2-1230-41b1-d1b0-86714ea6a485"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data... \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#IMPORTING DATA\n",
        "print('Importing data...', '\\n')\n",
        "\n",
        "train_df = pd.read_csv(str(project_path + dataset_folder + '/'+'train_dataset.csv'))\n",
        "train_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
        "\n",
        "test_df = pd.read_csv(str(project_path + dataset_folder + '/'+ 'test_dataset.csv'))\n",
        "test_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
        "\n",
        "val_df = pd.read_csv(str(project_path + dataset_folder + '/'+'val_dataset.csv'))\n",
        "val_df.drop(labels = 'Unnamed: 0', axis = 1, inplace = True)\n",
        "\n",
        "text = 'CONTEXT'\n",
        "target_list = train_df.columns[(train_df.columns!='CONTEXT')&(train_df.columns!='normalized')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVfJ32jk8Cdj"
      },
      "outputs": [],
      "source": [
        "# Mean length of sentence\n",
        "lens = []\n",
        "for t in train_df[text]:\n",
        "  lens.append(len(t))\n",
        "\n",
        "np.array(lens).mean()\n",
        "\n",
        "# hyperparameters\n",
        "MAX_LEN = 100\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "VALID_BATCH_SIZE = 32\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 1e-05\n",
        "#LEARNING_RATE = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "kgpZ6CYZQxYJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSoYRvIW_6iS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f19851-82ed-4040-c070-12ed9ad887e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer ... \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenization\n",
        "print( 'Tokenizer ...', '\\n')\n",
        "tokenizer = BertTokenizer.from_pretrained(project_path + tokenizer_model_path)\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.df = df\n",
        "        self.title = df['CONTEXT']\n",
        "        self.targets = self.df[target_list].values\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        title = str(self.title[index])\n",
        "        title = \" \".join(title.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            title,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_token_type_ids=True,\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
        "            'targets': torch.FloatTensor(self.targets[index])\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiVlzxLRAIzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c59342-2991-461b-f5af-66c4edfac215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Dataloaders ... \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Creating Dataloaders\n",
        "print('Creating Dataloaders ...', '\\n')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
        "valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN)\n",
        "test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN)\n",
        "\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "    batch_size=TRAIN_BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "    batch_size=VALID_BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model initialization"
      ],
      "metadata": {
        "id": "P4Rm6KTVQ7FG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzNsz_WgARbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296f62ba-ac0c-4527-f9cc-4b7b86ddbfea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model initialization...\n",
            "Using device: cuda \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model initialization\n",
        "print('Model initialization...')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('Using device:', device,'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6MimK3yAVsJ"
      },
      "outputs": [],
      "source": [
        "def load_ckp(checkpoint_fpath, model, optimizer):\n",
        "    \"\"\"\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    model: model that we want to load checkpoint parameters into       \n",
        "    optimizer: optimizer we defined in previous training\n",
        "    \"\"\"\n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath, map_location=torch.device('cuda'))\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    valid_loss_min = checkpoint['valid_loss_min']\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer\n",
        "    #checkpoint['epoch']\n",
        "    #, valid_loss_min.item()\n",
        "\n",
        "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
        "    \"\"\"\n",
        "    state: checkpoint we want to save\n",
        "    is_best: is this the best checkpoint; min validation loss\n",
        "    checkpoint_path: path to save checkpoint\n",
        "    best_model_path: path to save best model\n",
        "    \"\"\"\n",
        "    f_path = checkpoint_path\n",
        "    # save checkpoint data to the path given, checkpoint_path\n",
        "    torch.save(state, f_path)\n",
        "    # if it is a best model, min validation loss\n",
        "    if is_best:\n",
        "        best_fpath = best_model_path\n",
        "        # copy that checkpoint file to best path given, best_model_path\n",
        "        shutil.copyfile(f_path, best_fpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd8axxXDBLTg",
        "outputId": "60a72358-8124-4282-f294-f77d1fb5aac6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTClass(\n",
              "  (bert_model): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (linear): Linear(in_features=768, out_features=39, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained(str(project_path+classification_model_path), return_dict=True)\n",
        "        self.dropout = torch.nn.Dropout(0.3)\n",
        "        self.linear = torch.nn.Linear(768, len(target_list))\n",
        "    \n",
        "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids, \n",
        "            attention_mask=attn_mask, \n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        output_dropout = self.dropout(output.pooler_output)\n",
        "        output = self.linear(output_dropout)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzDfTHkGBMAy"
      },
      "outputs": [],
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "HFzUJFrfRCvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5RAg56kBOeJ"
      },
      "outputs": [],
      "source": [
        "val_targets=[]\n",
        "val_outputs=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhFoRKTzBWA3"
      },
      "outputs": [],
      "source": [
        "def train_model(n_epochs, training_loader, validation_loader, model, \n",
        "                optimizer, checkpoint_path, best_model_path):\n",
        "   \n",
        "  # initialize tracker for minimum validation loss\n",
        "  valid_loss_min = np.Inf\n",
        "   \n",
        " \n",
        "  for epoch in range(1, n_epochs+1):\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    print('############# Epoch {}: Training Start   #############'.format(epoch))\n",
        "    for batch_idx, data in enumerate(training_loader):\n",
        "        print('yyy epoch', batch_idx)\n",
        "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if batch_idx%5000==0:\n",
        "            print(f'Epoch: {epoch}, Training Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print('before loss data in training', loss.item(), train_loss)\n",
        "        train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.item() - train_loss))\n",
        "        print('after loss data in training', loss.item(), train_loss)\n",
        "    \n",
        "    print('############# Epoch {}: Training End     #############'.format(epoch))\n",
        "    \n",
        "    print('############# Epoch {}: Validation Start   #############'.format(epoch))\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        " \n",
        "    model.eval()\n",
        "   \n",
        "    with torch.no_grad():\n",
        "      for batch_idx, data in enumerate(validation_loader, 0):\n",
        "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            \n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            \n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.item() - valid_loss))\n",
        "            val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "\n",
        "      print('############# Epoch {}: Validation End     #############'.format(epoch))\n",
        "      # calculate average losses\n",
        "      print('before cal avg train loss', train_loss)\n",
        "      train_loss = train_loss/len(training_loader)\n",
        "      valid_loss = valid_loss/len(validation_loader)\n",
        "      # print training/validation statistics \n",
        "      print('Epoch: {} \\tAvgerage Training Loss: {:.6f} \\tAverage Validation Loss: {:.6f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "      \n",
        "      # create checkpoint variable and add important data\n",
        "      checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'valid_loss_min': valid_loss,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict()\n",
        "      }\n",
        "        \n",
        "        # save checkpoint\n",
        "      save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
        "        \n",
        "      ## TODO: save the model if validation loss has decreased\n",
        "      if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
        "        # save checkpoint as best model\n",
        "        save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
        "        valid_loss_min = valid_loss\n",
        "\n",
        "    print('############# Epoch {}  Done   #############\\n'.format(epoch))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка модели\n",
        "load_ckp(best_model_path, model, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0hqsyhNx7S5",
        "outputId": "72762d93-52ed-4599-ca21-5762b160a24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(BERTClass(\n",
              "   (bert_model): BertModel(\n",
              "     (embeddings): BertEmbeddings(\n",
              "       (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "       (position_embeddings): Embedding(512, 768)\n",
              "       (token_type_embeddings): Embedding(2, 768)\n",
              "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "       (dropout): Dropout(p=0.1, inplace=False)\n",
              "     )\n",
              "     (encoder): BertEncoder(\n",
              "       (layer): ModuleList(\n",
              "         (0): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (1): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (2): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (3): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (4): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (5): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (6): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (7): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (8): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (9): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (10): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "         (11): BertLayer(\n",
              "           (attention): BertAttention(\n",
              "             (self): BertSelfAttention(\n",
              "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "             (output): BertSelfOutput(\n",
              "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "               (dropout): Dropout(p=0.1, inplace=False)\n",
              "             )\n",
              "           )\n",
              "           (intermediate): BertIntermediate(\n",
              "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "           )\n",
              "           (output): BertOutput(\n",
              "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "             (dropout): Dropout(p=0.1, inplace=False)\n",
              "           )\n",
              "         )\n",
              "       )\n",
              "     )\n",
              "     (pooler): BertPooler(\n",
              "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "       (activation): Tanh()\n",
              "     )\n",
              "   )\n",
              "   (dropout): Dropout(p=0.3, inplace=False)\n",
              "   (linear): Linear(in_features=768, out_features=39, bias=True)\n",
              " ), Adam (\n",
              " Parameter Group 0\n",
              "     amsgrad: False\n",
              "     betas: (0.9, 0.999)\n",
              "     eps: 1e-08\n",
              "     lr: 1e-05\n",
              "     weight_decay: 0\n",
              " ))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckmmaUOSBYpX",
        "outputId": "7ba65f13-b293-4379-ac0d-e739fbef350a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mВыходные данные были обрезаны до нескольких последних строк (5000).\u001b[0m\n",
            "before loss data in training 0.07008767127990723 0.06982482578745455\n",
            "after loss data in training 0.07008767127990723 0.06982522045035613\n",
            "yyy epoch 666\n",
            "before loss data in training 0.06506309658288956 0.06982522045035613\n",
            "after loss data in training 0.06506309658288956 0.06981808083436293\n",
            "yyy epoch 667\n",
            "before loss data in training 0.06380659341812134 0.06981808083436293\n",
            "after loss data in training 0.06380659341812134 0.06980908160170389\n",
            "yyy epoch 668\n",
            "before loss data in training 0.06882757693529129 0.06980908160170389\n",
            "after loss data in training 0.06882757693529129 0.06980761447963152\n",
            "yyy epoch 669\n",
            "before loss data in training 0.07645534723997116 0.06980761447963152\n",
            "after loss data in training 0.07645534723997116 0.06981753646882606\n",
            "yyy epoch 670\n",
            "before loss data in training 0.07500267773866653 0.06981753646882606\n",
            "after loss data in training 0.07500267773866653 0.06982526395208961\n",
            "yyy epoch 671\n",
            "before loss data in training 0.08942871540784836 0.06982526395208961\n",
            "after loss data in training 0.08942871540784836 0.06985443575485116\n",
            "yyy epoch 672\n",
            "before loss data in training 0.060024190694093704 0.06985443575485116\n",
            "after loss data in training 0.060024190694093704 0.06983982915000606\n",
            "yyy epoch 673\n",
            "before loss data in training 0.06675077229738235 0.06983982915000606\n",
            "after loss data in training 0.06675077229738235 0.06983524597960157\n",
            "yyy epoch 674\n",
            "before loss data in training 0.061708223074674606 0.06983524597960157\n",
            "after loss data in training 0.061708223074674606 0.06982320594566835\n",
            "yyy epoch 675\n",
            "before loss data in training 0.07055272161960602 0.06982320594566835\n",
            "after loss data in training 0.07055272161960602 0.06982428511086648\n",
            "yyy epoch 676\n",
            "before loss data in training 0.06167973577976227 0.06982428511086648\n",
            "after loss data in training 0.06167973577976227 0.06981225475734934\n",
            "yyy epoch 677\n",
            "before loss data in training 0.0565924271941185 0.06981225475734934\n",
            "after loss data in training 0.0565924271941185 0.06979275648660711\n",
            "yyy epoch 678\n",
            "before loss data in training 0.06928945332765579 0.06979275648660711\n",
            "after loss data in training 0.06928945332765579 0.06979201524484135\n",
            "yyy epoch 679\n",
            "before loss data in training 0.07900618016719818 0.06979201524484135\n",
            "after loss data in training 0.07900618016719818 0.06980556548737424\n",
            "yyy epoch 680\n",
            "before loss data in training 0.05827612802386284 0.06980556548737424\n",
            "after loss data in training 0.05827612802386284 0.06978863532957172\n",
            "yyy epoch 681\n",
            "before loss data in training 0.07574141770601273 0.06978863532957172\n",
            "after loss data in training 0.07574141770601273 0.06979736374947852\n",
            "yyy epoch 682\n",
            "before loss data in training 0.06176930293440819 0.06979736374947852\n",
            "after loss data in training 0.06176930293440819 0.0697856096340831\n",
            "yyy epoch 683\n",
            "before loss data in training 0.06643331050872803 0.0697856096340831\n",
            "after loss data in training 0.06643331050872803 0.06978070861197001\n",
            "yyy epoch 684\n",
            "before loss data in training 0.06174385920166969 0.06978070861197001\n",
            "after loss data in training 0.06174385920166969 0.06976897598509366\n",
            "yyy epoch 685\n",
            "before loss data in training 0.05086881294846535 0.06976897598509366\n",
            "after loss data in training 0.05086881294846535 0.06974142472702277\n",
            "yyy epoch 686\n",
            "before loss data in training 0.06312539428472519 0.06974142472702277\n",
            "after loss data in training 0.06312539428472519 0.06973179440614606\n",
            "yyy epoch 687\n",
            "before loss data in training 0.04967011511325836 0.06973179440614606\n",
            "after loss data in training 0.04967011511325836 0.06970263498856918\n",
            "yyy epoch 688\n",
            "before loss data in training 0.04712863638997078 0.06970263498856918\n",
            "after loss data in training 0.04712863638997078 0.06966987156534915\n",
            "yyy epoch 689\n",
            "before loss data in training 0.07042573392391205 0.06966987156534915\n",
            "after loss data in training 0.07042573392391205 0.06967096701804272\n",
            "yyy epoch 690\n",
            "before loss data in training 0.07265691459178925 0.06967096701804272\n",
            "after loss data in training 0.07265691459178925 0.06967528821568925\n",
            "yyy epoch 691\n",
            "before loss data in training 0.07562839239835739 0.06967528821568925\n",
            "after loss data in training 0.07562839239835739 0.0696838909673983\n",
            "yyy epoch 692\n",
            "before loss data in training 0.06524663418531418 0.0696838909673983\n",
            "after loss data in training 0.06524663418531418 0.06967748799945879\n",
            "yyy epoch 693\n",
            "before loss data in training 0.05582606419920921 0.06967748799945879\n",
            "after loss data in training 0.05582606419920921 0.06965752917553912\n",
            "yyy epoch 694\n",
            "before loss data in training 0.0616757832467556 0.06965752917553912\n",
            "after loss data in training 0.0616757832467556 0.06964604464902288\n",
            "yyy epoch 695\n",
            "before loss data in training 0.05557897314429283 0.06964604464902288\n",
            "after loss data in training 0.05557897314429283 0.06962583333938965\n",
            "yyy epoch 696\n",
            "before loss data in training 0.06002383306622505 0.06962583333938965\n",
            "after loss data in training 0.06002383306622505 0.06961205715535355\n",
            "yyy epoch 697\n",
            "before loss data in training 0.07226262986660004 0.06961205715535355\n",
            "after loss data in training 0.07226262986660004 0.06961585453746136\n",
            "yyy epoch 698\n",
            "before loss data in training 0.08149231970310211 0.06961585453746136\n",
            "after loss data in training 0.08149231970310211 0.06963284518862822\n",
            "yyy epoch 699\n",
            "before loss data in training 0.07878747582435608 0.06963284518862822\n",
            "after loss data in training 0.07878747582435608 0.06964592323239355\n",
            "yyy epoch 700\n",
            "before loss data in training 0.05400804430246353 0.06964592323239355\n",
            "after loss data in training 0.05400804430246353 0.06962361527386297\n",
            "yyy epoch 701\n",
            "before loss data in training 0.06010677292943001 0.06962361527386297\n",
            "after loss data in training 0.06010677292943001 0.06961005851838657\n",
            "yyy epoch 702\n",
            "before loss data in training 0.0540967732667923 0.06961005851838657\n",
            "after loss data in training 0.0540967732667923 0.06958799125629327\n",
            "yyy epoch 703\n",
            "before loss data in training 0.06952399760484695 0.06958799125629327\n",
            "after loss data in training 0.06952399760484695 0.06958790035622019\n",
            "yyy epoch 704\n",
            "before loss data in training 0.07313045859336853 0.06958790035622019\n",
            "after loss data in training 0.07313045859336853 0.06959292526152111\n",
            "yyy epoch 705\n",
            "before loss data in training 0.05638164281845093 0.06959292526152111\n",
            "after loss data in training 0.05638164281845093 0.06957421239687087\n",
            "yyy epoch 706\n",
            "before loss data in training 0.08063925802707672 0.06957421239687087\n",
            "after loss data in training 0.08063925802707672 0.06958986309790369\n",
            "yyy epoch 707\n",
            "before loss data in training 0.08004621416330338 0.06958986309790369\n",
            "after loss data in training 0.08004621416330338 0.0696046319553407\n",
            "yyy epoch 708\n",
            "before loss data in training 0.06242365762591362 0.0696046319553407\n",
            "after loss data in training 0.06242365762591362 0.069594503641759\n",
            "yyy epoch 709\n",
            "before loss data in training 0.09070012718439102 0.069594503641759\n",
            "after loss data in training 0.09070012718439102 0.06962422987210073\n",
            "yyy epoch 710\n",
            "before loss data in training 0.06051810830831528 0.06962422987210073\n",
            "after loss data in training 0.06051810830831528 0.06961142238748219\n",
            "yyy epoch 711\n",
            "before loss data in training 0.067863829433918 0.06961142238748219\n",
            "after loss data in training 0.067863829433918 0.06960896790299685\n",
            "yyy epoch 712\n",
            "before loss data in training 0.056368615478277206 0.06960896790299685\n",
            "after loss data in training 0.056368615478277206 0.0695903979837476\n",
            "yyy epoch 713\n",
            "before loss data in training 0.04690016806125641 0.0695903979837476\n",
            "after loss data in training 0.04690016806125641 0.0695586189502427\n",
            "yyy epoch 714\n",
            "before loss data in training 0.07508137077093124 0.0695586189502427\n",
            "after loss data in training 0.07508137077093124 0.06956634307866325\n",
            "yyy epoch 715\n",
            "before loss data in training 0.07256896793842316 0.06956634307866325\n",
            "after loss data in training 0.07256896793842316 0.06957053668880257\n",
            "yyy epoch 716\n",
            "before loss data in training 0.04627646505832672 0.06957053668880257\n",
            "after loss data in training 0.04627646505832672 0.06953804844385073\n",
            "yyy epoch 717\n",
            "before loss data in training 0.07283928245306015 0.06953804844385073\n",
            "after loss data in training 0.07283928245306015 0.06954264626280506\n",
            "yyy epoch 718\n",
            "before loss data in training 0.07293824106454849 0.06954264626280506\n",
            "after loss data in training 0.07293824106454849 0.06954736892595073\n",
            "yyy epoch 719\n",
            "before loss data in training 0.0668722465634346 0.06954736892595073\n",
            "after loss data in training 0.0668722465634346 0.06954365347822501\n",
            "yyy epoch 720\n",
            "before loss data in training 0.05150113254785538 0.06954365347822501\n",
            "after loss data in training 0.05150113254785538 0.06951862917735072\n",
            "yyy epoch 721\n",
            "before loss data in training 0.05957262963056564 0.06951862917735072\n",
            "after loss data in training 0.05957262963056564 0.06950485355470974\n",
            "yyy epoch 722\n",
            "before loss data in training 0.06884464621543884 0.06950485355470974\n",
            "after loss data in training 0.06884464621543884 0.0695039404048629\n",
            "yyy epoch 723\n",
            "before loss data in training 0.04493746906518936 0.0695039404048629\n",
            "after loss data in training 0.04493746906518936 0.06947000881461472\n",
            "yyy epoch 724\n",
            "before loss data in training 0.07887369394302368 0.06947000881461472\n",
            "after loss data in training 0.07887369394302368 0.06948297941479184\n",
            "yyy epoch 725\n",
            "before loss data in training 0.06795840710401535 0.06948297941479184\n",
            "after loss data in training 0.06795840710401535 0.06948087945293127\n",
            "yyy epoch 726\n",
            "before loss data in training 0.07167172431945801 0.06948087945293127\n",
            "after loss data in training 0.07167172431945801 0.06948389299470091\n",
            "yyy epoch 727\n",
            "before loss data in training 0.07236428558826447 0.06948389299470091\n",
            "after loss data in training 0.07236428558826447 0.06948784957793383\n",
            "yyy epoch 728\n",
            "before loss data in training 0.06517192721366882 0.06948784957793383\n",
            "after loss data in training 0.06517192721366882 0.06948192924547256\n",
            "yyy epoch 729\n",
            "before loss data in training 0.056424807757139206 0.06948192924547256\n",
            "after loss data in training 0.056424807757139206 0.06946404277768031\n",
            "yyy epoch 730\n",
            "before loss data in training 0.07992218434810638 0.06946404277768031\n",
            "after loss data in training 0.07992218434810638 0.06947834940089567\n",
            "yyy epoch 731\n",
            "before loss data in training 0.07118962705135345 0.06947834940089567\n",
            "after loss data in training 0.07118962705135345 0.06948068721189357\n",
            "yyy epoch 732\n",
            "before loss data in training 0.06487368792295456 0.06948068721189357\n",
            "after loss data in training 0.06487368792295456 0.06947440208325928\n",
            "yyy epoch 733\n",
            "before loss data in training 0.05239640921354294 0.06947440208325928\n",
            "after loss data in training 0.05239640921354294 0.06945113506300081\n",
            "yyy epoch 734\n",
            "before loss data in training 0.056245144456624985 0.06945113506300081\n",
            "after loss data in training 0.056245144456624985 0.06943316772884248\n",
            "yyy epoch 735\n",
            "before loss data in training 0.0660482719540596 0.06943316772884248\n",
            "after loss data in training 0.0660482719540596 0.06942856868567021\n",
            "yyy epoch 736\n",
            "before loss data in training 0.0694361999630928 0.06942856868567021\n",
            "after loss data in training 0.0694361999630928 0.06942857904018503\n",
            "yyy epoch 737\n",
            "before loss data in training 0.07657168805599213 0.06942857904018503\n",
            "after loss data in training 0.07657168805599213 0.06943825804969155\n",
            "yyy epoch 738\n",
            "before loss data in training 0.07129214704036713 0.06943825804969155\n",
            "after loss data in training 0.07129214704036713 0.06944076669514578\n",
            "yyy epoch 739\n",
            "before loss data in training 0.07266488671302795 0.06944076669514578\n",
            "after loss data in training 0.07266488671302795 0.06944512361408886\n",
            "yyy epoch 740\n",
            "before loss data in training 0.054881028831005096 0.06944512361408886\n",
            "after loss data in training 0.054881028831005096 0.0694254689652588\n",
            "yyy epoch 741\n",
            "before loss data in training 0.06109294667840004 0.0694254689652588\n",
            "after loss data in training 0.06109294667840004 0.06941423915085602\n",
            "yyy epoch 742\n",
            "before loss data in training 0.06489265710115433 0.06941423915085602\n",
            "after loss data in training 0.06489265710115433 0.06940815357609195\n",
            "yyy epoch 743\n",
            "before loss data in training 0.06320390105247498 0.06940815357609195\n",
            "after loss data in training 0.06320390105247498 0.06939981452700107\n",
            "yyy epoch 744\n",
            "before loss data in training 0.05179876461625099 0.06939981452700107\n",
            "after loss data in training 0.05179876461625099 0.06937618895665107\n",
            "yyy epoch 745\n",
            "before loss data in training 0.05723479017615318 0.06937618895665107\n",
            "after loss data in training 0.05723479017615318 0.06935991362316515\n",
            "yyy epoch 746\n",
            "before loss data in training 0.07360325008630753 0.06935991362316515\n",
            "after loss data in training 0.07360325008630753 0.06936559412713188\n",
            "yyy epoch 747\n",
            "before loss data in training 0.06745006889104843 0.06936559412713188\n",
            "after loss data in training 0.06745006889104843 0.06936303326451679\n",
            "yyy epoch 748\n",
            "before loss data in training 0.0635194331407547 0.06936303326451679\n",
            "after loss data in training 0.0635194331407547 0.06935523139519267\n",
            "yyy epoch 749\n",
            "before loss data in training 0.0759250670671463 0.06935523139519267\n",
            "after loss data in training 0.0759250670671463 0.06936399117608862\n",
            "yyy epoch 750\n",
            "before loss data in training 0.06161040440201759 0.06936399117608862\n",
            "after loss data in training 0.06161040440201759 0.06935366682618972\n",
            "yyy epoch 751\n",
            "before loss data in training 0.06467737257480621 0.06935366682618972\n",
            "after loss data in training 0.06467737257480621 0.0693474483497916\n",
            "yyy epoch 752\n",
            "before loss data in training 0.05808509886264801 0.0693474483497916\n",
            "after loss data in training 0.05808509886264801 0.06933249171036644\n",
            "yyy epoch 753\n",
            "before loss data in training 0.04509103670716286 0.06933249171036644\n",
            "after loss data in training 0.04509103670716286 0.06930034123953992\n",
            "yyy epoch 754\n",
            "before loss data in training 0.0658494159579277 0.06930034123953992\n",
            "after loss data in training 0.0658494159579277 0.06929577047757751\n",
            "yyy epoch 755\n",
            "before loss data in training 0.07405858486890793 0.06929577047757751\n",
            "after loss data in training 0.07405858486890793 0.06930207049661366\n",
            "yyy epoch 756\n",
            "before loss data in training 0.0524897575378418 0.06930207049661366\n",
            "after loss data in training 0.0524897575378418 0.06927986136456772\n",
            "yyy epoch 757\n",
            "before loss data in training 0.07885657250881195 0.06927986136456772\n",
            "after loss data in training 0.07885657250881195 0.06929249554813532\n",
            "yyy epoch 758\n",
            "before loss data in training 0.05570603534579277 0.06929249554813532\n",
            "after loss data in training 0.05570603534579277 0.06927459507356043\n",
            "yyy epoch 759\n",
            "before loss data in training 0.0739394798874855 0.06927459507356043\n",
            "after loss data in training 0.0739394798874855 0.06928073307989455\n",
            "yyy epoch 760\n",
            "before loss data in training 0.061934567987918854 0.06928073307989455\n",
            "after loss data in training 0.061934567987918854 0.06927107977491166\n",
            "yyy epoch 761\n",
            "before loss data in training 0.05170406401157379 0.06927107977491166\n",
            "after loss data in training 0.05170406401157379 0.06924802594845059\n",
            "yyy epoch 762\n",
            "before loss data in training 0.05241040140390396 0.06924802594845059\n",
            "after loss data in training 0.05241040140390396 0.06922595828849705\n",
            "yyy epoch 763\n",
            "before loss data in training 0.07067519426345825 0.06922595828849705\n",
            "after loss data in training 0.07067519426345825 0.06922785519422343\n",
            "yyy epoch 764\n",
            "before loss data in training 0.08408749848604202 0.06922785519422343\n",
            "after loss data in training 0.08408749848604202 0.06924727956453954\n",
            "yyy epoch 765\n",
            "before loss data in training 0.06324034929275513 0.06924727956453954\n",
            "after loss data in training 0.06324034929275513 0.069239437619015\n",
            "yyy epoch 766\n",
            "before loss data in training 0.065824955701828 0.069239437619015\n",
            "after loss data in training 0.065824955701828 0.06923498588248674\n",
            "yyy epoch 767\n",
            "before loss data in training 0.05338320508599281 0.06923498588248674\n",
            "after loss data in training 0.05338320508599281 0.06921434554290797\n",
            "yyy epoch 768\n",
            "before loss data in training 0.06867101788520813 0.06921434554290797\n",
            "after loss data in training 0.06867101788520813 0.06921363900499158\n",
            "yyy epoch 769\n",
            "before loss data in training 0.04868689551949501 0.06921363900499158\n",
            "after loss data in training 0.04868689551949501 0.06918698089656886\n",
            "yyy epoch 770\n",
            "before loss data in training 0.06145991012454033 0.06918698089656886\n",
            "after loss data in training 0.06145991012454033 0.0691769587554897\n",
            "yyy epoch 771\n",
            "before loss data in training 0.06596606224775314 0.0691769587554897\n",
            "after loss data in training 0.06596606224775314 0.06917279956312217\n",
            "yyy epoch 772\n",
            "before loss data in training 0.06210784614086151 0.06917279956312217\n",
            "after loss data in training 0.06210784614086151 0.06916365990798341\n",
            "yyy epoch 773\n",
            "before loss data in training 0.0814175233244896 0.06916365990798341\n",
            "after loss data in training 0.0814175233244896 0.06917949177286262\n",
            "yyy epoch 774\n",
            "before loss data in training 0.057079192250967026 0.06917949177286262\n",
            "after loss data in training 0.057079192250967026 0.06916387848315694\n",
            "yyy epoch 775\n",
            "before loss data in training 0.066202811896801 0.06916387848315694\n",
            "after loss data in training 0.066202811896801 0.0691600626757003\n",
            "yyy epoch 776\n",
            "before loss data in training 0.051732540130615234 0.0691600626757003\n",
            "after loss data in training 0.051732540130615234 0.06913763343175552\n",
            "yyy epoch 777\n",
            "before loss data in training 0.05754002183675766 0.06913763343175552\n",
            "after loss data in training 0.05754002183675766 0.06912272647597789\n",
            "yyy epoch 778\n",
            "before loss data in training 0.05255918204784393 0.06912272647597789\n",
            "after loss data in training 0.05255918204784393 0.06910146390289941\n",
            "yyy epoch 779\n",
            "before loss data in training 0.07644060999155045 0.06910146390289941\n",
            "after loss data in training 0.07644060999155045 0.06911087306455152\n",
            "yyy epoch 780\n",
            "before loss data in training 0.0672527328133583 0.06911087306455152\n",
            "after loss data in training 0.0672527328133583 0.06910849388369211\n",
            "yyy epoch 781\n",
            "before loss data in training 0.059843841940164566 0.06910849388369211\n",
            "after loss data in training 0.059843841940164566 0.06909664650269016\n",
            "yyy epoch 782\n",
            "before loss data in training 0.07972492277622223 0.06909664650269016\n",
            "after loss data in training 0.07972492277622223 0.06911022029103439\n",
            "yyy epoch 783\n",
            "before loss data in training 0.06466590613126755 0.06911022029103439\n",
            "after loss data in training 0.06466590613126755 0.06910455152297347\n",
            "yyy epoch 784\n",
            "before loss data in training 0.05798264220356941 0.06910455152297347\n",
            "after loss data in training 0.05798264220356941 0.06909038348562391\n",
            "yyy epoch 785\n",
            "before loss data in training 0.05190045386552811 0.06909038348562391\n",
            "after loss data in training 0.05190045386552811 0.06906851334615814\n",
            "yyy epoch 786\n",
            "before loss data in training 0.05605914071202278 0.06906851334615814\n",
            "after loss data in training 0.05605914071202278 0.0690519830124426\n",
            "yyy epoch 787\n",
            "before loss data in training 0.05703675374388695 0.0690519830124426\n",
            "after loss data in training 0.05703675374388695 0.06903673525956372\n",
            "yyy epoch 788\n",
            "before loss data in training 0.06533946841955185 0.06903673525956372\n",
            "after loss data in training 0.06533946841955185 0.06903204924328994\n",
            "yyy epoch 789\n",
            "before loss data in training 0.06918957084417343 0.06903204924328994\n",
            "after loss data in training 0.06918957084417343 0.06903224863772144\n",
            "yyy epoch 790\n",
            "before loss data in training 0.054344356060028076 0.06903224863772144\n",
            "after loss data in training 0.054344356060028076 0.06901367987340071\n",
            "yyy epoch 791\n",
            "before loss data in training 0.05198313668370247 0.06901367987340071\n",
            "after loss data in training 0.05198313668370247 0.0689921766623026\n",
            "yyy epoch 792\n",
            "before loss data in training 0.06557191908359528 0.0689921766623026\n",
            "after loss data in training 0.06557191908359528 0.0689878636010432\n",
            "yyy epoch 793\n",
            "before loss data in training 0.07981197535991669 0.0689878636010432\n",
            "after loss data in training 0.07981197535991669 0.06900149598361105\n",
            "yyy epoch 794\n",
            "before loss data in training 0.08394671976566315 0.06900149598361105\n",
            "after loss data in training 0.08394671976566315 0.06902029500723628\n",
            "yyy epoch 795\n",
            "before loss data in training 0.07742802053689957 0.06902029500723628\n",
            "after loss data in training 0.07742802053689957 0.06903085747649465\n",
            "yyy epoch 796\n",
            "before loss data in training 0.07859133183956146 0.06903085747649465\n",
            "after loss data in training 0.07859133183956146 0.06904285305285986\n",
            "yyy epoch 797\n",
            "before loss data in training 0.0660082995891571 0.06904285305285986\n",
            "after loss data in training 0.0660082995891571 0.06903905035428379\n",
            "yyy epoch 798\n",
            "before loss data in training 0.05072125419974327 0.06903905035428379\n",
            "after loss data in training 0.05072125419974327 0.0690161244517124\n",
            "yyy epoch 799\n",
            "before loss data in training 0.06373376399278641 0.0690161244517124\n",
            "after loss data in training 0.06373376399278641 0.06900952150113875\n",
            "yyy epoch 800\n",
            "before loss data in training 0.06412113457918167 0.06900952150113875\n",
            "after loss data in training 0.06412113457918167 0.06900341864605515\n",
            "yyy epoch 801\n",
            "before loss data in training 0.06228019297122955 0.06900341864605515\n",
            "after loss data in training 0.06228019297122955 0.06899503557164764\n",
            "yyy epoch 802\n",
            "before loss data in training 0.07164020836353302 0.06899503557164764\n",
            "after loss data in training 0.07164020836353302 0.0689983296847135\n",
            "yyy epoch 803\n",
            "before loss data in training 0.05014548450708389 0.0689983296847135\n",
            "after loss data in training 0.05014548450708389 0.06897488087230352\n",
            "yyy epoch 804\n",
            "before loss data in training 0.04467722028493881 0.06897488087230352\n",
            "after loss data in training 0.04467722028493881 0.06894469744300245\n",
            "yyy epoch 805\n",
            "before loss data in training 0.07231990993022919 0.06894469744300245\n",
            "after loss data in training 0.07231990993022919 0.06894888505154739\n",
            "yyy epoch 806\n",
            "before loss data in training 0.0696326196193695 0.06894888505154739\n",
            "after loss data in training 0.0696326196193695 0.06894973230627828\n",
            "yyy epoch 807\n",
            "before loss data in training 0.05481044948101044 0.06894973230627828\n",
            "after loss data in training 0.05481044948101044 0.06893223319387078\n",
            "yyy epoch 808\n",
            "before loss data in training 0.06318189203739166 0.06893223319387078\n",
            "after loss data in training 0.06318189203739166 0.06892512523199626\n",
            "yyy epoch 809\n",
            "before loss data in training 0.048924658447504044 0.06892512523199626\n",
            "after loss data in training 0.048924658447504044 0.06890043329769442\n",
            "yyy epoch 810\n",
            "before loss data in training 0.05845482647418976 0.06890043329769442\n",
            "after loss data in training 0.05845482647418976 0.06888755338792438\n",
            "yyy epoch 811\n",
            "before loss data in training 0.06020483002066612 0.06888755338792438\n",
            "after loss data in training 0.06020483002066612 0.06887686037885139\n",
            "yyy epoch 812\n",
            "before loss data in training 0.06542332470417023 0.06887686037885139\n",
            "after loss data in training 0.06542332470417023 0.06887261248749262\n",
            "yyy epoch 813\n",
            "before loss data in training 0.06774390488862991 0.06887261248749262\n",
            "after loss data in training 0.06774390488862991 0.06887122586882079\n",
            "yyy epoch 814\n",
            "before loss data in training 0.06317833065986633 0.06887122586882079\n",
            "after loss data in training 0.06317833065986633 0.06886424072132513\n",
            "yyy epoch 815\n",
            "before loss data in training 0.05841886252164841 0.06886424072132513\n",
            "after loss data in training 0.05841886252164841 0.0688514400127471\n",
            "yyy epoch 816\n",
            "before loss data in training 0.05243963003158569 0.0688514400127471\n",
            "after loss data in training 0.05243963003158569 0.06883135211803332\n",
            "yyy epoch 817\n",
            "before loss data in training 0.06318537890911102 0.06883135211803332\n",
            "after loss data in training 0.06318537890911102 0.06882444995029625\n",
            "yyy epoch 818\n",
            "before loss data in training 0.07049474120140076 0.06882444995029625\n",
            "after loss data in training 0.07049474120140076 0.06882648937795328\n",
            "yyy epoch 819\n",
            "before loss data in training 0.051680199801921844 0.06882648937795328\n",
            "after loss data in training 0.051680199801921844 0.06880557926871422\n",
            "yyy epoch 820\n",
            "before loss data in training 0.07959682494401932 0.06880557926871422\n",
            "after loss data in training 0.07959682494401932 0.06881872329511532\n",
            "yyy epoch 821\n",
            "before loss data in training 0.05749412626028061 0.06881872329511532\n",
            "after loss data in training 0.05749412626028061 0.06880494641307781\n",
            "yyy epoch 822\n",
            "before loss data in training 0.05599527433514595 0.06880494641307781\n",
            "after loss data in training 0.05599527433514595 0.0687893818054497\n",
            "yyy epoch 823\n",
            "before loss data in training 0.07263457775115967 0.0687893818054497\n",
            "after loss data in training 0.07263457775115967 0.06879404830538381\n",
            "yyy epoch 824\n",
            "before loss data in training 0.07176999747753143 0.06879404830538381\n",
            "after loss data in training 0.07176999747753143 0.06879765551650156\n",
            "yyy epoch 825\n",
            "before loss data in training 0.05614302307367325 0.06879765551650156\n",
            "after loss data in training 0.05614302307367325 0.06878233513824147\n",
            "yyy epoch 826\n",
            "before loss data in training 0.07682516425848007 0.06878233513824147\n",
            "after loss data in training 0.07682516425848007 0.06879206044552108\n",
            "yyy epoch 827\n",
            "before loss data in training 0.056911151856184006 0.06879206044552108\n",
            "after loss data in training 0.056911151856184006 0.068777711522104\n",
            "yyy epoch 828\n",
            "before loss data in training 0.06909752637147903 0.068777711522104\n",
            "after loss data in training 0.06909752637147903 0.06877809730599951\n",
            "yyy epoch 829\n",
            "before loss data in training 0.04532533884048462 0.06877809730599951\n",
            "after loss data in training 0.04532533884048462 0.0687498409704989\n",
            "yyy epoch 830\n",
            "before loss data in training 0.0546715147793293 0.0687498409704989\n",
            "after loss data in training 0.0546715147793293 0.0687328995430727\n",
            "yyy epoch 831\n",
            "before loss data in training 0.05566760152578354 0.0687328995430727\n",
            "after loss data in training 0.05566760152578354 0.06871719605987885\n",
            "yyy epoch 832\n",
            "before loss data in training 0.07157497853040695 0.06871719605987885\n",
            "after loss data in training 0.07157497853040695 0.06872062677112797\n",
            "yyy epoch 833\n",
            "before loss data in training 0.06771447509527206 0.06872062677112797\n",
            "after loss data in training 0.06771447509527206 0.06871942035425045\n",
            "yyy epoch 834\n",
            "before loss data in training 0.05010319873690605 0.06871942035425045\n",
            "after loss data in training 0.05010319873690605 0.06869712547806202\n",
            "yyy epoch 835\n",
            "before loss data in training 0.06147991120815277 0.06869712547806202\n",
            "after loss data in training 0.06147991120815277 0.06868849244663869\n",
            "yyy epoch 836\n",
            "before loss data in training 0.06716413050889969 0.06868849244663869\n",
            "after loss data in training 0.06716413050889969 0.0686866712256856\n",
            "yyy epoch 837\n",
            "before loss data in training 0.05570491403341293 0.0686866712256856\n",
            "after loss data in training 0.05570491403341293 0.06867117986865424\n",
            "yyy epoch 838\n",
            "before loss data in training 0.058990832418203354 0.06867117986865424\n",
            "after loss data in training 0.058990832418203354 0.06865964190983369\n",
            "yyy epoch 839\n",
            "before loss data in training 0.05846333131194115 0.06865964190983369\n",
            "after loss data in training 0.05846333131194115 0.0686475034448362\n",
            "yyy epoch 840\n",
            "before loss data in training 0.06583913415670395 0.0686475034448362\n",
            "after loss data in training 0.06583913415670395 0.06864416412344722\n",
            "yyy epoch 841\n",
            "before loss data in training 0.062128473073244095 0.06864416412344722\n",
            "after loss data in training 0.062128473073244095 0.0686364257730313\n",
            "yyy epoch 842\n",
            "before loss data in training 0.056230392307043076 0.0686364257730313\n",
            "after loss data in training 0.056230392307043076 0.0686217092446019\n",
            "yyy epoch 843\n",
            "before loss data in training 0.0471702478826046 0.0686217092446019\n",
            "after loss data in training 0.0471702478826046 0.0685962928211872\n",
            "yyy epoch 844\n",
            "before loss data in training 0.061523742973804474 0.0685962928211872\n",
            "after loss data in training 0.061523742973804474 0.06858792293971101\n",
            "yyy epoch 845\n",
            "before loss data in training 0.06777564436197281 0.06858792293971101\n",
            "after loss data in training 0.06777564436197281 0.0685869627995482\n",
            "yyy epoch 846\n",
            "before loss data in training 0.07528333365917206 0.0685869627995482\n",
            "after loss data in training 0.07528333365917206 0.06859486878639547\n",
            "yyy epoch 847\n",
            "before loss data in training 0.053353212773799896 0.06859486878639547\n",
            "after loss data in training 0.053353212773799896 0.06857689513543722\n",
            "yyy epoch 848\n",
            "before loss data in training 0.07113130390644073 0.06857689513543722\n",
            "after loss data in training 0.07113130390644073 0.06857990386190482\n",
            "yyy epoch 849\n",
            "before loss data in training 0.06262265890836716 0.06857990386190482\n",
            "after loss data in training 0.06262265890836716 0.06857289533843007\n",
            "yyy epoch 850\n",
            "before loss data in training 0.07675302773714066 0.06857289533843007\n",
            "after loss data in training 0.07675302773714066 0.0685825077149268\n",
            "yyy epoch 851\n",
            "before loss data in training 0.06397715955972672 0.0685825077149268\n",
            "after loss data in training 0.06397715955972672 0.06857710237671646\n",
            "yyy epoch 852\n",
            "before loss data in training 0.05372302234172821 0.06857710237671646\n",
            "after loss data in training 0.05372302234172821 0.06855968844936008\n",
            "yyy epoch 853\n",
            "before loss data in training 0.08471988886594772 0.06855968844936008\n",
            "after loss data in training 0.08471988886594772 0.06857861140066757\n",
            "yyy epoch 854\n",
            "before loss data in training 0.05828706547617912 0.06857861140066757\n",
            "after loss data in training 0.05828706547617912 0.06856657450484945\n",
            "yyy epoch 855\n",
            "before loss data in training 0.05143367499113083 0.06856657450484945\n",
            "after loss data in training 0.05143367499113083 0.06854655943532408\n",
            "yyy epoch 856\n",
            "before loss data in training 0.05622538551688194 0.06854655943532408\n",
            "after loss data in training 0.05622538551688194 0.06853218233623605\n",
            "yyy epoch 857\n",
            "before loss data in training 0.06393222510814667 0.06853218233623605\n",
            "after loss data in training 0.06393222510814667 0.06852682108072546\n",
            "yyy epoch 858\n",
            "before loss data in training 0.05446002632379532 0.06852682108072546\n",
            "after loss data in training 0.05446002632379532 0.0685104453010317\n",
            "yyy epoch 859\n",
            "before loss data in training 0.06326289474964142 0.0685104453010317\n",
            "after loss data in training 0.06326289474964142 0.06850434349806497\n",
            "yyy epoch 860\n",
            "before loss data in training 0.07154572755098343 0.06850434349806497\n",
            "after loss data in training 0.07154572755098343 0.06850787588372458\n",
            "yyy epoch 861\n",
            "before loss data in training 0.06261741369962692 0.06850787588372458\n",
            "after loss data in training 0.06261741369962692 0.0685010424009124\n",
            "yyy epoch 862\n",
            "before loss data in training 0.054481275379657745 0.0685010424009124\n",
            "after loss data in training 0.054481275379657745 0.06848479701618325\n",
            "yyy epoch 863\n",
            "before loss data in training 0.0582544319331646 0.06848479701618325\n",
            "after loss data in training 0.0582544319331646 0.06847295631585568\n",
            "yyy epoch 864\n",
            "before loss data in training 0.07028187066316605 0.06847295631585568\n",
            "after loss data in training 0.07028187066316605 0.06847504754631499\n",
            "yyy epoch 865\n",
            "before loss data in training 0.05881809815764427 0.06847504754631499\n",
            "after loss data in training 0.05881809815764427 0.06846389633454979\n",
            "yyy epoch 866\n",
            "before loss data in training 0.057013899087905884 0.06846389633454979\n",
            "after loss data in training 0.057013899087905884 0.06845068987867131\n",
            "yyy epoch 867\n",
            "before loss data in training 0.0697702169418335 0.06845068987867131\n",
            "after loss data in training 0.0697702169418335 0.0684522100711404\n",
            "yyy epoch 868\n",
            "before loss data in training 0.05975095555186272 0.0684522100711404\n",
            "after loss data in training 0.05975095555186272 0.068442197120025\n",
            "yyy epoch 869\n",
            "before loss data in training 0.06124655529856682 0.068442197120025\n",
            "after loss data in training 0.06124655529856682 0.06843392626735666\n",
            "yyy epoch 870\n",
            "before loss data in training 0.05591149255633354 0.06843392626735666\n",
            "after loss data in training 0.05591149255633354 0.06841954919076536\n",
            "yyy epoch 871\n",
            "before loss data in training 0.059638988226652145 0.06841954919076536\n",
            "after loss data in training 0.059638988226652145 0.06840947974011843\n",
            "yyy epoch 872\n",
            "before loss data in training 0.049064911901950836 0.06840947974011843\n",
            "after loss data in training 0.049064911901950836 0.06838732101407242\n",
            "yyy epoch 873\n",
            "before loss data in training 0.05942138284444809 0.06838732101407242\n",
            "after loss data in training 0.05942138284444809 0.06837706250358086\n",
            "yyy epoch 874\n",
            "before loss data in training 0.059469688683748245 0.06837706250358086\n",
            "after loss data in training 0.059469688683748245 0.06836688264778676\n",
            "yyy epoch 875\n",
            "before loss data in training 0.08242946118116379 0.06836688264778676\n",
            "after loss data in training 0.08242946118116379 0.06838293581962851\n",
            "yyy epoch 876\n",
            "before loss data in training 0.05291948467493057 0.06838293581962851\n",
            "after loss data in training 0.05291948467493057 0.0683653036062366\n",
            "yyy epoch 877\n",
            "before loss data in training 0.06988448649644852 0.0683653036062366\n",
            "after loss data in training 0.06988448649644852 0.06836703388287693\n",
            "yyy epoch 878\n",
            "before loss data in training 0.05342723801732063 0.06836703388287693\n",
            "after loss data in training 0.05342723801732063 0.06835003752808108\n",
            "yyy epoch 879\n",
            "before loss data in training 0.07026750594377518 0.06835003752808108\n",
            "after loss data in training 0.07026750594377518 0.06835221646946255\n",
            "yyy epoch 880\n",
            "before loss data in training 0.07176361232995987 0.06835221646946255\n",
            "after loss data in training 0.07176361232995987 0.0683560886554563\n",
            "yyy epoch 881\n",
            "before loss data in training 0.0554099939763546 0.0683560886554563\n",
            "after loss data in training 0.0554099939763546 0.06834141054357523\n",
            "yyy epoch 882\n",
            "before loss data in training 0.05146606266498566 0.06834141054357523\n",
            "after loss data in training 0.05146606266498566 0.06832229916432428\n",
            "yyy epoch 883\n",
            "before loss data in training 0.06315384805202484 0.06832229916432428\n",
            "after loss data in training 0.06315384805202484 0.0683164525001701\n",
            "yyy epoch 884\n",
            "before loss data in training 0.044973935931921005 0.0683164525001701\n",
            "after loss data in training 0.044973935931921005 0.06829007677523423\n",
            "yyy epoch 885\n",
            "before loss data in training 0.061688605695962906 0.06829007677523423\n",
            "after loss data in training 0.061688605695962906 0.0682826259049416\n",
            "yyy epoch 886\n",
            "before loss data in training 0.053472328931093216 0.0682826259049416\n",
            "after loss data in training 0.053472328931093216 0.06826592883958213\n",
            "yyy epoch 887\n",
            "before loss data in training 0.06848619878292084 0.06826592883958213\n",
            "after loss data in training 0.06848619878292084 0.06826617689132013\n",
            "yyy epoch 888\n",
            "before loss data in training 0.06181931495666504 0.06826617689132013\n",
            "after loss data in training 0.06181931495666504 0.06825892507812029\n",
            "yyy epoch 889\n",
            "before loss data in training 0.05681021511554718 0.06825892507812029\n",
            "after loss data in training 0.05681021511554718 0.06824606135906122\n",
            "yyy epoch 890\n",
            "before loss data in training 0.05915652960538864 0.06824606135906122\n",
            "after loss data in training 0.05915652960538864 0.06823585986438818\n",
            "yyy epoch 891\n",
            "before loss data in training 0.0680244192481041 0.06823585986438818\n",
            "after loss data in training 0.0680244192481041 0.06823562282333853\n",
            "yyy epoch 892\n",
            "before loss data in training 0.0538845919072628 0.06823562282333853\n",
            "after loss data in training 0.0538845919072628 0.06821955224000587\n",
            "yyy epoch 893\n",
            "before loss data in training 0.05785676836967468 0.06821955224000587\n",
            "after loss data in training 0.05785676836967468 0.06820796075916657\n",
            "yyy epoch 894\n",
            "before loss data in training 0.048699017614126205 0.06820796075916657\n",
            "after loss data in training 0.048699017614126205 0.06818616305732854\n",
            "yyy epoch 895\n",
            "before loss data in training 0.07237119227647781 0.06818616305732854\n",
            "after loss data in training 0.07237119227647781 0.06819083384886777\n",
            "yyy epoch 896\n",
            "before loss data in training 0.06696664541959763 0.06819083384886777\n",
            "after loss data in training 0.06696664541959763 0.06818946909030671\n",
            "yyy epoch 897\n",
            "before loss data in training 0.06804892420768738 0.06818946909030671\n",
            "after loss data in training 0.06804892420768738 0.06818931258152873\n",
            "yyy epoch 898\n",
            "before loss data in training 0.06403370201587677 0.06818931258152873\n",
            "after loss data in training 0.06403370201587677 0.0681846901003656\n",
            "yyy epoch 899\n",
            "before loss data in training 0.07064305990934372 0.0681846901003656\n",
            "after loss data in training 0.07064305990934372 0.06818742162237557\n",
            "yyy epoch 900\n",
            "before loss data in training 0.047634199261665344 0.06818742162237557\n",
            "after loss data in training 0.047634199261665344 0.0681646100548276\n",
            "yyy epoch 901\n",
            "before loss data in training 0.06477928161621094 0.0681646100548276\n",
            "after loss data in training 0.06477928161621094 0.06816085691908634\n",
            "yyy epoch 902\n",
            "before loss data in training 0.05926491692662239 0.06816085691908634\n",
            "after loss data in training 0.05926491692662239 0.06815100537978129\n",
            "yyy epoch 903\n",
            "before loss data in training 0.06676743179559708 0.06815100537978129\n",
            "after loss data in training 0.06676743179559708 0.06814947487802887\n",
            "yyy epoch 904\n",
            "before loss data in training 0.05427965894341469 0.06814947487802887\n",
            "after loss data in training 0.05427965894341469 0.06813414911456521\n",
            "yyy epoch 905\n",
            "before loss data in training 0.07218379527330399 0.06813414911456521\n",
            "after loss data in training 0.07218379527330399 0.06813861892268744\n",
            "yyy epoch 906\n",
            "before loss data in training 0.06876950711011887 0.06813861892268744\n",
            "after loss data in training 0.06876950711011887 0.06813931449952033\n",
            "yyy epoch 907\n",
            "before loss data in training 0.06681395322084427 0.06813931449952033\n",
            "after loss data in training 0.06681395322084427 0.06813785485053502\n",
            "yyy epoch 908\n",
            "before loss data in training 0.05588335916399956 0.06813785485053502\n",
            "after loss data in training 0.05588335916399956 0.0681243735571505\n",
            "yyy epoch 909\n",
            "before loss data in training 0.06805983930826187 0.0681243735571505\n",
            "after loss data in training 0.06805983930826187 0.06812430264039347\n",
            "yyy epoch 910\n",
            "before loss data in training 0.06277669966220856 0.06812430264039347\n",
            "after loss data in training 0.06277669966220856 0.0681184326041935\n",
            "yyy epoch 911\n",
            "before loss data in training 0.051374729722738266 0.0681184326041935\n",
            "after loss data in training 0.051374729722738266 0.06810007328085857\n",
            "yyy epoch 912\n",
            "before loss data in training 0.06597575545310974 0.06810007328085857\n",
            "after loss data in training 0.06597575545310974 0.06809774653624986\n",
            "yyy epoch 913\n",
            "before loss data in training 0.06075923517346382 0.06809774653624986\n",
            "after loss data in training 0.06075923517346382 0.06808971753038248\n",
            "yyy epoch 914\n",
            "before loss data in training 0.07863100618124008 0.06808971753038248\n",
            "after loss data in training 0.07863100618124008 0.06810123806442714\n",
            "yyy epoch 915\n",
            "before loss data in training 0.05888407304883003 0.06810123806442714\n",
            "after loss data in training 0.05888407304883003 0.06809117565720488\n",
            "yyy epoch 916\n",
            "before loss data in training 0.06278648227453232 0.06809117565720488\n",
            "after loss data in training 0.06278648227453232 0.06808539082254547\n",
            "yyy epoch 917\n",
            "before loss data in training 0.061008941382169724 0.06808539082254547\n",
            "after loss data in training 0.061008941382169724 0.06807768227195683\n",
            "yyy epoch 918\n",
            "before loss data in training 0.05302397906780243 0.06807768227195683\n",
            "after loss data in training 0.05302397906780243 0.06806130174616341\n",
            "yyy epoch 919\n",
            "before loss data in training 0.06441627442836761 0.06806130174616341\n",
            "after loss data in training 0.06441627442836761 0.06805733975994842\n",
            "yyy epoch 920\n",
            "before loss data in training 0.06998447328805923 0.06805733975994842\n",
            "after loss data in training 0.06998447328805923 0.06805943219591813\n",
            "yyy epoch 921\n",
            "before loss data in training 0.055638816207647324 0.06805943219591813\n",
            "after loss data in training 0.055638816207647324 0.06804596081198291\n",
            "yyy epoch 922\n",
            "before loss data in training 0.0714431181550026 0.06804596081198291\n",
            "after loss data in training 0.0714431181550026 0.06804964137248457\n",
            "yyy epoch 923\n",
            "before loss data in training 0.057003725320100784 0.06804964137248457\n",
            "after loss data in training 0.057003725320100784 0.06803768691788242\n",
            "yyy epoch 924\n",
            "before loss data in training 0.04756082221865654 0.06803768691788242\n",
            "after loss data in training 0.04756082221865654 0.06801554976685623\n",
            "yyy epoch 925\n",
            "before loss data in training 0.07667724043130875 0.06801554976685623\n",
            "after loss data in training 0.07667724043130875 0.06802490364446363\n",
            "yyy epoch 926\n",
            "before loss data in training 0.0660003051161766 0.06802490364446363\n",
            "after loss data in training 0.0660003051161766 0.06802271961153128\n",
            "yyy epoch 927\n",
            "before loss data in training 0.06475286930799484 0.06802271961153128\n",
            "after loss data in training 0.06475286930799484 0.06801919606594557\n",
            "yyy epoch 928\n",
            "before loss data in training 0.05936720594763756 0.06801919606594557\n",
            "after loss data in training 0.05936720594763756 0.06800988283653943\n",
            "yyy epoch 929\n",
            "before loss data in training 0.06749070435762405 0.06800988283653943\n",
            "after loss data in training 0.06749070435762405 0.06800932458011048\n",
            "yyy epoch 930\n",
            "before loss data in training 0.050221823155879974 0.06800932458011048\n",
            "after loss data in training 0.050221823155879974 0.06799021877836588\n",
            "yyy epoch 931\n",
            "before loss data in training 0.051384035497903824 0.06799021877836588\n",
            "after loss data in training 0.051384035497903824 0.0679724009851465\n",
            "yyy epoch 932\n",
            "before loss data in training 0.054926786571741104 0.0679724009851465\n",
            "after loss data in training 0.054926786571741104 0.06795841854740436\n",
            "yyy epoch 933\n",
            "before loss data in training 0.0701175406575203 0.06795841854740436\n",
            "after loss data in training 0.0701175406575203 0.06796073024131241\n",
            "yyy epoch 934\n",
            "before loss data in training 0.06609661132097244 0.06796073024131241\n",
            "after loss data in training 0.06609661132097244 0.06795873653123718\n",
            "yyy epoch 935\n",
            "before loss data in training 0.059525031596422195 0.06795873653123718\n",
            "after loss data in training 0.059525031596422195 0.06794972616271708\n",
            "yyy epoch 936\n",
            "before loss data in training 0.07869797945022583 0.06794972616271708\n",
            "after loss data in training 0.07869797945022583 0.06796119708404846\n",
            "yyy epoch 937\n",
            "before loss data in training 0.05665864050388336 0.06796119708404846\n",
            "after loss data in training 0.05665864050388336 0.0679491474501677\n",
            "yyy epoch 938\n",
            "before loss data in training 0.07556093484163284 0.0679491474501677\n",
            "after loss data in training 0.07556093484163284 0.06795725372002016\n",
            "yyy epoch 939\n",
            "before loss data in training 0.04991665482521057 0.06795725372002016\n",
            "after loss data in training 0.04991665482521057 0.06793806159353633\n",
            "yyy epoch 940\n",
            "before loss data in training 0.0799114853143692 0.06793806159353633\n",
            "after loss data in training 0.0799114853143692 0.06795078574201754\n",
            "yyy epoch 941\n",
            "before loss data in training 0.08705614507198334 0.06795078574201754\n",
            "after loss data in training 0.08705614507198334 0.06797106743982005\n",
            "yyy epoch 942\n",
            "before loss data in training 0.05967821180820465 0.06797106743982005\n",
            "after loss data in training 0.05967821180820465 0.06796227331931994\n",
            "yyy epoch 943\n",
            "before loss data in training 0.0433872751891613 0.06796227331931994\n",
            "after loss data in training 0.0433872751891613 0.06793624048231765\n",
            "yyy epoch 944\n",
            "before loss data in training 0.05791846290230751 0.06793624048231765\n",
            "after loss data in training 0.05791846290230751 0.06792563965948166\n",
            "yyy epoch 945\n",
            "before loss data in training 0.06683124601840973 0.06792563965948166\n",
            "after loss data in training 0.06683124601840973 0.06792448279516763\n",
            "yyy epoch 946\n",
            "before loss data in training 0.05181632936000824 0.06792448279516763\n",
            "after loss data in training 0.05181632936000824 0.0679074731294494\n",
            "yyy epoch 947\n",
            "before loss data in training 0.05583958700299263 0.0679074731294494\n",
            "after loss data in training 0.05583958700299263 0.06789474329176327\n",
            "yyy epoch 948\n",
            "before loss data in training 0.07452259957790375 0.06789474329176327\n",
            "after loss data in training 0.07452259957790375 0.06790172733421443\n",
            "yyy epoch 949\n",
            "before loss data in training 0.07643003761768341 0.06790172733421443\n",
            "after loss data in training 0.07643003761768341 0.06791070450293386\n",
            "yyy epoch 950\n",
            "before loss data in training 0.06334088742733002 0.06791070450293386\n",
            "after loss data in training 0.06334088742733002 0.06790589922735489\n",
            "yyy epoch 951\n",
            "before loss data in training 0.07635930925607681 0.06790589922735489\n",
            "after loss data in training 0.07635930925607681 0.067914778859738\n",
            "yyy epoch 952\n",
            "before loss data in training 0.052240464836359024 0.067914778859738\n",
            "after loss data in training 0.052240464836359024 0.06789833152078377\n",
            "yyy epoch 953\n",
            "before loss data in training 0.08648952096700668 0.06789833152078377\n",
            "after loss data in training 0.08648952096700668 0.06791781914074836\n",
            "yyy epoch 954\n",
            "before loss data in training 0.055976856499910355 0.06791781914074836\n",
            "after loss data in training 0.055976856499910355 0.06790531551494644\n",
            "yyy epoch 955\n",
            "before loss data in training 0.053067732602357864 0.06790531551494644\n",
            "after loss data in training 0.053067732602357864 0.06788979503072826\n",
            "yyy epoch 956\n",
            "before loss data in training 0.07606034725904465 0.06788979503072826\n",
            "after loss data in training 0.07606034725904465 0.06789833270285815\n",
            "yyy epoch 957\n",
            "before loss data in training 0.07277141511440277 0.06789833270285815\n",
            "after loss data in training 0.07277141511440277 0.06790341942771362\n",
            "yyy epoch 958\n",
            "before loss data in training 0.0633184164762497 0.06790341942771362\n",
            "after loss data in training 0.0633184164762497 0.06789863840273816\n",
            "yyy epoch 959\n",
            "before loss data in training 0.06287138909101486 0.06789863840273816\n",
            "after loss data in training 0.06287138909101486 0.06789340168470512\n",
            "yyy epoch 960\n",
            "before loss data in training 0.0531451590359211 0.06789340168470512\n",
            "after loss data in training 0.0531451590359211 0.06787805491816112\n",
            "yyy epoch 961\n",
            "before loss data in training 0.07725051790475845 0.06787805491816112\n",
            "after loss data in training 0.07725051790475845 0.06788779760317838\n",
            "yyy epoch 962\n",
            "before loss data in training 0.058453384786844254 0.06788779760317838\n",
            "after loss data in training 0.058453384786844254 0.06787800070513442\n",
            "yyy epoch 963\n",
            "before loss data in training 0.05658639594912529 0.06787800070513442\n",
            "after loss data in training 0.05658639594912529 0.0678662874221925\n",
            "yyy epoch 964\n",
            "before loss data in training 0.06001216918230057 0.0678662874221925\n",
            "after loss data in training 0.06001216918230057 0.06785814843956049\n",
            "yyy epoch 965\n",
            "before loss data in training 0.050439201295375824 0.06785814843956049\n",
            "after loss data in training 0.050439201295375824 0.06784011640317934\n",
            "yyy epoch 966\n",
            "before loss data in training 0.06342743337154388 0.06784011640317934\n",
            "after loss data in training 0.06342743337154388 0.06783555313220557\n",
            "yyy epoch 967\n",
            "before loss data in training 0.0598764605820179 0.06783555313220557\n",
            "after loss data in training 0.0598764605820179 0.06782733092915785\n",
            "yyy epoch 968\n",
            "before loss data in training 0.05862099677324295 0.06782733092915785\n",
            "after loss data in training 0.05862099677324295 0.06781783006831583\n",
            "yyy epoch 969\n",
            "before loss data in training 0.05947333946824074 0.06781783006831583\n",
            "after loss data in training 0.05947333946824074 0.0678092275006869\n",
            "yyy epoch 970\n",
            "before loss data in training 0.0597577840089798 0.0678092275006869\n",
            "after loss data in training 0.0597577840089798 0.06780093559183858\n",
            "yyy epoch 971\n",
            "before loss data in training 0.0621158704161644 0.06780093559183858\n",
            "after loss data in training 0.0621158704161644 0.06779508675935332\n",
            "yyy epoch 972\n",
            "before loss data in training 0.062092915177345276 0.06779508675935332\n",
            "after loss data in training 0.062092915177345276 0.06778922635690521\n",
            "yyy epoch 973\n",
            "before loss data in training 0.07288810610771179 0.06778922635690521\n",
            "after loss data in training 0.07288810610771179 0.06779446134638242\n",
            "yyy epoch 974\n",
            "before loss data in training 0.05004815757274628 0.06779446134638242\n",
            "after loss data in training 0.05004815757274628 0.06777626000917869\n",
            "yyy epoch 975\n",
            "before loss data in training 0.06448744237422943 0.06777626000917869\n",
            "after loss data in training 0.06448744237422943 0.06777289031897894\n",
            "yyy epoch 976\n",
            "before loss data in training 0.05762021988630295 0.06777289031897894\n",
            "after loss data in training 0.05762021988630295 0.0677624986399281\n",
            "yyy epoch 977\n",
            "before loss data in training 0.08004507422447205 0.0677624986399281\n",
            "after loss data in training 0.08004507422447205 0.06777505751066894\n",
            "yyy epoch 978\n",
            "before loss data in training 0.0546400360763073 0.06777505751066894\n",
            "after loss data in training 0.0546400360763073 0.06776164073698726\n",
            "yyy epoch 979\n",
            "before loss data in training 0.06483468413352966 0.06776164073698726\n",
            "after loss data in training 0.06483468413352966 0.06775865404657556\n",
            "yyy epoch 980\n",
            "before loss data in training 0.08017431199550629 0.06775865404657556\n",
            "after loss data in training 0.08017431199550629 0.0677713101708864\n",
            "yyy epoch 981\n",
            "before loss data in training 0.04968227073550224 0.0677713101708864\n",
            "after loss data in training 0.04968227073550224 0.06775288956046341\n",
            "yyy epoch 982\n",
            "before loss data in training 0.05290735140442848 0.06775288956046341\n",
            "after loss data in training 0.05290735140442848 0.0677377872836007\n",
            "yyy epoch 983\n",
            "before loss data in training 0.05453956499695778 0.0677377872836007\n",
            "after loss data in training 0.05453956499695778 0.06772437445607363\n",
            "yyy epoch 984\n",
            "before loss data in training 0.07864947617053986 0.06772437445607363\n",
            "after loss data in training 0.07864947617053986 0.06773546592989542\n",
            "yyy epoch 985\n",
            "before loss data in training 0.067903071641922 0.06773546592989542\n",
            "after loss data in training 0.067903071641922 0.06773563591540457\n",
            "yyy epoch 986\n",
            "before loss data in training 0.04722645506262779 0.06773563591540457\n",
            "after loss data in training 0.04722645506262779 0.06771485660349699\n",
            "yyy epoch 987\n",
            "before loss data in training 0.07356715947389603 0.06771485660349699\n",
            "after loss data in training 0.07356715947389603 0.06772077998696906\n",
            "yyy epoch 988\n",
            "before loss data in training 0.05956445634365082 0.06772077998696906\n",
            "after loss data in training 0.05956445634365082 0.06771253294587369\n",
            "yyy epoch 989\n",
            "before loss data in training 0.06744574010372162 0.06771253294587369\n",
            "after loss data in training 0.06744574010372162 0.06771226345815434\n",
            "yyy epoch 990\n",
            "before loss data in training 0.0675937607884407 0.06771226345815434\n",
            "after loss data in training 0.0675937607884407 0.06771214387927471\n",
            "yyy epoch 991\n",
            "before loss data in training 0.06267808377742767 0.06771214387927471\n",
            "after loss data in training 0.06267808377742767 0.06770706922191398\n",
            "yyy epoch 992\n",
            "before loss data in training 0.05690143257379532 0.06770706922191398\n",
            "after loss data in training 0.05690143257379532 0.06769618741260067\n",
            "yyy epoch 993\n",
            "before loss data in training 0.05465201288461685 0.06769618741260067\n",
            "after loss data in training 0.05465201288461685 0.06768306450060069\n",
            "yyy epoch 994\n",
            "before loss data in training 0.04672731086611748 0.06768306450060069\n",
            "after loss data in training 0.04672731086611748 0.06766200344167156\n",
            "yyy epoch 995\n",
            "before loss data in training 0.056580934673547745 0.06766200344167156\n",
            "after loss data in training 0.056580934673547745 0.06765087787061923\n",
            "yyy epoch 996\n",
            "before loss data in training 0.06338085234165192 0.06765087787061923\n",
            "after loss data in training 0.06338085234165192 0.0676465949964678\n",
            "yyy epoch 997\n",
            "before loss data in training 0.05245974287390709 0.0676465949964678\n",
            "after loss data in training 0.05245974287390709 0.06763137770977184\n",
            "yyy epoch 998\n",
            "before loss data in training 0.07790829241275787 0.06763137770977184\n",
            "after loss data in training 0.07790829241275787 0.06764166491167674\n",
            "yyy epoch 999\n",
            "before loss data in training 0.048895031213760376 0.06764166491167674\n",
            "after loss data in training 0.048895031213760376 0.06762291827797882\n",
            "yyy epoch 1000\n",
            "before loss data in training 0.05446918308734894 0.06762291827797882\n",
            "after loss data in training 0.05446918308734894 0.06760977768338279\n",
            "yyy epoch 1001\n",
            "before loss data in training 0.06468221545219421 0.06760977768338279\n",
            "after loss data in training 0.06468221545219421 0.0676068559645892\n",
            "yyy epoch 1002\n",
            "before loss data in training 0.06006348505616188 0.0676068559645892\n",
            "after loss data in training 0.06006348505616188 0.06759933515610622\n",
            "yyy epoch 1003\n",
            "before loss data in training 0.057309817522764206 0.06759933515610622\n",
            "after loss data in training 0.057309817522764206 0.06758908663256703\n",
            "yyy epoch 1004\n",
            "before loss data in training 0.06711124628782272 0.06758908663256703\n",
            "after loss data in training 0.06711124628782272 0.06758861116953743\n",
            "yyy epoch 1005\n",
            "before loss data in training 0.07900133728981018 0.06758861116953743\n",
            "after loss data in training 0.07900133728981018 0.06759995582770868\n",
            "yyy epoch 1006\n",
            "before loss data in training 0.04451726749539375 0.06759995582770868\n",
            "after loss data in training 0.04451726749539375 0.06757703359500529\n",
            "yyy epoch 1007\n",
            "before loss data in training 0.06125708296895027 0.06757703359500529\n",
            "after loss data in training 0.06125708296895027 0.06757076380271754\n",
            "yyy epoch 1008\n",
            "before loss data in training 0.047396086156368256 0.06757076380271754\n",
            "after loss data in training 0.047396086156368256 0.06755076907759727\n",
            "yyy epoch 1009\n",
            "before loss data in training 0.06763734668493271 0.06755076907759727\n",
            "after loss data in training 0.06763734668493271 0.06755085479800058\n",
            "yyy epoch 1010\n",
            "before loss data in training 0.06372689455747604 0.06755085479800058\n",
            "after loss data in training 0.06372689455747604 0.06754707244365782\n",
            "yyy epoch 1011\n",
            "before loss data in training 0.055129777640104294 0.06754707244365782\n",
            "after loss data in training 0.055129777640104294 0.06753480238950411\n",
            "yyy epoch 1012\n",
            "before loss data in training 0.060491543263196945 0.06753480238950411\n",
            "after loss data in training 0.060491543263196945 0.06752784951771111\n",
            "yyy epoch 1013\n",
            "before loss data in training 0.06392184644937515 0.06752784951771111\n",
            "after loss data in training 0.06392184644937515 0.06752429330166738\n",
            "yyy epoch 1014\n",
            "before loss data in training 0.044937219470739365 0.06752429330166738\n",
            "after loss data in training 0.044937219470739365 0.06750204002695712\n",
            "yyy epoch 1015\n",
            "before loss data in training 0.0599927119910717 0.06750204002695712\n",
            "after loss data in training 0.0599927119910717 0.06749464895605566\n",
            "yyy epoch 1016\n",
            "before loss data in training 0.06073292717337608 0.06749464895605566\n",
            "after loss data in training 0.06073292717337608 0.06748800026207072\n",
            "yyy epoch 1017\n",
            "before loss data in training 0.053350988775491714 0.06748800026207072\n",
            "after loss data in training 0.053350988775491714 0.06747411321738842\n",
            "yyy epoch 1018\n",
            "before loss data in training 0.07138444483280182 0.06747411321738842\n",
            "after loss data in training 0.07138444483280182 0.06747795063801199\n",
            "yyy epoch 1019\n",
            "before loss data in training 0.052432022988796234 0.06747795063801199\n",
            "after loss data in training 0.052432022988796234 0.06746319972855197\n",
            "yyy epoch 1020\n",
            "before loss data in training 0.05271008610725403 0.06746319972855197\n",
            "after loss data in training 0.05271008610725403 0.06744875005801201\n",
            "yyy epoch 1021\n",
            "before loss data in training 0.06172953546047211 0.06744875005801201\n",
            "after loss data in training 0.06172953546047211 0.06744315395762303\n",
            "yyy epoch 1022\n",
            "before loss data in training 0.050357505679130554 0.06744315395762303\n",
            "after loss data in training 0.050357505679130554 0.06742645244415432\n",
            "yyy epoch 1023\n",
            "before loss data in training 0.07096761465072632 0.06742645244415432\n",
            "after loss data in training 0.07096761465072632 0.06742991061037168\n",
            "yyy epoch 1024\n",
            "before loss data in training 0.0647503212094307 0.06742991061037168\n",
            "after loss data in training 0.0647503212094307 0.06742729637680979\n",
            "yyy epoch 1025\n",
            "before loss data in training 0.05595344305038452 0.06742729637680979\n",
            "after loss data in training 0.05595344305038452 0.06741611328389904\n",
            "yyy epoch 1026\n",
            "before loss data in training 0.07345651090145111 0.06741611328389904\n",
            "after loss data in training 0.07345651090145111 0.06742199487846336\n",
            "yyy epoch 1027\n",
            "before loss data in training 0.05096859857439995 0.06742199487846336\n",
            "after loss data in training 0.05096859857439995 0.06740598962914034\n",
            "yyy epoch 1028\n",
            "before loss data in training 0.06555715948343277 0.06740598962914034\n",
            "after loss data in training 0.06555715948343277 0.06740419290402302\n",
            "yyy epoch 1029\n",
            "before loss data in training 0.05132325738668442 0.06740419290402302\n",
            "after loss data in training 0.05132325738668442 0.06738858034526833\n",
            "yyy epoch 1030\n",
            "before loss data in training 0.05789179727435112 0.06738858034526833\n",
            "after loss data in training 0.05789179727435112 0.06737936911047598\n",
            "yyy epoch 1031\n",
            "before loss data in training 0.061615314334630966 0.06737936911047598\n",
            "after loss data in training 0.061615314334630966 0.06737378378608079\n",
            "yyy epoch 1032\n",
            "before loss data in training 0.05779184028506279 0.06737378378608079\n",
            "after loss data in training 0.05779184028506279 0.06736450794532473\n",
            "yyy epoch 1033\n",
            "before loss data in training 0.058971013873815536 0.06736450794532473\n",
            "after loss data in training 0.058971013873815536 0.06735639044622269\n",
            "yyy epoch 1034\n",
            "before loss data in training 0.055823832750320435 0.06735639044622269\n",
            "after loss data in training 0.055823832750320435 0.06734524787840056\n",
            "yyy epoch 1035\n",
            "before loss data in training 0.05315147340297699 0.06734524787840056\n",
            "after loss data in training 0.05315147340297699 0.0673315473238876\n",
            "yyy epoch 1036\n",
            "before loss data in training 0.05243328958749771 0.0673315473238876\n",
            "after loss data in training 0.05243328958749771 0.06731718063368858\n",
            "yyy epoch 1037\n",
            "before loss data in training 0.06037083640694618 0.06731718063368858\n",
            "after loss data in training 0.06037083640694618 0.06731048858722737\n",
            "yyy epoch 1038\n",
            "before loss data in training 0.04446769505739212 0.06731048858722737\n",
            "after loss data in training 0.04446769505739212 0.06728850322290607\n",
            "yyy epoch 1039\n",
            "before loss data in training 0.07643577456474304 0.06728850322290607\n",
            "after loss data in training 0.07643577456474304 0.06729729867611937\n",
            "yyy epoch 1040\n",
            "before loss data in training 0.06205490231513977 0.06729729867611937\n",
            "after loss data in training 0.06205490231513977 0.06729226275262179\n",
            "yyy epoch 1041\n",
            "before loss data in training 0.049395009875297546 0.06729226275262179\n",
            "after loss data in training 0.049395009875297546 0.06727508688613683\n",
            "yyy epoch 1042\n",
            "before loss data in training 0.06211023032665253 0.06727508688613683\n",
            "after loss data in training 0.06211023032665253 0.06727013496230223\n",
            "yyy epoch 1043\n",
            "before loss data in training 0.054889533668756485 0.06727013496230223\n",
            "after loss data in training 0.054889533668756485 0.06725827614880267\n",
            "yyy epoch 1044\n",
            "before loss data in training 0.05614921450614929 0.06725827614880267\n",
            "after loss data in training 0.05614921450614929 0.06724764546780491\n",
            "yyy epoch 1045\n",
            "before loss data in training 0.06120899319648743 0.06724764546780491\n",
            "after loss data in training 0.06120899319648743 0.06724187237767937\n",
            "yyy epoch 1046\n",
            "before loss data in training 0.06401140987873077 0.06724187237767937\n",
            "after loss data in training 0.06401140987873077 0.06723878693116653\n",
            "yyy epoch 1047\n",
            "before loss data in training 0.05754094198346138 0.06723878693116653\n",
            "after loss data in training 0.05754094198346138 0.0672295332623233\n",
            "yyy epoch 1048\n",
            "before loss data in training 0.06031681224703789 0.0672295332623233\n",
            "after loss data in training 0.06031681224703789 0.06722294344248032\n",
            "yyy epoch 1049\n",
            "before loss data in training 0.054605718702077866 0.06722294344248032\n",
            "after loss data in training 0.054605718702077866 0.06721092703796565\n",
            "yyy epoch 1050\n",
            "before loss data in training 0.06106780096888542 0.06721092703796565\n",
            "after loss data in training 0.06106780096888542 0.06720508200840421\n",
            "yyy epoch 1051\n",
            "before loss data in training 0.062532939016819 0.06720508200840421\n",
            "after loss data in training 0.062532939016819 0.06720064080784187\n",
            "yyy epoch 1052\n",
            "before loss data in training 0.05224383994936943 0.06720064080784187\n",
            "after loss data in training 0.05224383994936943 0.06718643681842262\n",
            "yyy epoch 1053\n",
            "before loss data in training 0.05565540865063667 0.06718643681842262\n",
            "after loss data in training 0.05565540865063667 0.06717549656399398\n",
            "yyy epoch 1054\n",
            "before loss data in training 0.05593603104352951 0.06717549656399398\n",
            "after loss data in training 0.05593603104352951 0.06716484304217363\n",
            "yyy epoch 1055\n",
            "before loss data in training 0.04986574128270149 0.06716484304217363\n",
            "after loss data in training 0.04986574128270149 0.06714846131702261\n",
            "yyy epoch 1056\n",
            "before loss data in training 0.0427505299448967 0.06714846131702261\n",
            "after loss data in training 0.0427505299448967 0.06712537907352958\n",
            "yyy epoch 1057\n",
            "before loss data in training 0.05338544026017189 0.06712537907352958\n",
            "after loss data in training 0.05338544026017189 0.06711239236387613\n",
            "yyy epoch 1058\n",
            "before loss data in training 0.05593976005911827 0.06711239236387613\n",
            "after loss data in training 0.05593976005911827 0.0671018421917281\n",
            "yyy epoch 1059\n",
            "before loss data in training 0.05348758399486542 0.0671018421917281\n",
            "after loss data in training 0.05348758399486542 0.06708899855191974\n",
            "yyy epoch 1060\n",
            "before loss data in training 0.06522474437952042 0.06708899855191974\n",
            "after loss data in training 0.06522474437952042 0.06708724147918421\n",
            "yyy epoch 1061\n",
            "before loss data in training 0.08038366585969925 0.06708724147918421\n",
            "after loss data in training 0.08038366585969925 0.06709976165280052\n",
            "yyy epoch 1062\n",
            "before loss data in training 0.05847420543432236 0.06709976165280052\n",
            "after loss data in training 0.05847420543432236 0.06709164730076056\n",
            "yyy epoch 1063\n",
            "before loss data in training 0.058139074593782425 0.06709164730076056\n",
            "after loss data in training 0.058139074593782425 0.06708323322866754\n",
            "yyy epoch 1064\n",
            "before loss data in training 0.04381426423788071 0.06708323322866754\n",
            "after loss data in training 0.04381426423788071 0.06706138443149309\n",
            "yyy epoch 1065\n",
            "before loss data in training 0.05314255505800247 0.06706138443149309\n",
            "after loss data in training 0.05314255505800247 0.06704832736829094\n",
            "yyy epoch 1066\n",
            "before loss data in training 0.050379883497953415 0.06704832736829094\n",
            "after loss data in training 0.050379883497953415 0.0670327055839701\n",
            "yyy epoch 1067\n",
            "before loss data in training 0.05096385255455971 0.0670327055839701\n",
            "after loss data in training 0.05096385255455971 0.06701765984143321\n",
            "yyy epoch 1068\n",
            "before loss data in training 0.052642419934272766 0.06701765984143321\n",
            "after loss data in training 0.052642419934272766 0.06700421247014494\n",
            "yyy epoch 1069\n",
            "before loss data in training 0.06059841066598892 0.06700421247014494\n",
            "after loss data in training 0.06059841066598892 0.06699822573948686\n",
            "yyy epoch 1070\n",
            "before loss data in training 0.0654570609331131 0.06699822573948686\n",
            "after loss data in training 0.0654570609331131 0.06699678674340247\n",
            "yyy epoch 1071\n",
            "before loss data in training 0.05421082302927971 0.06699678674340247\n",
            "after loss data in training 0.05421082302927971 0.06698485953844527\n",
            "yyy epoch 1072\n",
            "before loss data in training 0.045539021492004395 0.06698485953844527\n",
            "after loss data in training 0.045539021492004395 0.06696487273691085\n",
            "yyy epoch 1073\n",
            "before loss data in training 0.06402416527271271 0.06696487273691085\n",
            "after loss data in training 0.06402416527271271 0.06696213464802425\n",
            "yyy epoch 1074\n",
            "before loss data in training 0.06289276480674744 0.06696213464802425\n",
            "after loss data in training 0.06289276480674744 0.06695834918770678\n",
            "yyy epoch 1075\n",
            "before loss data in training 0.04691804572939873 0.06695834918770678\n",
            "after loss data in training 0.04691804572939873 0.06693972437036634\n",
            "yyy epoch 1076\n",
            "before loss data in training 0.06708671152591705 0.06693972437036634\n",
            "after loss data in training 0.06708671152591705 0.0669398608486909\n",
            "yyy epoch 1077\n",
            "before loss data in training 0.06033828854560852 0.0669398608486909\n",
            "after loss data in training 0.06033828854560852 0.06693373694117412\n",
            "yyy epoch 1078\n",
            "before loss data in training 0.05604752525687218 0.06693373694117412\n",
            "after loss data in training 0.05604752525687218 0.06692364777371879\n",
            "yyy epoch 1079\n",
            "before loss data in training 0.05289037525653839 0.06692364777371879\n",
            "after loss data in training 0.05289037525653839 0.06691065400286955\n",
            "yyy epoch 1080\n",
            "before loss data in training 0.06566272675991058 0.06691065400286955\n",
            "after loss data in training 0.06566272675991058 0.06690949958358837\n",
            "yyy epoch 1081\n",
            "before loss data in training 0.05994953215122223 0.06690949958358837\n",
            "after loss data in training 0.05994953215122223 0.06690306708134035\n",
            "yyy epoch 1082\n",
            "before loss data in training 0.07341428101062775 0.06690306708134035\n",
            "after loss data in training 0.07341428101062775 0.06690907928256776\n",
            "yyy epoch 1083\n",
            "before loss data in training 0.06874321401119232 0.06690907928256776\n",
            "after loss data in training 0.06874321401119232 0.06691077128877497\n",
            "yyy epoch 1084\n",
            "before loss data in training 0.058781806379556656 0.06691077128877497\n",
            "after loss data in training 0.058781806379556656 0.06690327915521808\n",
            "yyy epoch 1085\n",
            "before loss data in training 0.04187323525547981 0.06690327915521808\n",
            "after loss data in training 0.04187323525547981 0.06688023123265847\n",
            "yyy epoch 1086\n",
            "before loss data in training 0.06455489248037338 0.06688023123265847\n",
            "after loss data in training 0.06455489248037338 0.06687809200657542\n",
            "yyy epoch 1087\n",
            "before loss data in training 0.06205017864704132 0.06687809200657542\n",
            "after loss data in training 0.06205017864704132 0.0668736545862082\n",
            "yyy epoch 1088\n",
            "before loss data in training 0.043230168521404266 0.0668736545862082\n",
            "after loss data in training 0.043230168521404266 0.06685194339606605\n",
            "yyy epoch 1089\n",
            "before loss data in training 0.05533628165721893 0.06685194339606605\n",
            "after loss data in training 0.05533628165721893 0.06684137856878271\n",
            "yyy epoch 1090\n",
            "before loss data in training 0.07558789849281311 0.06684137856878271\n",
            "after loss data in training 0.07558789849281311 0.06684939554396514\n",
            "yyy epoch 1091\n",
            "before loss data in training 0.04181278496980667 0.06684939554396514\n",
            "after loss data in training 0.04181278496980667 0.06682646824490455\n",
            "yyy epoch 1092\n",
            "before loss data in training 0.05052034929394722 0.06682646824490455\n",
            "after loss data in training 0.05052034929394722 0.06681154956333918\n",
            "yyy epoch 1093\n",
            "before loss data in training 0.044251181185245514 0.06681154956333918\n",
            "after loss data in training 0.044251181185245514 0.06679092765440126\n",
            "yyy epoch 1094\n",
            "before loss data in training 0.047872889786958694 0.06679092765440126\n",
            "after loss data in training 0.047872889786958694 0.06677365090749035\n",
            "yyy epoch 1095\n",
            "before loss data in training 0.053033266216516495 0.06677365090749035\n",
            "after loss data in training 0.053033266216516495 0.06676111406014457\n",
            "yyy epoch 1096\n",
            "before loss data in training 0.06259828805923462 0.06676111406014457\n",
            "after loss data in training 0.06259828805923462 0.0667573193235895\n",
            "yyy epoch 1097\n",
            "before loss data in training 0.05567488819360733 0.0667573193235895\n",
            "after loss data in training 0.05567488819360733 0.06674722603476438\n",
            "yyy epoch 1098\n",
            "before loss data in training 0.048531271517276764 0.06674722603476438\n",
            "after loss data in training 0.048531271517276764 0.06673065100790589\n",
            "yyy epoch 1099\n",
            "before loss data in training 0.066738560795784 0.06673065100790589\n",
            "after loss data in training 0.066738560795784 0.06673065819862214\n",
            "yyy epoch 1100\n",
            "before loss data in training 0.05047715827822685 0.06673065819862214\n",
            "after loss data in training 0.05047715827822685 0.06671589571004777\n",
            "yyy epoch 1101\n",
            "before loss data in training 0.06577109545469284 0.06671589571004777\n",
            "after loss data in training 0.06577109545469284 0.06671503835954382\n",
            "yyy epoch 1102\n",
            "before loss data in training 0.037966519594192505 0.06671503835954382\n",
            "after loss data in training 0.037966519594192505 0.06668897442593968\n",
            "yyy epoch 1103\n",
            "before loss data in training 0.05458175390958786 0.06668897442593968\n",
            "after loss data in training 0.05458175390958786 0.06667800774068937\n",
            "yyy epoch 1104\n",
            "before loss data in training 0.06116822361946106 0.06667800774068937\n",
            "after loss data in training 0.06116822361946106 0.0666730215107154\n",
            "yyy epoch 1105\n",
            "before loss data in training 0.05718189850449562 0.0666730215107154\n",
            "after loss data in training 0.05718189850449562 0.06666444002517632\n",
            "yyy epoch 1106\n",
            "before loss data in training 0.06666380167007446 0.06666444002517632\n",
            "after loss data in training 0.06666380167007446 0.06666443944852311\n",
            "yyy epoch 1107\n",
            "before loss data in training 0.05706850811839104 0.06666443944852311\n",
            "after loss data in training 0.05706850811839104 0.06665577886068004\n",
            "yyy epoch 1108\n",
            "before loss data in training 0.03713211044669151 0.06665577886068004\n",
            "after loss data in training 0.03713211044669151 0.06662915697752947\n",
            "yyy epoch 1109\n",
            "before loss data in training 0.056985341012477875 0.06662915697752947\n",
            "after loss data in training 0.056985341012477875 0.06662046885503843\n",
            "yyy epoch 1110\n",
            "before loss data in training 0.03667822480201721 0.06662046885503843\n",
            "after loss data in training 0.03667822480201721 0.06659351814031925\n",
            "yyy epoch 1111\n",
            "before loss data in training 0.06202714890241623 0.06659351814031925\n",
            "after loss data in training 0.06202714890241623 0.06658941169316286\n",
            "yyy epoch 1112\n",
            "before loss data in training 0.08189617842435837 0.06658941169316286\n",
            "after loss data in training 0.08189617842435837 0.06660316440361318\n",
            "yyy epoch 1113\n",
            "before loss data in training 0.05828213691711426 0.06660316440361318\n",
            "after loss data in training 0.05828213691711426 0.0665956948995858\n",
            "yyy epoch 1114\n",
            "before loss data in training 0.04178370535373688 0.0665956948995858\n",
            "after loss data in training 0.04178370535373688 0.06657344199416353\n",
            "yyy epoch 1115\n",
            "before loss data in training 0.06390034407377243 0.06657344199416353\n",
            "after loss data in training 0.06390034407377243 0.06657104674513092\n",
            "yyy epoch 1116\n",
            "before loss data in training 0.06216263398528099 0.06657104674513092\n",
            "after loss data in training 0.06216263398528099 0.0665671000909144\n",
            "yyy epoch 1117\n",
            "before loss data in training 0.06321381777524948 0.0665671000909144\n",
            "after loss data in training 0.06321381777524948 0.06656410073285031\n",
            "yyy epoch 1118\n",
            "before loss data in training 0.05203891545534134 0.06656410073285031\n",
            "after loss data in training 0.05203891545534134 0.06655112022768721\n",
            "yyy epoch 1119\n",
            "before loss data in training 0.04952574893832207 0.06655112022768721\n",
            "after loss data in training 0.04952574893832207 0.0665359190033217\n",
            "yyy epoch 1120\n",
            "before loss data in training 0.06098422035574913 0.0665359190033217\n",
            "after loss data in training 0.06098422035574913 0.06653096655136133\n",
            "yyy epoch 1121\n",
            "before loss data in training 0.04191301763057709 0.06653096655136133\n",
            "after loss data in training 0.04191301763057709 0.06650902542041588\n",
            "yyy epoch 1122\n",
            "before loss data in training 0.05309603363275528 0.06650902542041588\n",
            "after loss data in training 0.05309603363275528 0.0664970815274616\n",
            "yyy epoch 1123\n",
            "before loss data in training 0.05380922928452492 0.0664970815274616\n",
            "after loss data in training 0.05380922928452492 0.0664857934026903\n",
            "yyy epoch 1124\n",
            "before loss data in training 0.058001916855573654 0.0664857934026903\n",
            "after loss data in training 0.058001916855573654 0.06647825217909287\n",
            "yyy epoch 1125\n",
            "before loss data in training 0.046408358961343765 0.06647825217909287\n",
            "after loss data in training 0.046408358961343765 0.06646042811762062\n",
            "yyy epoch 1126\n",
            "before loss data in training 0.0493306890130043 0.06646042811762062\n",
            "after loss data in training 0.0493306890130043 0.06644522870404067\n",
            "yyy epoch 1127\n",
            "before loss data in training 0.0559234581887722 0.06644522870404067\n",
            "after loss data in training 0.0559234581887722 0.06643590089330018\n",
            "yyy epoch 1128\n",
            "before loss data in training 0.05086684226989746 0.06643590089330018\n",
            "after loss data in training 0.05086684226989746 0.06642211076165855\n",
            "yyy epoch 1129\n",
            "before loss data in training 0.05598830059170723 0.06642211076165855\n",
            "after loss data in training 0.05598830059170723 0.06641287730133116\n",
            "yyy epoch 1130\n",
            "before loss data in training 0.05822846293449402 0.06641287730133116\n",
            "after loss data in training 0.05822846293449402 0.06640564086068851\n",
            "yyy epoch 1131\n",
            "before loss data in training 0.05348123610019684 0.06640564086068851\n",
            "after loss data in training 0.05348123610019684 0.06639422354199549\n",
            "yyy epoch 1132\n",
            "before loss data in training 0.055145032703876495 0.06639422354199549\n",
            "after loss data in training 0.055145032703876495 0.06638429486517455\n",
            "yyy epoch 1133\n",
            "before loss data in training 0.06171460822224617 0.06638429486517455\n",
            "after loss data in training 0.06171460822224617 0.06638017697571871\n",
            "yyy epoch 1134\n",
            "before loss data in training 0.05110654607415199 0.06638017697571871\n",
            "after loss data in training 0.05110654607415199 0.0663667200321931\n",
            "yyy epoch 1135\n",
            "before loss data in training 0.04658792167901993 0.0663667200321931\n",
            "after loss data in training 0.04658792167901993 0.06634930911814982\n",
            "yyy epoch 1136\n",
            "before loss data in training 0.06370016187429428 0.06634930911814982\n",
            "after loss data in training 0.06370016187429428 0.06634697917334431\n",
            "yyy epoch 1137\n",
            "before loss data in training 0.07507984340190887 0.06634697917334431\n",
            "after loss data in training 0.07507984340190887 0.06635465304349243\n",
            "yyy epoch 1138\n",
            "before loss data in training 0.05358649417757988 0.06635465304349243\n",
            "after loss data in training 0.05358649417757988 0.06634344307082701\n",
            "yyy epoch 1139\n",
            "before loss data in training 0.04681625962257385 0.06634344307082701\n",
            "after loss data in training 0.04681625962257385 0.06632631396253907\n",
            "yyy epoch 1140\n",
            "before loss data in training 0.06429640203714371 0.06632631396253907\n",
            "after loss data in training 0.06429640203714371 0.0663245348986255\n",
            "yyy epoch 1141\n",
            "before loss data in training 0.054217129945755005 0.0663245348986255\n",
            "after loss data in training 0.054217129945755005 0.06631393296784364\n",
            "yyy epoch 1142\n",
            "before loss data in training 0.04746845364570618 0.06631393296784364\n",
            "after loss data in training 0.04746845364570618 0.06629744523440345\n",
            "yyy epoch 1143\n",
            "before loss data in training 0.06140891835093498 0.06629744523440345\n",
            "after loss data in training 0.06140891835093498 0.06629317204656825\n",
            "yyy epoch 1144\n",
            "before loss data in training 0.06122389808297157 0.06629317204656825\n",
            "after loss data in training 0.06122389808297157 0.06628874473306293\n",
            "yyy epoch 1145\n",
            "before loss data in training 0.05455473065376282 0.06628874473306293\n",
            "after loss data in training 0.05455473065376282 0.06627850562828169\n",
            "yyy epoch 1146\n",
            "before loss data in training 0.06346902996301651 0.06627850562828169\n",
            "after loss data in training 0.06346902996301651 0.06627605621619341\n",
            "yyy epoch 1147\n",
            "before loss data in training 0.058261457830667496 0.06627605621619341\n",
            "after loss data in training 0.058261457830667496 0.06626907485871474\n",
            "yyy epoch 1148\n",
            "before loss data in training 0.05757089704275131 0.06626907485871474\n",
            "after loss data in training 0.05757089704275131 0.06626150464303504\n",
            "yyy epoch 1149\n",
            "before loss data in training 0.06125704571604729 0.06626150464303504\n",
            "after loss data in training 0.06125704571604729 0.06625715293962027\n",
            "yyy epoch 1150\n",
            "before loss data in training 0.07018189877271652 0.06625715293962027\n",
            "after loss data in training 0.07018189877271652 0.06626056279699047\n",
            "yyy epoch 1151\n",
            "before loss data in training 0.05995143949985504 0.06626056279699047\n",
            "after loss data in training 0.05995143949985504 0.0662550861274617\n",
            "yyy epoch 1152\n",
            "before loss data in training 0.055558767169713974 0.0662550861274617\n",
            "after loss data in training 0.055558767169713974 0.06624580918127111\n",
            "yyy epoch 1153\n",
            "before loss data in training 0.060577020049095154 0.06624580918127111\n",
            "after loss data in training 0.060577020049095154 0.06624089688566263\n",
            "yyy epoch 1154\n",
            "before loss data in training 0.05159280449151993 0.06624089688566263\n",
            "after loss data in training 0.05159280449151993 0.06622821455458545\n",
            "yyy epoch 1155\n",
            "before loss data in training 0.058809470385313034 0.06622821455458545\n",
            "after loss data in training 0.058809470385313034 0.06622179695582311\n",
            "yyy epoch 1156\n",
            "before loss data in training 0.04842764884233475 0.06622179695582311\n",
            "after loss data in training 0.04842764884233475 0.0662064173982488\n",
            "yyy epoch 1157\n",
            "before loss data in training 0.051667213439941406 0.0662064173982488\n",
            "after loss data in training 0.051667213439941406 0.06619386195441607\n",
            "yyy epoch 1158\n",
            "before loss data in training 0.05207493528723717 0.06619386195441607\n",
            "after loss data in training 0.05207493528723717 0.06618167996419418\n",
            "yyy epoch 1159\n",
            "before loss data in training 0.05885737016797066 0.06618167996419418\n",
            "after loss data in training 0.05885737016797066 0.06617536590402502\n",
            "yyy epoch 1160\n",
            "before loss data in training 0.06795608252286911 0.06617536590402502\n",
            "after loss data in training 0.06795608252286911 0.06617689968233582\n",
            "yyy epoch 1161\n",
            "before loss data in training 0.06505222618579865 0.06617689968233582\n",
            "after loss data in training 0.06505222618579865 0.0661759318049722\n",
            "yyy epoch 1162\n",
            "before loss data in training 0.06262639909982681 0.0661759318049722\n",
            "after loss data in training 0.06262639909982681 0.06617287975621454\n",
            "############# Epoch 2: Training End     #############\n",
            "############# Epoch 2: Validation Start   #############\n",
            "############# Epoch 2: Validation End     #############\n",
            "before cal avg train loss 0.06617287975621454\n",
            "Epoch: 2 \tAvgerage Training Loss: 0.000057 \tAverage Validation Loss: 0.000214\n",
            "Validation loss decreased (0.000281 --> 0.000214).  Saving model ...\n",
            "############# Epoch 2  Done   #############\n",
            "\n",
            "############# Epoch 3: Training Start   #############\n",
            "yyy epoch 0\n",
            "Epoch: 3, Training Loss:  0.0686771497130394\n",
            "before loss data in training 0.0686771497130394 0\n",
            "after loss data in training 0.0686771497130394 0.0686771497130394\n",
            "yyy epoch 1\n",
            "before loss data in training 0.04793592542409897 0.0686771497130394\n",
            "after loss data in training 0.04793592542409897 0.05830653756856918\n",
            "yyy epoch 2\n",
            "before loss data in training 0.07245491445064545 0.05830653756856918\n",
            "after loss data in training 0.07245491445064545 0.06302266319592793\n",
            "yyy epoch 3\n",
            "before loss data in training 0.053923387080430984 0.06302266319592793\n",
            "after loss data in training 0.053923387080430984 0.0607478441670537\n",
            "yyy epoch 4\n",
            "before loss data in training 0.07154017686843872 0.0607478441670537\n",
            "after loss data in training 0.07154017686843872 0.0629063107073307\n",
            "yyy epoch 5\n",
            "before loss data in training 0.06558791548013687 0.0629063107073307\n",
            "after loss data in training 0.06558791548013687 0.06335324483613172\n",
            "yyy epoch 6\n",
            "before loss data in training 0.047563791275024414 0.06335324483613172\n",
            "after loss data in training 0.047563791275024414 0.061097608613116396\n",
            "yyy epoch 7\n",
            "before loss data in training 0.06485626846551895 0.061097608613116396\n",
            "after loss data in training 0.06485626846551895 0.06156744109466671\n",
            "yyy epoch 8\n",
            "before loss data in training 0.057304445654153824 0.06156744109466671\n",
            "after loss data in training 0.057304445654153824 0.061093774934609726\n",
            "yyy epoch 9\n",
            "before loss data in training 0.06145239993929863 0.061093774934609726\n",
            "after loss data in training 0.06145239993929863 0.06112963743507862\n",
            "yyy epoch 10\n",
            "before loss data in training 0.08318168669939041 0.06112963743507862\n",
            "after loss data in training 0.08318168669939041 0.06313436918637969\n",
            "yyy epoch 11\n",
            "before loss data in training 0.06396690011024475 0.06313436918637969\n",
            "after loss data in training 0.06396690011024475 0.06320374676336844\n",
            "yyy epoch 12\n",
            "before loss data in training 0.043234650045633316 0.06320374676336844\n",
            "after loss data in training 0.043234650045633316 0.06166766240046574\n",
            "yyy epoch 13\n",
            "before loss data in training 0.03612704947590828 0.06166766240046574\n",
            "after loss data in training 0.03612704947590828 0.059843332905854495\n",
            "yyy epoch 14\n",
            "before loss data in training 0.04637935385107994 0.059843332905854495\n",
            "after loss data in training 0.04637935385107994 0.05894573430220286\n",
            "yyy epoch 15\n",
            "before loss data in training 0.06778760254383087 0.05894573430220286\n",
            "after loss data in training 0.06778760254383087 0.05949835106730461\n",
            "yyy epoch 16\n",
            "before loss data in training 0.05921662226319313 0.05949835106730461\n",
            "after loss data in training 0.05921662226319313 0.05948177878470982\n",
            "yyy epoch 17\n",
            "before loss data in training 0.053925637155771255 0.05948177878470982\n",
            "after loss data in training 0.053925637155771255 0.059173104249768786\n",
            "yyy epoch 18\n",
            "before loss data in training 0.06465816497802734 0.059173104249768786\n",
            "after loss data in training 0.06465816497802734 0.059461791656519235\n",
            "yyy epoch 19\n",
            "before loss data in training 0.04515731334686279 0.059461791656519235\n",
            "after loss data in training 0.04515731334686279 0.05874656774103641\n",
            "yyy epoch 20\n",
            "before loss data in training 0.06325270980596542 0.05874656774103641\n",
            "after loss data in training 0.06325270980596542 0.05896114593460446\n",
            "yyy epoch 21\n",
            "before loss data in training 0.057918813079595566 0.05896114593460446\n",
            "after loss data in training 0.057918813079595566 0.0589137671684677\n",
            "yyy epoch 22\n",
            "before loss data in training 0.05990217998623848 0.0589137671684677\n",
            "after loss data in training 0.05990217998623848 0.05895674163880556\n",
            "yyy epoch 23\n",
            "before loss data in training 0.05448506399989128 0.05895674163880556\n",
            "after loss data in training 0.05448506399989128 0.05877042173718413\n",
            "yyy epoch 24\n",
            "before loss data in training 0.05852130055427551 0.05877042173718413\n",
            "after loss data in training 0.05852130055427551 0.058760456889867786\n",
            "yyy epoch 25\n",
            "before loss data in training 0.052187491208314896 0.058760456889867786\n",
            "after loss data in training 0.052187491208314896 0.05850765051750037\n",
            "yyy epoch 26\n",
            "before loss data in training 0.05023146793246269 0.05850765051750037\n",
            "after loss data in training 0.05023146793246269 0.05820112523657305\n",
            "yyy epoch 27\n",
            "before loss data in training 0.05998258292675018 0.05820112523657305\n",
            "after loss data in training 0.05998258292675018 0.05826474872550795\n",
            "yyy epoch 28\n",
            "before loss data in training 0.06811892241239548 0.05826474872550795\n",
            "after loss data in training 0.06811892241239548 0.05860454781815924\n",
            "yyy epoch 29\n",
            "before loss data in training 0.05050459876656532 0.05860454781815924\n",
            "after loss data in training 0.05050459876656532 0.058334549516439446\n",
            "yyy epoch 30\n",
            "before loss data in training 0.03782801330089569 0.058334549516439446\n",
            "after loss data in training 0.03782801330089569 0.0576730483481961\n",
            "yyy epoch 31\n",
            "before loss data in training 0.05036526545882225 0.0576730483481961\n",
            "after loss data in training 0.05036526545882225 0.057444680132903166\n",
            "yyy epoch 32\n",
            "before loss data in training 0.05736631155014038 0.057444680132903166\n",
            "after loss data in training 0.05736631155014038 0.0574423053273649\n",
            "yyy epoch 33\n",
            "before loss data in training 0.05518391355872154 0.0574423053273649\n",
            "after loss data in training 0.05518391355872154 0.05737588204005186\n",
            "yyy epoch 34\n",
            "before loss data in training 0.056314677000045776 0.05737588204005186\n",
            "after loss data in training 0.056314677000045776 0.05734556189605169\n",
            "yyy epoch 35\n",
            "before loss data in training 0.05034498870372772 0.05734556189605169\n",
            "after loss data in training 0.05034498870372772 0.05715110152959824\n",
            "yyy epoch 36\n",
            "before loss data in training 0.061107978224754333 0.05715110152959824\n",
            "after loss data in training 0.061107978224754333 0.05725804414298084\n",
            "yyy epoch 37\n",
            "before loss data in training 0.05858445167541504 0.05725804414298084\n",
            "after loss data in training 0.05858445167541504 0.05729294960436069\n",
            "yyy epoch 38\n",
            "before loss data in training 0.06555957347154617 0.05729294960436069\n",
            "after loss data in training 0.06555957347154617 0.05750491431890391\n",
            "yyy epoch 39\n",
            "before loss data in training 0.03913189843297005 0.05750491431890391\n",
            "after loss data in training 0.03913189843297005 0.05704558892175556\n",
            "yyy epoch 40\n",
            "before loss data in training 0.06004970520734787 0.05704558892175556\n",
            "after loss data in training 0.06004970520734787 0.057118860050672446\n",
            "yyy epoch 41\n",
            "before loss data in training 0.06435022503137589 0.057118860050672446\n",
            "after loss data in training 0.06435022503137589 0.05729103540735586\n",
            "yyy epoch 42\n",
            "before loss data in training 0.053869567811489105 0.05729103540735586\n",
            "after loss data in training 0.053869567811489105 0.05721146639349849\n",
            "yyy epoch 43\n",
            "before loss data in training 0.059515610337257385 0.05721146639349849\n",
            "after loss data in training 0.059515610337257385 0.0572638333013112\n",
            "yyy epoch 44\n",
            "before loss data in training 0.059030186384916306 0.0572638333013112\n",
            "after loss data in training 0.059030186384916306 0.057303085592057976\n",
            "yyy epoch 45\n",
            "before loss data in training 0.061832867562770844 0.057303085592057976\n",
            "after loss data in training 0.061832867562770844 0.05740155911316043\n",
            "yyy epoch 46\n",
            "before loss data in training 0.07151228189468384 0.05740155911316043\n",
            "after loss data in training 0.07151228189468384 0.057701787257448164\n",
            "yyy epoch 47\n",
            "before loss data in training 0.05144709348678589 0.057701787257448164\n",
            "after loss data in training 0.05144709348678589 0.05757148113722604\n",
            "yyy epoch 48\n",
            "before loss data in training 0.06216878816485405 0.05757148113722604\n",
            "after loss data in training 0.06216878816485405 0.05766530372962661\n",
            "yyy epoch 49\n",
            "before loss data in training 0.05698150396347046 0.05766530372962661\n",
            "after loss data in training 0.05698150396347046 0.05765162773430349\n",
            "yyy epoch 50\n",
            "before loss data in training 0.0436369962990284 0.05765162773430349\n",
            "after loss data in training 0.0436369962990284 0.057376831039494175\n",
            "yyy epoch 51\n",
            "before loss data in training 0.058506183326244354 0.057376831039494175\n",
            "after loss data in training 0.058506183326244354 0.05739854935270091\n",
            "yyy epoch 52\n",
            "before loss data in training 0.05705755203962326 0.05739854935270091\n",
            "after loss data in training 0.05705755203962326 0.05739211544113341\n",
            "yyy epoch 53\n",
            "before loss data in training 0.06632766127586365 0.05739211544113341\n",
            "after loss data in training 0.06632766127586365 0.05755758851214693\n",
            "yyy epoch 54\n",
            "before loss data in training 0.051715001463890076 0.05755758851214693\n",
            "after loss data in training 0.051715001463890076 0.05745135965672408\n",
            "yyy epoch 55\n",
            "before loss data in training 0.06940694153308868 0.05745135965672408\n",
            "after loss data in training 0.06940694153308868 0.05766485219023059\n",
            "yyy epoch 56\n",
            "before loss data in training 0.046767376363277435 0.05766485219023059\n",
            "after loss data in training 0.046767376363277435 0.05747366840379282\n",
            "yyy epoch 57\n",
            "before loss data in training 0.06029362604022026 0.05747366840379282\n",
            "after loss data in training 0.06029362604022026 0.05752228836304157\n",
            "yyy epoch 58\n",
            "before loss data in training 0.05680864304304123 0.05752228836304157\n",
            "after loss data in training 0.05680864304304123 0.05751019267965173\n",
            "yyy epoch 59\n",
            "before loss data in training 0.05331885442137718 0.05751019267965173\n",
            "after loss data in training 0.05331885442137718 0.057440337042013824\n",
            "yyy epoch 60\n",
            "before loss data in training 0.05257720872759819 0.057440337042013824\n",
            "after loss data in training 0.05257720872759819 0.0573606136270234\n",
            "yyy epoch 61\n",
            "before loss data in training 0.04848447069525719 0.0573606136270234\n",
            "after loss data in training 0.04848447069525719 0.05721745003134975\n",
            "yyy epoch 62\n",
            "before loss data in training 0.05793532356619835 0.05721745003134975\n",
            "after loss data in training 0.05793532356619835 0.05722884484936322\n",
            "yyy epoch 63\n",
            "before loss data in training 0.056837473064661026 0.05722884484936322\n",
            "after loss data in training 0.056837473064661026 0.05722272966522725\n",
            "yyy epoch 64\n",
            "before loss data in training 0.04980245977640152 0.05722272966522725\n",
            "after loss data in training 0.04980245977640152 0.05710857166693762\n",
            "yyy epoch 65\n",
            "before loss data in training 0.057852305471897125 0.05710857166693762\n",
            "after loss data in training 0.057852305471897125 0.057119840360952155\n",
            "yyy epoch 66\n",
            "before loss data in training 0.06701469421386719 0.057119840360952155\n",
            "after loss data in training 0.06701469421386719 0.05726752474681656\n",
            "yyy epoch 67\n",
            "before loss data in training 0.06115734949707985 0.05726752474681656\n",
            "after loss data in training 0.06115734949707985 0.05732472805196749\n",
            "yyy epoch 68\n",
            "before loss data in training 0.07792641967535019 0.05732472805196749\n",
            "after loss data in training 0.07792641967535019 0.057623303292886084\n",
            "yyy epoch 69\n",
            "before loss data in training 0.04838830232620239 0.057623303292886084\n",
            "after loss data in training 0.04838830232620239 0.057491374707647744\n",
            "yyy epoch 70\n",
            "before loss data in training 0.05799710378050804 0.057491374707647744\n",
            "after loss data in training 0.05799710378050804 0.05749849765233592\n",
            "yyy epoch 71\n",
            "before loss data in training 0.05881567671895027 0.05749849765233592\n",
            "after loss data in training 0.05881567671895027 0.05751679180603889\n",
            "yyy epoch 72\n",
            "before loss data in training 0.06228434666991234 0.05751679180603889\n",
            "after loss data in training 0.06228434666991234 0.05758210077677688\n",
            "yyy epoch 73\n",
            "before loss data in training 0.07086332142353058 0.05758210077677688\n",
            "after loss data in training 0.07086332142353058 0.057761576731462747\n",
            "yyy epoch 74\n",
            "before loss data in training 0.040046099573373795 0.057761576731462747\n",
            "after loss data in training 0.040046099573373795 0.05752537036935489\n",
            "yyy epoch 75\n",
            "before loss data in training 0.04948985204100609 0.05752537036935489\n",
            "after loss data in training 0.04948985204100609 0.05741963986503451\n",
            "yyy epoch 76\n",
            "before loss data in training 0.06619385629892349 0.05741963986503451\n",
            "after loss data in training 0.06619385629892349 0.05753359072781229\n",
            "yyy epoch 77\n",
            "before loss data in training 0.07839219272136688 0.05753359072781229\n",
            "after loss data in training 0.07839219272136688 0.05780100870208863\n",
            "yyy epoch 78\n",
            "before loss data in training 0.050380393862724304 0.05780100870208863\n",
            "after loss data in training 0.050380393862724304 0.05770707686867896\n",
            "yyy epoch 79\n",
            "before loss data in training 0.04901528358459473 0.05770707686867896\n",
            "after loss data in training 0.04901528358459473 0.05759842945262791\n",
            "yyy epoch 80\n",
            "before loss data in training 0.05597279220819473 0.05759842945262791\n",
            "after loss data in training 0.05597279220819473 0.05757835985701762\n",
            "yyy epoch 81\n",
            "before loss data in training 0.049556463956832886 0.05757835985701762\n",
            "after loss data in training 0.049556463956832886 0.05748053185823488\n",
            "yyy epoch 82\n",
            "before loss data in training 0.0530136376619339 0.05748053185823488\n",
            "after loss data in training 0.0530136376619339 0.05742671385586981\n",
            "yyy epoch 83\n",
            "before loss data in training 0.050561681389808655 0.05742671385586981\n",
            "after loss data in training 0.050561681389808655 0.05734498727889289\n",
            "yyy epoch 84\n",
            "before loss data in training 0.04447690770030022 0.05734498727889289\n",
            "after loss data in training 0.04447690770030022 0.057193598107380034\n",
            "yyy epoch 85\n",
            "before loss data in training 0.06199543550610542 0.057193598107380034\n",
            "after loss data in training 0.06199543550610542 0.057249433425969866\n",
            "yyy epoch 86\n",
            "before loss data in training 0.0587681382894516 0.057249433425969866\n",
            "after loss data in training 0.0587681382894516 0.05726688980371104\n",
            "yyy epoch 87\n",
            "before loss data in training 0.05606578662991524 0.05726688980371104\n",
            "after loss data in training 0.05606578662991524 0.05725324090400881\n",
            "yyy epoch 88\n",
            "before loss data in training 0.05812434107065201 0.05725324090400881\n",
            "after loss data in training 0.05812434107065201 0.05726302854633064\n",
            "yyy epoch 89\n",
            "before loss data in training 0.042321719229221344 0.05726302854633064\n",
            "after loss data in training 0.042321719229221344 0.05709701399836276\n",
            "yyy epoch 90\n",
            "before loss data in training 0.05452722683548927 0.05709701399836276\n",
            "after loss data in training 0.05452722683548927 0.057068774578990525\n",
            "yyy epoch 91\n",
            "before loss data in training 0.05535396188497543 0.057068774578990525\n",
            "after loss data in training 0.05535396188497543 0.057050135310577316\n",
            "yyy epoch 92\n",
            "before loss data in training 0.06861242651939392 0.057050135310577316\n",
            "after loss data in training 0.06861242651939392 0.057174461022500074\n",
            "yyy epoch 93\n",
            "before loss data in training 0.04613993316888809 0.057174461022500074\n",
            "after loss data in training 0.04613993316888809 0.05705707242831271\n",
            "yyy epoch 94\n",
            "before loss data in training 0.0431765578687191 0.05705707242831271\n",
            "after loss data in training 0.0431765578687191 0.05691096174873804\n",
            "yyy epoch 95\n",
            "before loss data in training 0.05827396363019943 0.05691096174873804\n",
            "after loss data in training 0.05827396363019943 0.056925159685003265\n",
            "yyy epoch 96\n",
            "before loss data in training 0.053686369210481644 0.056925159685003265\n",
            "after loss data in training 0.053686369210481644 0.056891770092482426\n",
            "yyy epoch 97\n",
            "before loss data in training 0.04745921120047569 0.056891770092482426\n",
            "after loss data in training 0.04745921120047569 0.05679551949154358\n",
            "yyy epoch 98\n",
            "before loss data in training 0.04681447148323059 0.05679551949154358\n",
            "after loss data in training 0.04681447148323059 0.05669470082479294\n",
            "yyy epoch 99\n",
            "before loss data in training 0.049670036882162094 0.05669470082479294\n",
            "after loss data in training 0.049670036882162094 0.056624454185366636\n",
            "yyy epoch 100\n",
            "before loss data in training 0.046770162880420685 0.056624454185366636\n",
            "after loss data in training 0.046770162880420685 0.05652688694472361\n",
            "yyy epoch 101\n",
            "before loss data in training 0.056247539818286896 0.05652688694472361\n",
            "after loss data in training 0.056247539818286896 0.056524148247405606\n",
            "yyy epoch 102\n",
            "before loss data in training 0.06271756440401077 0.056524148247405606\n",
            "after loss data in training 0.06271756440401077 0.05658427850135323\n",
            "yyy epoch 103\n",
            "before loss data in training 0.047424908727407455 0.05658427850135323\n",
            "after loss data in training 0.047424908727407455 0.05649620763814221\n",
            "yyy epoch 104\n",
            "before loss data in training 0.06467865407466888 0.05649620763814221\n",
            "after loss data in training 0.06467865407466888 0.05657413569944247\n",
            "yyy epoch 105\n",
            "before loss data in training 0.0510270819067955 0.05657413569944247\n",
            "after loss data in training 0.0510270819067955 0.05652180500328542\n",
            "yyy epoch 106\n",
            "before loss data in training 0.05524884909391403 0.05652180500328542\n",
            "after loss data in training 0.05524884909391403 0.05650990821908569\n",
            "yyy epoch 107\n",
            "before loss data in training 0.06323514878749847 0.05650990821908569\n",
            "after loss data in training 0.06323514878749847 0.056572178965089516\n",
            "yyy epoch 108\n",
            "before loss data in training 0.07446733117103577 0.056572178965089516\n",
            "after loss data in training 0.07446733117103577 0.05673635467340095\n",
            "yyy epoch 109\n",
            "before loss data in training 0.05501721426844597 0.05673635467340095\n",
            "after loss data in training 0.05501721426844597 0.05672072612426499\n",
            "yyy epoch 110\n",
            "before loss data in training 0.06737132370471954 0.05672072612426499\n",
            "after loss data in training 0.06737132370471954 0.05681667745381864\n",
            "yyy epoch 111\n",
            "before loss data in training 0.044419124722480774 0.05681667745381864\n",
            "after loss data in training 0.044419124722480774 0.05670598501871741\n",
            "yyy epoch 112\n",
            "before loss data in training 0.061665769666433334 0.05670598501871741\n",
            "after loss data in training 0.061665769666433334 0.05674987691825472\n",
            "yyy epoch 113\n",
            "before loss data in training 0.055921103805303574 0.05674987691825472\n",
            "after loss data in training 0.055921103805303574 0.056742606978667426\n",
            "yyy epoch 114\n",
            "before loss data in training 0.06498299539089203 0.056742606978667426\n",
            "after loss data in training 0.06498299539089203 0.056814262530078076\n",
            "yyy epoch 115\n",
            "before loss data in training 0.0570259764790535 0.056814262530078076\n",
            "after loss data in training 0.0570259764790535 0.05681608765032786\n",
            "yyy epoch 116\n",
            "before loss data in training 0.05269044637680054 0.05681608765032786\n",
            "after loss data in training 0.05269044637680054 0.056780825759101136\n",
            "yyy epoch 117\n",
            "before loss data in training 0.05537132918834686 0.056780825759101136\n",
            "after loss data in training 0.05537132918834686 0.0567688808729083\n",
            "yyy epoch 118\n",
            "before loss data in training 0.04744454473257065 0.0567688808729083\n",
            "after loss data in training 0.04744454473257065 0.05669052510702311\n",
            "yyy epoch 119\n",
            "before loss data in training 0.05414411425590515 0.05669052510702311\n",
            "after loss data in training 0.05414411425590515 0.05666930501659713\n",
            "yyy epoch 120\n",
            "before loss data in training 0.052489668130874634 0.05666930501659713\n",
            "after loss data in training 0.052489668130874634 0.05663476256299612\n",
            "yyy epoch 121\n",
            "before loss data in training 0.045235421508550644 0.05663476256299612\n",
            "after loss data in training 0.045235421508550644 0.05654132534123837\n",
            "yyy epoch 122\n",
            "before loss data in training 0.046931181102991104 0.05654132534123837\n",
            "after loss data in training 0.046931181102991104 0.056463194087268874\n",
            "yyy epoch 123\n",
            "before loss data in training 0.05039655789732933 0.056463194087268874\n",
            "after loss data in training 0.05039655789732933 0.056414269601866136\n",
            "yyy epoch 124\n",
            "before loss data in training 0.06963536143302917 0.056414269601866136\n",
            "after loss data in training 0.06963536143302917 0.05652003833651544\n",
            "yyy epoch 125\n",
            "before loss data in training 0.04814919829368591 0.05652003833651544\n",
            "after loss data in training 0.04814919829368591 0.056453603098080285\n",
            "yyy epoch 126\n",
            "before loss data in training 0.052323874086141586 0.056453603098080285\n",
            "after loss data in training 0.052323874086141586 0.05642108554680518\n",
            "yyy epoch 127\n",
            "before loss data in training 0.057091884315013885 0.05642108554680518\n",
            "after loss data in training 0.057091884315013885 0.05642632616218181\n",
            "yyy epoch 128\n",
            "before loss data in training 0.06451699882745743 0.05642632616218181\n",
            "after loss data in training 0.06451699882745743 0.056489044554935884\n",
            "yyy epoch 129\n",
            "before loss data in training 0.0540279857814312 0.056489044554935884\n",
            "after loss data in training 0.0540279857814312 0.05647011333360123\n",
            "yyy epoch 130\n",
            "before loss data in training 0.0634596049785614 0.05647011333360123\n",
            "after loss data in training 0.0634596049785614 0.05652346823165436\n",
            "yyy epoch 131\n",
            "before loss data in training 0.052666548639535904 0.05652346823165436\n",
            "after loss data in training 0.052666548639535904 0.05649424914383528\n",
            "yyy epoch 132\n",
            "before loss data in training 0.054458167403936386 0.05649424914383528\n",
            "after loss data in training 0.054458167403936386 0.056478940258572884\n",
            "yyy epoch 133\n",
            "before loss data in training 0.07666486501693726 0.056478940258572884\n",
            "after loss data in training 0.07666486501693726 0.056629581488112916\n",
            "yyy epoch 134\n",
            "before loss data in training 0.048807863146066666 0.056629581488112916\n",
            "after loss data in training 0.048807863146066666 0.05657164283372739\n",
            "yyy epoch 135\n",
            "before loss data in training 0.04429430142045021 0.05657164283372739\n",
            "after loss data in training 0.04429430142045021 0.05648136826451212\n",
            "yyy epoch 136\n",
            "before loss data in training 0.0638103112578392 0.05648136826451212\n",
            "after loss data in training 0.0638103112578392 0.05653486419876998\n",
            "yyy epoch 137\n",
            "before loss data in training 0.06684722006320953 0.05653486419876998\n",
            "after loss data in training 0.06684722006320953 0.05660959141517896\n",
            "yyy epoch 138\n",
            "before loss data in training 0.050086744129657745 0.05660959141517896\n",
            "after loss data in training 0.050086744129657745 0.05656266445629032\n",
            "yyy epoch 139\n",
            "before loss data in training 0.06803596019744873 0.05656266445629032\n",
            "after loss data in training 0.06803596019744873 0.056644616568727164\n",
            "yyy epoch 140\n",
            "before loss data in training 0.06570922583341599 0.056644616568727164\n",
            "after loss data in training 0.06570922583341599 0.05670890457769659\n",
            "yyy epoch 141\n",
            "before loss data in training 0.055863313376903534 0.05670890457769659\n",
            "after loss data in training 0.055863313376903534 0.05670294971008537\n",
            "yyy epoch 142\n",
            "before loss data in training 0.03765377029776573 0.05670294971008537\n",
            "after loss data in training 0.03765377029776573 0.05656973866524397\n",
            "yyy epoch 143\n",
            "before loss data in training 0.04169484227895737 0.05656973866524397\n",
            "after loss data in training 0.04169484227895737 0.056466440773672535\n",
            "yyy epoch 144\n",
            "before loss data in training 0.07402290403842926 0.056466440773672535\n",
            "after loss data in training 0.07402290403842926 0.056587519830670854\n",
            "yyy epoch 145\n",
            "before loss data in training 0.07617909461259842 0.056587519830670854\n",
            "after loss data in training 0.07617909461259842 0.056721708699040224\n",
            "yyy epoch 146\n",
            "before loss data in training 0.04059572145342827 0.056721708699040224\n",
            "after loss data in training 0.04059572145342827 0.05661200810553266\n",
            "yyy epoch 147\n",
            "before loss data in training 0.0464598573744297 0.05661200810553266\n",
            "after loss data in training 0.0464598573744297 0.056543412492484665\n",
            "yyy epoch 148\n",
            "before loss data in training 0.060554664582014084 0.056543412492484665\n",
            "after loss data in training 0.060554664582014084 0.05657033364744795\n",
            "yyy epoch 149\n",
            "before loss data in training 0.06507037580013275 0.05657033364744795\n",
            "after loss data in training 0.06507037580013275 0.056627000595132516\n",
            "yyy epoch 150\n",
            "before loss data in training 0.05284646153450012 0.056627000595132516\n",
            "after loss data in training 0.05284646153450012 0.05660196391261177\n",
            "yyy epoch 151\n",
            "before loss data in training 0.045060403645038605 0.05660196391261177\n",
            "after loss data in training 0.045060403645038605 0.05652603259506195\n",
            "yyy epoch 152\n",
            "before loss data in training 0.05865490809082985 0.05652603259506195\n",
            "after loss data in training 0.05865490809082985 0.05653994681398854\n",
            "yyy epoch 153\n",
            "before loss data in training 0.0467306524515152 0.05653994681398854\n",
            "after loss data in training 0.0467306524515152 0.0564762500973491\n",
            "yyy epoch 154\n",
            "before loss data in training 0.06296802312135696 0.0564762500973491\n",
            "after loss data in training 0.06296802312135696 0.056518132503955605\n",
            "yyy epoch 155\n",
            "before loss data in training 0.04581785574555397 0.056518132503955605\n",
            "after loss data in training 0.04581785574555397 0.05644954098627354\n",
            "yyy epoch 156\n",
            "before loss data in training 0.04647126793861389 0.05644954098627354\n",
            "after loss data in training 0.04647126793861389 0.056385985106989084\n",
            "yyy epoch 157\n",
            "before loss data in training 0.05668171867728233 0.056385985106989084\n",
            "after loss data in training 0.05668171867728233 0.05638785683844664\n",
            "yyy epoch 158\n",
            "before loss data in training 0.05173578858375549 0.05638785683844664\n",
            "after loss data in training 0.05173578858375549 0.05635859854753663\n",
            "yyy epoch 159\n",
            "before loss data in training 0.05840185284614563 0.05635859854753663\n",
            "after loss data in training 0.05840185284614563 0.05637136888690294\n",
            "yyy epoch 160\n",
            "before loss data in training 0.042501166462898254 0.05637136888690294\n",
            "after loss data in training 0.042501166462898254 0.056285218561288\n",
            "yyy epoch 161\n",
            "before loss data in training 0.0528557114303112 0.056285218561288\n",
            "after loss data in training 0.0528557114303112 0.05626404876418321\n",
            "yyy epoch 162\n",
            "before loss data in training 0.05547107756137848 0.05626404876418321\n",
            "after loss data in training 0.05547107756137848 0.05625918391017827\n",
            "yyy epoch 163\n",
            "before loss data in training 0.055537909269332886 0.05625918391017827\n",
            "after loss data in training 0.055537909269332886 0.05625478589407555\n",
            "yyy epoch 164\n",
            "before loss data in training 0.056470222771167755 0.05625478589407555\n",
            "after loss data in training 0.056470222771167755 0.056256091572118534\n",
            "yyy epoch 165\n",
            "before loss data in training 0.039035364985466 0.056256091572118534\n",
            "after loss data in training 0.039035364985466 0.056152352255331474\n",
            "yyy epoch 166\n",
            "before loss data in training 0.060093365609645844 0.056152352255331474\n",
            "after loss data in training 0.060093365609645844 0.05617595113769264\n",
            "yyy epoch 167\n",
            "before loss data in training 0.056884877383708954 0.05617595113769264\n",
            "after loss data in training 0.056884877383708954 0.05618017093677607\n",
            "yyy epoch 168\n",
            "before loss data in training 0.05096885561943054 0.05618017093677607\n",
            "after loss data in training 0.05096885561943054 0.05614933475146633\n",
            "yyy epoch 169\n",
            "before loss data in training 0.04763117432594299 0.05614933475146633\n",
            "after loss data in training 0.04763117432594299 0.056099227925433835\n",
            "yyy epoch 170\n",
            "before loss data in training 0.0648944079875946 0.056099227925433835\n",
            "after loss data in training 0.0648944079875946 0.05615066172696694\n",
            "yyy epoch 171\n",
            "before loss data in training 0.053330570459365845 0.05615066172696694\n",
            "after loss data in training 0.053330570459365845 0.056134265847504146\n",
            "yyy epoch 172\n",
            "before loss data in training 0.07200363278388977 0.056134265847504146\n",
            "after loss data in training 0.07200363278388977 0.056225996292223135\n",
            "yyy epoch 173\n",
            "before loss data in training 0.053649913519620895 0.056225996292223135\n",
            "after loss data in training 0.053649913519620895 0.056211191218817375\n",
            "yyy epoch 174\n",
            "before loss data in training 0.05751042068004608 0.056211191218817375\n",
            "after loss data in training 0.05751042068004608 0.056218615387167256\n",
            "yyy epoch 175\n",
            "before loss data in training 0.07204493880271912 0.056218615387167256\n",
            "after loss data in training 0.07204493880271912 0.05630853767930107\n",
            "yyy epoch 176\n",
            "before loss data in training 0.05859988182783127 0.05630853767930107\n",
            "after loss data in training 0.05859988182783127 0.05632148312646791\n",
            "yyy epoch 177\n",
            "before loss data in training 0.053791653364896774 0.05632148312646791\n",
            "after loss data in training 0.053791653364896774 0.05630727059971751\n",
            "yyy epoch 178\n",
            "before loss data in training 0.05082905292510986 0.05630727059971751\n",
            "after loss data in training 0.05082905292510986 0.05627666603170294\n",
            "yyy epoch 179\n",
            "before loss data in training 0.04899198189377785 0.05627666603170294\n",
            "after loss data in training 0.04899198189377785 0.05623619556427002\n",
            "yyy epoch 180\n",
            "before loss data in training 0.05096851661801338 0.05623619556427002\n",
            "after loss data in training 0.05096851661801338 0.056207092365671917\n",
            "yyy epoch 181\n",
            "before loss data in training 0.05609673634171486 0.056207092365671917\n",
            "after loss data in training 0.05609673634171486 0.05620648601389193\n",
            "yyy epoch 182\n",
            "before loss data in training 0.049137409776449203 0.05620648601389193\n",
            "after loss data in training 0.049137409776449203 0.05616785718199333\n",
            "yyy epoch 183\n",
            "before loss data in training 0.04909677058458328 0.05616785718199333\n",
            "after loss data in training 0.04909677058458328 0.05612942736352915\n",
            "yyy epoch 184\n",
            "before loss data in training 0.048141371458768845 0.05612942736352915\n",
            "after loss data in training 0.048141371458768845 0.05608624868296287\n",
            "yyy epoch 185\n",
            "before loss data in training 0.04518876224756241 0.05608624868296287\n",
            "after loss data in training 0.04518876224756241 0.05602766004621341\n",
            "yyy epoch 186\n",
            "before loss data in training 0.0528559572994709 0.05602766004621341\n",
            "after loss data in training 0.0528559572994709 0.0560106990689581\n",
            "yyy epoch 187\n",
            "before loss data in training 0.055249039083719254 0.0560106990689581\n",
            "after loss data in training 0.055249039083719254 0.05600664768605789\n",
            "yyy epoch 188\n",
            "before loss data in training 0.04866015538573265 0.05600664768605789\n",
            "after loss data in training 0.04866015538573265 0.05596777735642654\n",
            "yyy epoch 189\n",
            "before loss data in training 0.05047918111085892 0.05596777735642654\n",
            "after loss data in training 0.05047918111085892 0.055938890007765654\n",
            "yyy epoch 190\n",
            "before loss data in training 0.05448725447058678 0.055938890007765654\n",
            "after loss data in training 0.05448725447058678 0.05593128982170713\n",
            "yyy epoch 191\n",
            "before loss data in training 0.05057665333151817 0.05593128982170713\n",
            "after loss data in training 0.05057665333151817 0.05590340108998739\n",
            "yyy epoch 192\n",
            "before loss data in training 0.05768546089529991 0.05590340108998739\n",
            "after loss data in training 0.05768546089529991 0.05591263456048124\n",
            "yyy epoch 193\n",
            "before loss data in training 0.05512383207678795 0.05591263456048124\n",
            "after loss data in training 0.05512383207678795 0.055908568568297255\n",
            "yyy epoch 194\n",
            "before loss data in training 0.06486895680427551 0.055908568568297255\n",
            "after loss data in training 0.06486895680427551 0.05595451927719971\n",
            "yyy epoch 195\n",
            "before loss data in training 0.057178203016519547 0.05595451927719971\n",
            "after loss data in training 0.057178203016519547 0.05596076256158399\n",
            "yyy epoch 196\n",
            "before loss data in training 0.05777299031615257 0.05596076256158399\n",
            "after loss data in training 0.05777299031615257 0.0559699616872417\n",
            "yyy epoch 197\n",
            "before loss data in training 0.0628199428319931 0.0559699616872417\n",
            "after loss data in training 0.0628199428319931 0.05600455755160913\n",
            "yyy epoch 198\n",
            "before loss data in training 0.053935687988996506 0.05600455755160913\n",
            "after loss data in training 0.053935687988996506 0.055994161222148765\n",
            "yyy epoch 199\n",
            "before loss data in training 0.04743896424770355 0.055994161222148765\n",
            "after loss data in training 0.04743896424770355 0.05595138523727654\n",
            "yyy epoch 200\n",
            "before loss data in training 0.0585586354136467 0.05595138523727654\n",
            "after loss data in training 0.0585586354136467 0.05596435663118883\n",
            "yyy epoch 201\n",
            "before loss data in training 0.044942449778318405 0.05596435663118883\n",
            "after loss data in training 0.044942449778318405 0.05590979273587759\n",
            "yyy epoch 202\n",
            "before loss data in training 0.06658927351236343 0.05590979273587759\n",
            "after loss data in training 0.06658927351236343 0.05596240101556471\n",
            "yyy epoch 203\n",
            "before loss data in training 0.05344686657190323 0.05596240101556471\n",
            "after loss data in training 0.05344686657190323 0.05595006996437029\n",
            "yyy epoch 204\n",
            "before loss data in training 0.04468976706266403 0.05595006996437029\n",
            "after loss data in training 0.04468976706266403 0.0558951416575327\n",
            "yyy epoch 205\n",
            "before loss data in training 0.052337147295475006 0.0558951416575327\n",
            "after loss data in training 0.052337147295475006 0.05587786984024116\n",
            "yyy epoch 206\n",
            "before loss data in training 0.055646516382694244 0.05587786984024116\n",
            "after loss data in training 0.055646516382694244 0.055876752190687794\n",
            "yyy epoch 207\n",
            "before loss data in training 0.05704699456691742 0.055876752190687794\n",
            "after loss data in training 0.05704699456691742 0.05588237835595813\n",
            "yyy epoch 208\n",
            "before loss data in training 0.05316103622317314 0.05588237835595813\n",
            "after loss data in training 0.05316103622317314 0.05586935758020318\n",
            "yyy epoch 209\n",
            "before loss data in training 0.043019358068704605 0.05586935758020318\n",
            "after loss data in training 0.043019358068704605 0.0558081671063389\n",
            "yyy epoch 210\n",
            "before loss data in training 0.047914620488882065 0.0558081671063389\n",
            "after loss data in training 0.047914620488882065 0.05577075693279645\n",
            "yyy epoch 211\n",
            "before loss data in training 0.07736202329397202 0.05577075693279645\n",
            "after loss data in training 0.07736202329397202 0.05587260252883973\n",
            "yyy epoch 212\n",
            "before loss data in training 0.05034581944346428 0.05587260252883973\n",
            "after loss data in training 0.05034581944346428 0.05584665519041074\n",
            "yyy epoch 213\n",
            "before loss data in training 0.06077474355697632 0.05584665519041074\n",
            "after loss data in training 0.06077474355697632 0.05586968364072179\n",
            "yyy epoch 214\n",
            "before loss data in training 0.06260594725608826 0.05586968364072179\n",
            "after loss data in training 0.06260594725608826 0.05590101509939791\n",
            "yyy epoch 215\n",
            "before loss data in training 0.05058035999536514 0.05590101509939791\n",
            "after loss data in training 0.05058035999536514 0.05587638243687924\n",
            "yyy epoch 216\n",
            "before loss data in training 0.05092998221516609 0.05587638243687924\n",
            "after loss data in training 0.05092998221516609 0.05585358796581144\n",
            "yyy epoch 217\n",
            "before loss data in training 0.059222541749477386 0.05585358796581144\n",
            "after loss data in training 0.059222541749477386 0.055869041882250274\n",
            "yyy epoch 218\n",
            "before loss data in training 0.056450024247169495 0.055869041882250274\n",
            "after loss data in training 0.056450024247169495 0.055871694769761324\n",
            "yyy epoch 219\n",
            "before loss data in training 0.07744363695383072 0.055871694769761324\n",
            "after loss data in training 0.07744363695383072 0.05596974905241618\n",
            "yyy epoch 220\n",
            "before loss data in training 0.047483641654253006 0.05596974905241618\n",
            "after loss data in training 0.047483641654253006 0.0559313503764064\n",
            "yyy epoch 221\n",
            "before loss data in training 0.06552347540855408 0.0559313503764064\n",
            "after loss data in training 0.06552347540855408 0.05597455814682148\n",
            "yyy epoch 222\n",
            "before loss data in training 0.04636433348059654 0.05597455814682148\n",
            "after loss data in training 0.04636433348059654 0.05593146296894603\n",
            "yyy epoch 223\n",
            "before loss data in training 0.05900096893310547 0.05593146296894603\n",
            "after loss data in training 0.05900096893310547 0.05594516612057174\n",
            "yyy epoch 224\n",
            "before loss data in training 0.04991660267114639 0.05594516612057174\n",
            "after loss data in training 0.04991660267114639 0.05591837250524096\n",
            "yyy epoch 225\n",
            "before loss data in training 0.04619749262928963 0.05591837250524096\n",
            "after loss data in training 0.04619749262928963 0.05587535976242702\n",
            "yyy epoch 226\n",
            "before loss data in training 0.04928738996386528 0.05587535976242702\n",
            "after loss data in training 0.04928738996386528 0.05584633786904128\n",
            "yyy epoch 227\n",
            "before loss data in training 0.046930860728025436 0.05584633786904128\n",
            "after loss data in training 0.046930860728025436 0.05580723489912454\n",
            "yyy epoch 228\n",
            "before loss data in training 0.06580600142478943 0.05580723489912454\n",
            "after loss data in training 0.06580600142478943 0.05585089763504448\n",
            "yyy epoch 229\n",
            "before loss data in training 0.06253135949373245 0.05585089763504448\n",
            "after loss data in training 0.06253135949373245 0.055879943121386604\n",
            "yyy epoch 230\n",
            "before loss data in training 0.05831284448504448 0.055879943121386604\n",
            "after loss data in training 0.05831284448504448 0.05589047516192192\n",
            "yyy epoch 231\n",
            "before loss data in training 0.05640489235520363 0.05589047516192192\n",
            "after loss data in training 0.05640489235520363 0.0558926924774102\n",
            "yyy epoch 232\n",
            "before loss data in training 0.0636630728840828 0.0558926924774102\n",
            "after loss data in training 0.0636630728840828 0.05592604174954184\n",
            "yyy epoch 233\n",
            "before loss data in training 0.06177963316440582 0.05592604174954184\n",
            "after loss data in training 0.06177963316440582 0.05595105709746861\n",
            "yyy epoch 234\n",
            "before loss data in training 0.05517375096678734 0.05595105709746861\n",
            "after loss data in training 0.05517375096678734 0.055947749411806136\n",
            "yyy epoch 235\n",
            "before loss data in training 0.04757294803857803 0.055947749411806136\n",
            "after loss data in training 0.04757294803857803 0.055912262965309406\n",
            "yyy epoch 236\n",
            "before loss data in training 0.045335933566093445 0.055912262965309406\n",
            "after loss data in training 0.045335933566093445 0.05586763710286546\n",
            "yyy epoch 237\n",
            "before loss data in training 0.04997273162007332 0.05586763710286546\n",
            "after loss data in training 0.04997273162007332 0.05584286859243356\n",
            "yyy epoch 238\n",
            "before loss data in training 0.059835128486156464 0.05584286859243356\n",
            "after loss data in training 0.059835128486156464 0.05585957260872528\n",
            "yyy epoch 239\n",
            "before loss data in training 0.06750695407390594 0.05585957260872528\n",
            "after loss data in training 0.06750695407390594 0.0559081033648302\n",
            "yyy epoch 240\n",
            "before loss data in training 0.06137615069746971 0.0559081033648302\n",
            "after loss data in training 0.06137615069746971 0.05593079235791169\n",
            "yyy epoch 241\n",
            "before loss data in training 0.0481196865439415 0.05593079235791169\n",
            "after loss data in training 0.0481196865439415 0.055898515061159754\n",
            "yyy epoch 242\n",
            "before loss data in training 0.05609346181154251 0.055898515061159754\n",
            "after loss data in training 0.05609346181154251 0.05589931731116133\n",
            "yyy epoch 243\n",
            "before loss data in training 0.05592415854334831 0.05589931731116133\n",
            "after loss data in training 0.05592415854334831 0.055899419119489965\n",
            "yyy epoch 244\n",
            "before loss data in training 0.06145123392343521 0.055899419119489965\n",
            "after loss data in training 0.06145123392343521 0.055922079588077496\n",
            "yyy epoch 245\n",
            "before loss data in training 0.049189113080501556 0.055922079588077496\n",
            "after loss data in training 0.049189113080501556 0.055894709805526376\n",
            "yyy epoch 246\n",
            "before loss data in training 0.056063033640384674 0.055894709805526376\n",
            "after loss data in training 0.056063033640384674 0.055895391278542\n",
            "yyy epoch 247\n",
            "before loss data in training 0.0637039989233017 0.055895391278542\n",
            "after loss data in training 0.0637039989233017 0.05592687759969022\n",
            "yyy epoch 248\n",
            "before loss data in training 0.04431621730327606 0.05592687759969022\n",
            "after loss data in training 0.04431621730327606 0.055880248441873295\n",
            "yyy epoch 249\n",
            "before loss data in training 0.05586521700024605 0.055880248441873295\n",
            "after loss data in training 0.05586521700024605 0.055880188316106784\n",
            "yyy epoch 250\n",
            "before loss data in training 0.04773890972137451 0.055880188316106784\n",
            "after loss data in training 0.04773890972137451 0.055847752943219406\n",
            "yyy epoch 251\n",
            "before loss data in training 0.04079478234052658 0.055847752943219406\n",
            "after loss data in training 0.04079478234052658 0.05578801893289126\n",
            "yyy epoch 252\n",
            "before loss data in training 0.06050248071551323 0.05578801893289126\n",
            "after loss data in training 0.06050248071551323 0.055806653169186204\n",
            "yyy epoch 253\n",
            "before loss data in training 0.06320282071828842 0.055806653169186204\n",
            "after loss data in training 0.06320282071828842 0.05583577193906456\n",
            "yyy epoch 254\n",
            "before loss data in training 0.06757204234600067 0.05583577193906456\n",
            "after loss data in training 0.06757204234600067 0.05588179652889568\n",
            "yyy epoch 255\n",
            "before loss data in training 0.06894652545452118 0.05588179652889568\n",
            "after loss data in training 0.06894652545452118 0.05593283062626141\n",
            "yyy epoch 256\n",
            "before loss data in training 0.04650203883647919 0.05593283062626141\n",
            "after loss data in training 0.04650203883647919 0.05589613493836342\n",
            "yyy epoch 257\n",
            "before loss data in training 0.06685357540845871 0.05589613493836342\n",
            "after loss data in training 0.06685357540845871 0.05593860563785991\n",
            "yyy epoch 258\n",
            "before loss data in training 0.06046284735202789 0.05593860563785991\n",
            "after loss data in training 0.06046284735202789 0.05595607375258643\n",
            "yyy epoch 259\n",
            "before loss data in training 0.06137828528881073 0.05595607375258643\n",
            "after loss data in training 0.06137828528881073 0.05597692841234114\n",
            "yyy epoch 260\n",
            "before loss data in training 0.054689571261405945 0.05597692841234114\n",
            "after loss data in training 0.054689571261405945 0.055971996009463995\n",
            "yyy epoch 261\n",
            "before loss data in training 0.0442584790289402 0.055971996009463995\n",
            "after loss data in training 0.0442584790289402 0.05592728792938566\n",
            "yyy epoch 262\n",
            "before loss data in training 0.06136699393391609 0.05592728792938566\n",
            "after loss data in training 0.06136699393391609 0.05594797122217855\n",
            "yyy epoch 263\n",
            "before loss data in training 0.0448097288608551 0.05594797122217855\n",
            "after loss data in training 0.0448097288608551 0.055905780910203845\n",
            "yyy epoch 264\n",
            "before loss data in training 0.05815506726503372 0.055905780910203845\n",
            "after loss data in training 0.05815506726503372 0.05591426878324094\n",
            "yyy epoch 265\n",
            "before loss data in training 0.06319787353277206 0.05591426878324094\n",
            "after loss data in training 0.06319787353277206 0.055941650755983535\n",
            "yyy epoch 266\n",
            "before loss data in training 0.0647183284163475 0.055941650755983535\n",
            "after loss data in training 0.0647183284163475 0.05597452220789501\n",
            "yyy epoch 267\n",
            "before loss data in training 0.05672004818916321 0.05597452220789501\n",
            "after loss data in training 0.05672004818916321 0.05597730402125795\n",
            "yyy epoch 268\n",
            "before loss data in training 0.04245296120643616 0.05597730402125795\n",
            "after loss data in training 0.04245296120643616 0.055927027653916606\n",
            "yyy epoch 269\n",
            "before loss data in training 0.0547700859606266 0.055927027653916606\n",
            "after loss data in training 0.0547700859606266 0.0559227426846822\n",
            "yyy epoch 270\n",
            "before loss data in training 0.045824166387319565 0.0559227426846822\n",
            "after loss data in training 0.045824166387319565 0.055885478565503736\n",
            "yyy epoch 271\n",
            "before loss data in training 0.05565737560391426 0.055885478565503736\n",
            "after loss data in training 0.05565737560391426 0.055884639951674364\n",
            "yyy epoch 272\n",
            "before loss data in training 0.05236879363656044 0.055884639951674364\n",
            "after loss data in training 0.05236879363656044 0.055871761393743544\n",
            "yyy epoch 273\n",
            "before loss data in training 0.054931458085775375 0.055871761393743544\n",
            "after loss data in training 0.054931458085775375 0.05586832962984585\n",
            "yyy epoch 274\n",
            "before loss data in training 0.05347020924091339 0.05586832962984585\n",
            "after loss data in training 0.05347020924091339 0.055859609192067915\n",
            "yyy epoch 275\n",
            "before loss data in training 0.06614994257688522 0.055859609192067915\n",
            "after loss data in training 0.06614994257688522 0.055896893008679574\n",
            "yyy epoch 276\n",
            "before loss data in training 0.05395814776420593 0.055896893008679574\n",
            "after loss data in training 0.05395814776420593 0.055889893928374616\n",
            "yyy epoch 277\n",
            "before loss data in training 0.05660822615027428 0.055889893928374616\n",
            "after loss data in training 0.05660822615027428 0.05589247785723037\n",
            "yyy epoch 278\n",
            "before loss data in training 0.05618518590927124 0.05589247785723037\n",
            "after loss data in training 0.05618518590927124 0.05589352699003339\n",
            "yyy epoch 279\n",
            "before loss data in training 0.05661093816161156 0.05589352699003339\n",
            "after loss data in training 0.05661093816161156 0.05589608917278902\n",
            "yyy epoch 280\n",
            "before loss data in training 0.06830748915672302 0.05589608917278902\n",
            "after loss data in training 0.06830748915672302 0.05594025785600587\n",
            "yyy epoch 281\n",
            "before loss data in training 0.07593570649623871 0.05594025785600587\n",
            "after loss data in training 0.07593570649623871 0.05601116370224783\n",
            "yyy epoch 282\n",
            "before loss data in training 0.05487799271941185 0.05601116370224783\n",
            "after loss data in training 0.05487799271941185 0.056007159564499295\n",
            "yyy epoch 283\n",
            "before loss data in training 0.06284060329198837 0.056007159564499295\n",
            "after loss data in training 0.06284060329198837 0.056031220986074964\n",
            "yyy epoch 284\n",
            "before loss data in training 0.050046250224113464 0.056031220986074964\n",
            "after loss data in training 0.050046250224113464 0.056010221088664575\n",
            "yyy epoch 285\n",
            "before loss data in training 0.05382728576660156 0.056010221088664575\n",
            "after loss data in training 0.05382728576660156 0.05600258844767834\n",
            "yyy epoch 286\n",
            "before loss data in training 0.0649588331580162 0.05600258844767834\n",
            "after loss data in training 0.0649588331580162 0.05603379487524049\n",
            "yyy epoch 287\n",
            "before loss data in training 0.059837475419044495 0.05603379487524049\n",
            "after loss data in training 0.059837475419044495 0.05604700209935092\n",
            "yyy epoch 288\n",
            "before loss data in training 0.047575101256370544 0.05604700209935092\n",
            "after loss data in training 0.047575101256370544 0.05601768756356206\n",
            "yyy epoch 289\n",
            "before loss data in training 0.03928820416331291 0.05601768756356206\n",
            "after loss data in training 0.03928820416331291 0.0559599996897681\n",
            "yyy epoch 290\n",
            "before loss data in training 0.05681717023253441 0.0559599996897681\n",
            "after loss data in training 0.05681717023253441 0.05596294529300785\n",
            "yyy epoch 291\n",
            "before loss data in training 0.047208476811647415 0.05596294529300785\n",
            "after loss data in training 0.047208476811647415 0.05593296423656483\n",
            "yyy epoch 292\n",
            "before loss data in training 0.055807530879974365 0.05593296423656483\n",
            "after loss data in training 0.055807530879974365 0.05593253613637169\n",
            "yyy epoch 293\n",
            "before loss data in training 0.04187159612774849 0.05593253613637169\n",
            "after loss data in training 0.04187159612774849 0.05588470980981175\n",
            "yyy epoch 294\n",
            "before loss data in training 0.05734744668006897 0.05588470980981175\n",
            "after loss data in training 0.05734744668006897 0.05588966823988042\n",
            "yyy epoch 295\n",
            "before loss data in training 0.05711815878748894 0.05588966823988042\n",
            "after loss data in training 0.05711815878748894 0.0558938185457845\n",
            "yyy epoch 296\n",
            "before loss data in training 0.04735231772065163 0.0558938185457845\n",
            "after loss data in training 0.04735231772065163 0.055865059283747014\n",
            "yyy epoch 297\n",
            "before loss data in training 0.04597592353820801 0.055865059283747014\n",
            "after loss data in training 0.04597592353820801 0.05583187426446668\n",
            "yyy epoch 298\n",
            "before loss data in training 0.05671092867851257 0.05583187426446668\n",
            "after loss data in training 0.05671092867851257 0.05583481424578456\n",
            "yyy epoch 299\n",
            "before loss data in training 0.06972985714673996 0.05583481424578456\n",
            "after loss data in training 0.06972985714673996 0.05588113105545441\n",
            "yyy epoch 300\n",
            "before loss data in training 0.04702954366803169 0.05588113105545441\n",
            "after loss data in training 0.04702954366803169 0.05585172378838656\n",
            "yyy epoch 301\n",
            "before loss data in training 0.050862789154052734 0.05585172378838656\n",
            "after loss data in training 0.050862789154052734 0.0558352041372795\n",
            "yyy epoch 302\n",
            "before loss data in training 0.0610397607088089 0.0558352041372795\n",
            "after loss data in training 0.0610397607088089 0.05585238089164098\n",
            "yyy epoch 303\n",
            "before loss data in training 0.053995806723833084 0.05585238089164098\n",
            "after loss data in training 0.053995806723833084 0.055846273739773196\n",
            "yyy epoch 304\n",
            "before loss data in training 0.04341323301196098 0.055846273739773196\n",
            "after loss data in training 0.04341323301196098 0.05580550967181316\n",
            "yyy epoch 305\n",
            "before loss data in training 0.06255987286567688 0.05580550967181316\n",
            "after loss data in training 0.06255987286567688 0.055827582754146045\n",
            "yyy epoch 306\n",
            "before loss data in training 0.04537881165742874 0.055827582754146045\n",
            "after loss data in training 0.04537881165742874 0.05579354766914045\n",
            "yyy epoch 307\n",
            "before loss data in training 0.05015690624713898 0.05579354766914045\n",
            "after loss data in training 0.05015690624713898 0.05577524688530278\n",
            "yyy epoch 308\n",
            "before loss data in training 0.0454641617834568 0.05577524688530278\n",
            "after loss data in training 0.0454641617834568 0.05574187767785344\n",
            "yyy epoch 309\n",
            "before loss data in training 0.048610612750053406 0.05574187767785344\n",
            "after loss data in training 0.048610612750053406 0.05571887359744118\n",
            "yyy epoch 310\n",
            "before loss data in training 0.06112224608659744 0.05571887359744118\n",
            "after loss data in training 0.06112224608659744 0.05573624778550921\n",
            "yyy epoch 311\n",
            "before loss data in training 0.045047976076602936 0.05573624778550921\n",
            "after loss data in training 0.045047976076602936 0.05570199050439092\n",
            "yyy epoch 312\n",
            "before loss data in training 0.07588233053684235 0.05570199050439092\n",
            "after loss data in training 0.07588233053684235 0.055766464434207054\n",
            "yyy epoch 313\n",
            "before loss data in training 0.05234831944108009 0.055766464434207054\n",
            "after loss data in training 0.05234831944108009 0.05575557862212703\n",
            "yyy epoch 314\n",
            "before loss data in training 0.043881043791770935 0.05575557862212703\n",
            "after loss data in training 0.043881043791770935 0.05571788168615765\n",
            "yyy epoch 315\n",
            "before loss data in training 0.03864850103855133 0.05571788168615765\n",
            "after loss data in training 0.03864850103855133 0.055663864658791806\n",
            "yyy epoch 316\n",
            "before loss data in training 0.0618634968996048 0.055663864658791806\n",
            "after loss data in training 0.0618634968996048 0.05568342185828964\n",
            "yyy epoch 317\n",
            "before loss data in training 0.052575528621673584 0.05568342185828964\n",
            "after loss data in training 0.052575528621673584 0.05567364860911789\n",
            "yyy epoch 318\n",
            "before loss data in training 0.0536281019449234 0.05567364860911789\n",
            "after loss data in training 0.0536281019449234 0.05566723623712982\n",
            "yyy epoch 319\n",
            "before loss data in training 0.0440010242164135 0.05566723623712982\n",
            "after loss data in training 0.0440010242164135 0.05563077932456508\n",
            "yyy epoch 320\n",
            "before loss data in training 0.034827303141355515 0.05563077932456508\n",
            "after loss data in training 0.034827303141355515 0.05556597098754574\n",
            "yyy epoch 321\n",
            "before loss data in training 0.05125897750258446 0.05556597098754574\n",
            "after loss data in training 0.05125897750258446 0.055552595231381265\n",
            "yyy epoch 322\n",
            "before loss data in training 0.059175241738557816 0.055552595231381265\n",
            "after loss data in training 0.059175241738557816 0.05556381085524249\n",
            "yyy epoch 323\n",
            "before loss data in training 0.0761139765381813 0.05556381085524249\n",
            "after loss data in training 0.0761139765381813 0.05562723729253551\n",
            "yyy epoch 324\n",
            "before loss data in training 0.04476747661828995 0.05562723729253551\n",
            "after loss data in training 0.04476747661828995 0.05559382264430707\n",
            "yyy epoch 325\n",
            "before loss data in training 0.055553216487169266 0.05559382264430707\n",
            "after loss data in training 0.055553216487169266 0.05559369808554284\n",
            "yyy epoch 326\n",
            "before loss data in training 0.057656947523355484 0.05559369808554284\n",
            "after loss data in training 0.057656947523355484 0.055600007716851134\n",
            "yyy epoch 327\n",
            "before loss data in training 0.055516477674245834 0.055600007716851134\n",
            "after loss data in training 0.055516477674245834 0.05559975305208709\n",
            "yyy epoch 328\n",
            "before loss data in training 0.05650710687041283 0.05559975305208709\n",
            "after loss data in training 0.05650710687041283 0.05560251096642851\n",
            "yyy epoch 329\n",
            "before loss data in training 0.04171721637248993 0.05560251096642851\n",
            "after loss data in training 0.04171721637248993 0.055560434316143846\n",
            "yyy epoch 330\n",
            "before loss data in training 0.04244031012058258 0.055560434316143846\n",
            "after loss data in training 0.04244031012058258 0.05552079647869502\n",
            "yyy epoch 331\n",
            "before loss data in training 0.043862588703632355 0.05552079647869502\n",
            "after loss data in training 0.043862588703632355 0.055485681395035194\n",
            "yyy epoch 332\n",
            "before loss data in training 0.04730197414755821 0.055485681395035194\n",
            "after loss data in training 0.04730197414755821 0.055461105697595325\n",
            "yyy epoch 333\n",
            "before loss data in training 0.04653800651431084 0.055461105697595325\n",
            "after loss data in training 0.04653800651431084 0.055434389831777106\n",
            "yyy epoch 334\n",
            "before loss data in training 0.04272287338972092 0.055434389831777106\n",
            "after loss data in training 0.04272287338972092 0.055396445006576936\n",
            "yyy epoch 335\n",
            "before loss data in training 0.047095395624637604 0.055396445006576936\n",
            "after loss data in training 0.047095395624637604 0.05537173950246402\n",
            "yyy epoch 336\n",
            "before loss data in training 0.05375412106513977 0.05537173950246402\n",
            "after loss data in training 0.05375412106513977 0.05536693944775386\n",
            "yyy epoch 337\n",
            "before loss data in training 0.03243980184197426 0.05536693944775386\n",
            "after loss data in training 0.03243980184197426 0.05529910767968942\n",
            "yyy epoch 338\n",
            "before loss data in training 0.05396945774555206 0.05529910767968942\n",
            "after loss data in training 0.05396945774555206 0.055295185408497274\n",
            "yyy epoch 339\n",
            "before loss data in training 0.04346129298210144 0.055295185408497274\n",
            "after loss data in training 0.04346129298210144 0.05526037984253729\n",
            "yyy epoch 340\n",
            "before loss data in training 0.05197754502296448 0.05526037984253729\n",
            "after loss data in training 0.05197754502296448 0.055250752760954966\n",
            "yyy epoch 341\n",
            "before loss data in training 0.06671798229217529 0.055250752760954966\n",
            "after loss data in training 0.06671798229217529 0.055284282671864964\n",
            "yyy epoch 342\n",
            "before loss data in training 0.04024767503142357 0.055284282671864964\n",
            "after loss data in training 0.04024767503142357 0.05524044416562461\n",
            "yyy epoch 343\n",
            "before loss data in training 0.05996916443109512 0.05524044416562461\n",
            "after loss data in training 0.05996916443109512 0.055254190445466096\n",
            "yyy epoch 344\n",
            "before loss data in training 0.07743804156780243 0.055254190445466096\n",
            "after loss data in training 0.07743804156780243 0.055318491463212\n",
            "yyy epoch 345\n",
            "before loss data in training 0.04626401141285896 0.055318491463212\n",
            "after loss data in training 0.04626401141285896 0.055292322445725434\n",
            "yyy epoch 346\n",
            "before loss data in training 0.06282631307840347 0.055292322445725434\n",
            "after loss data in training 0.06282631307840347 0.05531403423429223\n",
            "yyy epoch 347\n",
            "before loss data in training 0.04651866853237152 0.05531403423429223\n",
            "after loss data in training 0.04651866853237152 0.05528876019491889\n",
            "yyy epoch 348\n",
            "before loss data in training 0.04613347724080086 0.05528876019491889\n",
            "after loss data in training 0.04613347724080086 0.05526252729247157\n",
            "yyy epoch 349\n",
            "before loss data in training 0.06000910699367523 0.05526252729247157\n",
            "after loss data in training 0.06000910699367523 0.05527608894876072\n",
            "yyy epoch 350\n",
            "before loss data in training 0.0472831130027771 0.05527608894876072\n",
            "after loss data in training 0.0472831130027771 0.0552533169375186\n",
            "yyy epoch 351\n",
            "before loss data in training 0.059391457587480545 0.0552533169375186\n",
            "after loss data in training 0.059391457587480545 0.05526507301891054\n",
            "yyy epoch 352\n",
            "before loss data in training 0.057745348662137985 0.05526507301891054\n",
            "after loss data in training 0.057745348662137985 0.055272099295520245\n",
            "yyy epoch 353\n",
            "before loss data in training 0.0526481531560421 0.055272099295520245\n",
            "after loss data in training 0.0526481531560421 0.05526468701829008\n",
            "yyy epoch 354\n",
            "before loss data in training 0.045354507863521576 0.05526468701829008\n",
            "after loss data in training 0.045354507863521576 0.05523677102067101\n",
            "yyy epoch 355\n",
            "before loss data in training 0.05836362764239311 0.05523677102067101\n",
            "after loss data in training 0.05836362764239311 0.055245554325788204\n",
            "yyy epoch 356\n",
            "before loss data in training 0.05448978394269943 0.055245554325788204\n",
            "after loss data in training 0.05448978394269943 0.05524343732191401\n",
            "yyy epoch 357\n",
            "before loss data in training 0.05076276510953903 0.05524343732191401\n",
            "after loss data in training 0.05076276510953903 0.055230921477745366\n",
            "yyy epoch 358\n",
            "before loss data in training 0.06820323318243027 0.055230921477745366\n",
            "after loss data in training 0.06820323318243027 0.055267056050738915\n",
            "yyy epoch 359\n",
            "before loss data in training 0.047083914279937744 0.055267056050738915\n",
            "after loss data in training 0.047083914279937744 0.05524432510137558\n",
            "yyy epoch 360\n",
            "before loss data in training 0.06108789145946503 0.05524432510137558\n",
            "after loss data in training 0.06108789145946503 0.05526051226580242\n",
            "yyy epoch 361\n",
            "before loss data in training 0.06538096070289612 0.05526051226580242\n",
            "after loss data in training 0.06538096070289612 0.055288469305683895\n",
            "yyy epoch 362\n",
            "before loss data in training 0.04244457185268402 0.055288469305683895\n",
            "after loss data in training 0.04244457185268402 0.055253086668072326\n",
            "yyy epoch 363\n",
            "before loss data in training 0.053538233041763306 0.055253086668072326\n",
            "after loss data in training 0.053538233041763306 0.05524837553173631\n",
            "yyy epoch 364\n",
            "before loss data in training 0.05054575949907303 0.05524837553173631\n",
            "after loss data in training 0.05054575949907303 0.05523549165219477\n",
            "yyy epoch 365\n",
            "before loss data in training 0.06429004669189453 0.05523549165219477\n",
            "after loss data in training 0.06429004669189453 0.055260230873614716\n",
            "yyy epoch 366\n",
            "before loss data in training 0.054764796048402786 0.055260230873614716\n",
            "after loss data in training 0.054764796048402786 0.055258880914962914\n",
            "yyy epoch 367\n",
            "before loss data in training 0.04902121424674988 0.055258880914962914\n",
            "after loss data in training 0.04902121424674988 0.055241930733799294\n",
            "yyy epoch 368\n",
            "before loss data in training 0.05939694121479988 0.055241930733799294\n",
            "after loss data in training 0.05939694121479988 0.05525319092480471\n",
            "yyy epoch 369\n",
            "before loss data in training 0.03988042101264 0.05525319092480471\n",
            "after loss data in training 0.03988042101264 0.05521164289801508\n",
            "yyy epoch 370\n",
            "before loss data in training 0.060044143348932266 0.05521164289801508\n",
            "after loss data in training 0.060044143348932266 0.05522466850569949\n",
            "yyy epoch 371\n",
            "before loss data in training 0.06442169845104218 0.05522466850569949\n",
            "after loss data in training 0.06442169845104218 0.05524939170447729\n",
            "yyy epoch 372\n",
            "before loss data in training 0.04456663876771927 0.05524939170447729\n",
            "after loss data in training 0.04456663876771927 0.05522075161617499\n",
            "yyy epoch 373\n",
            "before loss data in training 0.04186205565929413 0.05522075161617499\n",
            "after loss data in training 0.04186205565929413 0.05518503317778761\n",
            "yyy epoch 374\n",
            "before loss data in training 0.0582953616976738 0.05518503317778761\n",
            "after loss data in training 0.0582953616976738 0.055193327387173974\n",
            "yyy epoch 375\n",
            "before loss data in training 0.04390453174710274 0.055193327387173974\n",
            "after loss data in training 0.04390453174710274 0.05516330399451421\n",
            "yyy epoch 376\n",
            "before loss data in training 0.05446377396583557 0.05516330399451421\n",
            "after loss data in training 0.05446377396583557 0.05516144847719676\n",
            "yyy epoch 377\n",
            "before loss data in training 0.058840733021497726 0.05516144847719676\n",
            "after loss data in training 0.058840733021497726 0.055171182034192266\n",
            "yyy epoch 378\n",
            "before loss data in training 0.07033580541610718 0.055171182034192266\n",
            "after loss data in training 0.07033580541610718 0.055211194233089136\n",
            "yyy epoch 379\n",
            "before loss data in training 0.05038551241159439 0.055211194233089136\n",
            "after loss data in training 0.05038551241159439 0.05519849507040099\n",
            "yyy epoch 380\n",
            "before loss data in training 0.03521249443292618 0.05519849507040099\n",
            "after loss data in training 0.03521249443292618 0.05514603837581444\n",
            "yyy epoch 381\n",
            "before loss data in training 0.05611218139529228 0.05514603837581444\n",
            "after loss data in training 0.05611218139529228 0.0551485675460225\n",
            "yyy epoch 382\n",
            "before loss data in training 0.05244491249322891 0.0551485675460225\n",
            "after loss data in training 0.05244491249322891 0.055141508394448625\n",
            "yyy epoch 383\n",
            "before loss data in training 0.06245066598057747 0.055141508394448625\n",
            "after loss data in training 0.06245066598057747 0.05516054265899584\n",
            "yyy epoch 384\n",
            "before loss data in training 0.06530869752168655 0.05516054265899584\n",
            "after loss data in training 0.06530869752168655 0.05518690150279503\n",
            "yyy epoch 385\n",
            "before loss data in training 0.0724061131477356 0.05518690150279503\n",
            "after loss data in training 0.0724061131477356 0.05523151085938814\n",
            "yyy epoch 386\n",
            "before loss data in training 0.04411403834819794 0.05523151085938814\n",
            "after loss data in training 0.04411403834819794 0.055202783540237776\n",
            "yyy epoch 387\n",
            "before loss data in training 0.04619128257036209 0.055202783540237776\n",
            "after loss data in training 0.04619128257036209 0.05517955802227418\n",
            "yyy epoch 388\n",
            "before loss data in training 0.04377138987183571 0.05517955802227418\n",
            "after loss data in training 0.04377138987183571 0.05515023111186174\n",
            "yyy epoch 389\n",
            "before loss data in training 0.06524505466222763 0.05515023111186174\n",
            "after loss data in training 0.06524505466222763 0.0551761152748114\n",
            "yyy epoch 390\n",
            "before loss data in training 0.04652169346809387 0.0551761152748114\n",
            "after loss data in training 0.04652169346809387 0.055153981203694476\n",
            "yyy epoch 391\n",
            "before loss data in training 0.041181545704603195 0.055153981203694476\n",
            "after loss data in training 0.041181545704603195 0.05511833723558455\n",
            "yyy epoch 392\n",
            "before loss data in training 0.050710927695035934 0.05511833723558455\n",
            "after loss data in training 0.050710927695035934 0.05510712245303862\n",
            "yyy epoch 393\n",
            "before loss data in training 0.05579885095357895 0.05510712245303862\n",
            "after loss data in training 0.05579885095357895 0.055108878109131364\n",
            "yyy epoch 394\n",
            "before loss data in training 0.04917600750923157 0.055108878109131364\n",
            "after loss data in training 0.04917600750923157 0.055093858183561994\n",
            "yyy epoch 395\n",
            "before loss data in training 0.05994224548339844 0.055093858183561994\n",
            "after loss data in training 0.05994224548339844 0.05510610158583431\n",
            "yyy epoch 396\n",
            "before loss data in training 0.05993064492940903 0.05510610158583431\n",
            "after loss data in training 0.05993064492940903 0.05511825408795918\n",
            "yyy epoch 397\n",
            "before loss data in training 0.06355815380811691 0.05511825408795918\n",
            "after loss data in training 0.06355815380811691 0.05513945986615053\n",
            "yyy epoch 398\n",
            "before loss data in training 0.047121889889240265 0.05513945986615053\n",
            "after loss data in training 0.047121889889240265 0.055119365705807395\n",
            "yyy epoch 399\n",
            "before loss data in training 0.04619511216878891 0.055119365705807395\n",
            "after loss data in training 0.04619511216878891 0.05509705507196485\n",
            "yyy epoch 400\n",
            "before loss data in training 0.045909006148576736 0.05509705507196485\n",
            "after loss data in training 0.045909006148576736 0.0550741422317569\n",
            "yyy epoch 401\n",
            "before loss data in training 0.04626060277223587 0.0550741422317569\n",
            "after loss data in training 0.04626060277223587 0.05505221800424565\n",
            "yyy epoch 402\n",
            "before loss data in training 0.06290394067764282 0.05505221800424565\n",
            "after loss data in training 0.06290394067764282 0.05507170118705805\n",
            "yyy epoch 403\n",
            "before loss data in training 0.04689590632915497 0.05507170118705805\n",
            "after loss data in training 0.04689590632915497 0.05505146407107314\n",
            "yyy epoch 404\n",
            "before loss data in training 0.04876936972141266 0.05505146407107314\n",
            "after loss data in training 0.04876936972141266 0.055035952726999904\n",
            "yyy epoch 405\n",
            "before loss data in training 0.06133848801255226 0.055035952726999904\n",
            "after loss data in training 0.06133848801255226 0.05505147621292491\n",
            "yyy epoch 406\n",
            "before loss data in training 0.05582943931221962 0.05505147621292491\n",
            "after loss data in training 0.05582943931221962 0.05505338767017133\n",
            "yyy epoch 407\n",
            "before loss data in training 0.04904986545443535 0.05505338767017133\n",
            "after loss data in training 0.04904986545443535 0.055038673154936685\n",
            "yyy epoch 408\n",
            "before loss data in training 0.05288787558674812 0.055038673154936685\n",
            "after loss data in training 0.05288787558674812 0.05503341448117583\n",
            "yyy epoch 409\n",
            "before loss data in training 0.053002357482910156 0.05503341448117583\n",
            "after loss data in training 0.053002357482910156 0.055028460683619086\n",
            "yyy epoch 410\n",
            "before loss data in training 0.04896856099367142 0.055028460683619086\n",
            "after loss data in training 0.04896856099367142 0.05501371640213503\n",
            "yyy epoch 411\n",
            "before loss data in training 0.06263784319162369 0.05501371640213503\n",
            "after loss data in training 0.06263784319162369 0.05503222156424544\n",
            "yyy epoch 412\n",
            "before loss data in training 0.054068729281425476 0.05503222156424544\n",
            "after loss data in training 0.054068729281425476 0.05502988865314903\n",
            "yyy epoch 413\n",
            "before loss data in training 0.04845760017633438 0.05502988865314903\n",
            "after loss data in training 0.04845760017633438 0.05501401356020986\n",
            "yyy epoch 414\n",
            "before loss data in training 0.036417800933122635 0.05501401356020986\n",
            "after loss data in training 0.036417800933122635 0.05496920340930121\n",
            "yyy epoch 415\n",
            "before loss data in training 0.0465104803442955 0.05496920340930121\n",
            "after loss data in training 0.0465104803442955 0.05494886994039495\n",
            "yyy epoch 416\n",
            "before loss data in training 0.05124282464385033 0.05494886994039495\n",
            "after loss data in training 0.05124282464385033 0.054939982541602275\n",
            "yyy epoch 417\n",
            "before loss data in training 0.04977792128920555 0.054939982541602275\n",
            "after loss data in training 0.04977792128920555 0.05492763311276879\n",
            "yyy epoch 418\n",
            "before loss data in training 0.051811959594488144 0.05492763311276879\n",
            "after loss data in training 0.051811959594488144 0.05492019713778483\n",
            "yyy epoch 419\n",
            "before loss data in training 0.05102232098579407 0.05492019713778483\n",
            "after loss data in training 0.05102232098579407 0.05491091648028009\n",
            "yyy epoch 420\n",
            "before loss data in training 0.05203244090080261 0.05491091648028009\n",
            "after loss data in training 0.05203244090080261 0.05490407924612456\n",
            "yyy epoch 421\n",
            "before loss data in training 0.051852546632289886 0.05490407924612456\n",
            "after loss data in training 0.051852546632289886 0.054896848126186566\n",
            "yyy epoch 422\n",
            "before loss data in training 0.06555009633302689 0.054896848126186566\n",
            "after loss data in training 0.06555009633302689 0.05492203311012709\n",
            "yyy epoch 423\n",
            "before loss data in training 0.06802631169557571 0.05492203311012709\n",
            "after loss data in training 0.06802631169557571 0.0549529394275456\n",
            "yyy epoch 424\n",
            "before loss data in training 0.04774528369307518 0.0549529394275456\n",
            "after loss data in training 0.04774528369307518 0.054935980237582144\n",
            "yyy epoch 425\n",
            "before loss data in training 0.05618916451931 0.054935980237582144\n",
            "after loss data in training 0.05618916451931 0.05493892198472235\n",
            "yyy epoch 426\n",
            "before loss data in training 0.043708059936761856 0.05493892198472235\n",
            "after loss data in training 0.043708059936761856 0.0549126202000667\n",
            "yyy epoch 427\n",
            "before loss data in training 0.0528375543653965 0.0549126202000667\n",
            "after loss data in training 0.0528375543653965 0.05490777191540626\n",
            "yyy epoch 428\n",
            "before loss data in training 0.03824017569422722 0.05490777191540626\n",
            "after loss data in training 0.03824017569422722 0.05486891970976249\n",
            "yyy epoch 429\n",
            "before loss data in training 0.04314999654889107 0.05486891970976249\n",
            "after loss data in training 0.04314999654889107 0.054841666400086045\n",
            "yyy epoch 430\n",
            "before loss data in training 0.049694888293743134 0.054841666400086045\n",
            "after loss data in training 0.049694888293743134 0.05482972491956089\n",
            "yyy epoch 431\n",
            "before loss data in training 0.04123559221625328 0.05482972491956089\n",
            "after loss data in training 0.04123559221625328 0.05479825701978471\n",
            "yyy epoch 432\n",
            "before loss data in training 0.05197296291589737 0.05479825701978471\n",
            "after loss data in training 0.05197296291589737 0.05479173209113832\n",
            "yyy epoch 433\n",
            "before loss data in training 0.04250921681523323 0.05479173209113832\n",
            "after loss data in training 0.04250921681523323 0.05476343136469614\n",
            "yyy epoch 434\n",
            "before loss data in training 0.06739315390586853 0.05476343136469614\n",
            "after loss data in training 0.06739315390586853 0.05479246520961838\n",
            "yyy epoch 435\n",
            "before loss data in training 0.042792871594429016 0.05479246520961838\n",
            "after loss data in training 0.042792871594429016 0.05476494320591382\n",
            "yyy epoch 436\n",
            "before loss data in training 0.04358622431755066 0.05476494320591382\n",
            "after loss data in training 0.04358622431755066 0.05473936261349193\n",
            "yyy epoch 437\n",
            "before loss data in training 0.05336520075798035 0.05473936261349193\n",
            "after loss data in training 0.05336520075798035 0.05473622525765743\n",
            "yyy epoch 438\n",
            "before loss data in training 0.039858877658843994 0.05473622525765743\n",
            "after loss data in training 0.039858877658843994 0.05470233608317266\n",
            "yyy epoch 439\n",
            "before loss data in training 0.058023612946271896 0.05470233608317266\n",
            "after loss data in training 0.058023612946271896 0.0547098844396797\n",
            "yyy epoch 440\n",
            "before loss data in training 0.04443884640932083 0.0547098844396797\n",
            "after loss data in training 0.04443884640932083 0.05468659410400995\n",
            "yyy epoch 441\n",
            "before loss data in training 0.05231195315718651 0.05468659410400995\n",
            "after loss data in training 0.05231195315718651 0.05468122161318004\n",
            "yyy epoch 442\n",
            "before loss data in training 0.06075586378574371 0.05468122161318004\n",
            "after loss data in training 0.06075586378574371 0.054694934123727584\n",
            "yyy epoch 443\n",
            "before loss data in training 0.05404742807149887 0.054694934123727584\n",
            "after loss data in training 0.05404742807149887 0.054693475776763104\n",
            "yyy epoch 444\n",
            "before loss data in training 0.04551851749420166 0.054693475776763104\n",
            "after loss data in training 0.04551851749420166 0.05467285789298207\n",
            "yyy epoch 445\n",
            "before loss data in training 0.05837319418787956 0.05467285789298207\n",
            "after loss data in training 0.05837319418787956 0.054681154611132064\n",
            "yyy epoch 446\n",
            "before loss data in training 0.043352674692869186 0.054681154611132064\n",
            "after loss data in training 0.043352674692869186 0.054655811255610226\n",
            "yyy epoch 447\n",
            "before loss data in training 0.04746508598327637 0.054655811255610226\n",
            "after loss data in training 0.04746508598327637 0.05463976052955591\n",
            "yyy epoch 448\n",
            "before loss data in training 0.05949029698967934 0.05463976052955591\n",
            "after loss data in training 0.05949029698967934 0.054650563506081794\n",
            "yyy epoch 449\n",
            "before loss data in training 0.04061968997120857 0.054650563506081794\n",
            "after loss data in training 0.04061968997120857 0.05461938378711541\n",
            "yyy epoch 450\n",
            "before loss data in training 0.04926532506942749 0.05461938378711541\n",
            "after loss data in training 0.04926532506942749 0.05460751226002519\n",
            "yyy epoch 451\n",
            "before loss data in training 0.054350145161151886 0.05460751226002519\n",
            "after loss data in training 0.054350145161151886 0.05460694286378875\n",
            "yyy epoch 452\n",
            "before loss data in training 0.04052304849028587 0.05460694286378875\n",
            "after loss data in training 0.04052304849028587 0.05457585258923355\n",
            "yyy epoch 453\n",
            "before loss data in training 0.06117487698793411 0.05457585258923355\n",
            "after loss data in training 0.06117487698793411 0.054590387885265934\n",
            "yyy epoch 454\n",
            "before loss data in training 0.04213249310851097 0.054590387885265934\n",
            "after loss data in training 0.04213249310851097 0.05456300789674559\n",
            "yyy epoch 455\n",
            "before loss data in training 0.04856321960687637 0.05456300789674559\n",
            "after loss data in training 0.04856321960687637 0.054549850466285354\n",
            "yyy epoch 456\n",
            "before loss data in training 0.062208764255046844 0.054549850466285354\n",
            "after loss data in training 0.062208764255046844 0.0545666095774205\n",
            "yyy epoch 457\n",
            "before loss data in training 0.05195184797048569 0.0545666095774205\n",
            "after loss data in training 0.05195184797048569 0.05456090049094248\n",
            "yyy epoch 458\n",
            "before loss data in training 0.0558297224342823 0.05456090049094248\n",
            "after loss data in training 0.0558297224342823 0.054563664808901825\n",
            "yyy epoch 459\n",
            "before loss data in training 0.048211049288511276 0.054563664808901825\n",
            "after loss data in training 0.048211049288511276 0.054549854775161846\n",
            "yyy epoch 460\n",
            "before loss data in training 0.058488160371780396 0.054549854775161846\n",
            "after loss data in training 0.058488160371780396 0.054558397737410474\n",
            "yyy epoch 461\n",
            "before loss data in training 0.057894159108400345 0.054558397737410474\n",
            "after loss data in training 0.057894159108400345 0.05456561800011824\n",
            "yyy epoch 462\n",
            "before loss data in training 0.06195196881890297 0.05456561800011824\n",
            "after loss data in training 0.06195196881890297 0.0545815712416275\n",
            "yyy epoch 463\n",
            "before loss data in training 0.05446777492761612 0.0545815712416275\n",
            "after loss data in training 0.05446777492761612 0.05458132599095075\n",
            "yyy epoch 464\n",
            "before loss data in training 0.048863090574741364 0.05458132599095075\n",
            "after loss data in training 0.048863090574741364 0.05456902871048579\n",
            "yyy epoch 465\n",
            "before loss data in training 0.044160548597574234 0.05456902871048579\n",
            "after loss data in training 0.044160548597574234 0.05454669291625207\n",
            "yyy epoch 466\n",
            "before loss data in training 0.05030076205730438 0.05454669291625207\n",
            "after loss data in training 0.05030076205730438 0.05453760098721792\n",
            "yyy epoch 467\n",
            "before loss data in training 0.06179266795516014 0.05453760098721792\n",
            "after loss data in training 0.06179266795516014 0.054553103267063946\n",
            "yyy epoch 468\n",
            "before loss data in training 0.04720195010304451 0.054553103267063946\n",
            "after loss data in training 0.04720195010304451 0.054537429166501006\n",
            "yyy epoch 469\n",
            "before loss data in training 0.05865681543946266 0.054537429166501006\n",
            "after loss data in training 0.05865681543946266 0.05454619381814561\n",
            "yyy epoch 470\n",
            "before loss data in training 0.05039859935641289 0.05454619381814561\n",
            "after loss data in training 0.05039859935641289 0.054537387885105836\n",
            "yyy epoch 471\n",
            "before loss data in training 0.044299256056547165 0.054537387885105836\n",
            "after loss data in training 0.044299256056547165 0.05451569692784194\n",
            "yyy epoch 472\n",
            "before loss data in training 0.0551288016140461 0.05451569692784194\n",
            "after loss data in training 0.0551288016140461 0.054516993132252514\n",
            "yyy epoch 473\n",
            "before loss data in training 0.07772262394428253 0.054516993132252514\n",
            "after loss data in training 0.07772262394428253 0.05456595015928211\n",
            "yyy epoch 474\n",
            "before loss data in training 0.03386494517326355 0.05456595015928211\n",
            "after loss data in training 0.03386494517326355 0.05452236909615365\n",
            "yyy epoch 475\n",
            "before loss data in training 0.0655955821275711 0.05452236909615365\n",
            "after loss data in training 0.0655955821275711 0.05454563214874066\n",
            "yyy epoch 476\n",
            "before loss data in training 0.07012150436639786 0.05454563214874066\n",
            "after loss data in training 0.07012150436639786 0.05457828596890347\n",
            "yyy epoch 477\n",
            "before loss data in training 0.0483495332300663 0.05457828596890347\n",
            "after loss data in training 0.0483495332300663 0.0545652551054331\n",
            "yyy epoch 478\n",
            "before loss data in training 0.05456702411174774 0.0545652551054331\n",
            "after loss data in training 0.05456702411174774 0.05456525879855693\n",
            "yyy epoch 479\n",
            "before loss data in training 0.057553935796022415 0.05456525879855693\n",
            "after loss data in training 0.057553935796022415 0.05457148520896831\n",
            "yyy epoch 480\n",
            "before loss data in training 0.040503937751054764 0.05457148520896831\n",
            "after loss data in training 0.040503937751054764 0.05454223874855685\n",
            "yyy epoch 481\n",
            "before loss data in training 0.06086636334657669 0.05454223874855685\n",
            "after loss data in training 0.06086636334657669 0.05455535933900917\n",
            "yyy epoch 482\n",
            "before loss data in training 0.05039726942777634 0.05455535933900917\n",
            "after loss data in training 0.05039726942777634 0.054546750457205376\n",
            "yyy epoch 483\n",
            "before loss data in training 0.051931388676166534 0.054546750457205376\n",
            "after loss data in training 0.051931388676166534 0.054541346817161905\n",
            "yyy epoch 484\n",
            "before loss data in training 0.03845621272921562 0.054541346817161905\n",
            "after loss data in training 0.03845621272921562 0.0545081815922383\n",
            "yyy epoch 485\n",
            "before loss data in training 0.05123543739318848 0.0545081815922383\n",
            "after loss data in training 0.05123543739318848 0.05450144755067647\n",
            "yyy epoch 486\n",
            "before loss data in training 0.04346736520528793 0.05450144755067647\n",
            "after loss data in training 0.04346736520528793 0.05447879029740052\n",
            "yyy epoch 487\n",
            "before loss data in training 0.057135287672281265 0.05447879029740052\n",
            "after loss data in training 0.057135287672281265 0.054484233939562164\n",
            "yyy epoch 488\n",
            "before loss data in training 0.053977955132722855 0.054484233939562164\n",
            "after loss data in training 0.053977955132722855 0.054483198604578856\n",
            "yyy epoch 489\n",
            "before loss data in training 0.06142066791653633 0.054483198604578856\n",
            "after loss data in training 0.06142066791653633 0.0544973567052155\n",
            "yyy epoch 490\n",
            "before loss data in training 0.050844382494688034 0.0544973567052155\n",
            "after loss data in training 0.050844382494688034 0.05448991683920628\n",
            "yyy epoch 491\n",
            "before loss data in training 0.05965989828109741 0.05448991683920628\n",
            "after loss data in training 0.05965989828109741 0.05450042493156785\n",
            "yyy epoch 492\n",
            "before loss data in training 0.0451284684240818 0.05450042493156785\n",
            "after loss data in training 0.0451284684240818 0.05448141487780013\n",
            "yyy epoch 493\n",
            "before loss data in training 0.04783526435494423 0.05448141487780013\n",
            "after loss data in training 0.04783526435494423 0.054467961131802446\n",
            "yyy epoch 494\n",
            "before loss data in training 0.03548507019877434 0.054467961131802446\n",
            "after loss data in training 0.03548507019877434 0.054429611857190266\n",
            "yyy epoch 495\n",
            "before loss data in training 0.057110145688056946 0.054429611857190266\n",
            "after loss data in training 0.057110145688056946 0.05443501615926863\n",
            "yyy epoch 496\n",
            "before loss data in training 0.054794274270534515 0.05443501615926863\n",
            "after loss data in training 0.054794274270534515 0.054435739012611215\n",
            "yyy epoch 497\n",
            "before loss data in training 0.047037120908498764 0.054435739012611215\n",
            "after loss data in training 0.047037120908498764 0.05442088234975155\n",
            "yyy epoch 498\n",
            "before loss data in training 0.06663242727518082 0.05442088234975155\n",
            "after loss data in training 0.06663242727518082 0.05444535438367025\n",
            "yyy epoch 499\n",
            "before loss data in training 0.04404018446803093 0.05444535438367025\n",
            "after loss data in training 0.04404018446803093 0.05442454404383897\n",
            "yyy epoch 500\n",
            "before loss data in training 0.056405793875455856 0.05442454404383897\n",
            "after loss data in training 0.056405793875455856 0.05442849863432124\n",
            "yyy epoch 501\n",
            "before loss data in training 0.03988338261842728 0.05442849863432124\n",
            "after loss data in training 0.03988338261842728 0.05439952429962822\n",
            "yyy epoch 502\n",
            "before loss data in training 0.051744185388088226 0.05439952429962822\n",
            "after loss data in training 0.051744185388088226 0.054394245295827946\n",
            "yyy epoch 503\n",
            "before loss data in training 0.053001366555690765 0.054394245295827946\n",
            "after loss data in training 0.053001366555690765 0.054391481647534025\n",
            "yyy epoch 504\n",
            "before loss data in training 0.05264509841799736 0.054391481647534025\n",
            "after loss data in training 0.05264509841799736 0.054388023462921084\n",
            "yyy epoch 505\n",
            "before loss data in training 0.06161674112081528 0.054388023462921084\n",
            "after loss data in training 0.06161674112081528 0.054402309466197556\n",
            "yyy epoch 506\n",
            "before loss data in training 0.05464093014597893 0.054402309466197556\n",
            "after loss data in training 0.05464093014597893 0.05440278011842592\n",
            "yyy epoch 507\n",
            "before loss data in training 0.03553915396332741 0.05440278011842592\n",
            "after loss data in training 0.03553915396332741 0.054365646996073366\n",
            "yyy epoch 508\n",
            "before loss data in training 0.04894610121846199 0.054365646996073366\n",
            "after loss data in training 0.04894610121846199 0.05435499955839633\n",
            "yyy epoch 509\n",
            "before loss data in training 0.0492655411362648 0.05435499955839633\n",
            "after loss data in training 0.0492655411362648 0.054345020228156855\n",
            "yyy epoch 510\n",
            "before loss data in training 0.059156063944101334 0.054345020228156855\n",
            "after loss data in training 0.059156063944101334 0.05435443518650508\n",
            "yyy epoch 511\n",
            "before loss data in training 0.03813858702778816 0.05435443518650508\n",
            "after loss data in training 0.03813858702778816 0.05432276360807009\n",
            "yyy epoch 512\n",
            "before loss data in training 0.035981886088848114 0.05432276360807009\n",
            "after loss data in training 0.035981886088848114 0.05428701141017687\n",
            "yyy epoch 513\n",
            "before loss data in training 0.04776618629693985 0.05428701141017687\n",
            "after loss data in training 0.04776618629693985 0.05427432497999547\n",
            "yyy epoch 514\n",
            "before loss data in training 0.041144393384456635 0.05427432497999547\n",
            "after loss data in training 0.041144393384456635 0.0542488299671886\n",
            "yyy epoch 515\n",
            "before loss data in training 0.051584091037511826 0.0542488299671886\n",
            "after loss data in training 0.051584091037511826 0.054243665744456666\n",
            "yyy epoch 516\n",
            "before loss data in training 0.04915498569607735 0.054243665744456666\n",
            "after loss data in training 0.04915498569607735 0.05423382303643272\n",
            "yyy epoch 517\n",
            "before loss data in training 0.05401544272899628 0.05423382303643272\n",
            "after loss data in training 0.05401544272899628 0.05423340145282763\n",
            "yyy epoch 518\n",
            "before loss data in training 0.045790910720825195 0.05423340145282763\n",
            "after loss data in training 0.045790910720825195 0.05421713461134015\n",
            "yyy epoch 519\n",
            "before loss data in training 0.051661282777786255 0.05421713461134015\n",
            "after loss data in training 0.051661282777786255 0.05421221951166024\n",
            "yyy epoch 520\n",
            "before loss data in training 0.03756270557641983 0.05421221951166024\n",
            "after loss data in training 0.03756270557641983 0.05418026267109356\n",
            "yyy epoch 521\n",
            "before loss data in training 0.05865075811743736 0.05418026267109356\n",
            "after loss data in training 0.05865075811743736 0.05418882683861529\n",
            "yyy epoch 522\n",
            "before loss data in training 0.054162122309207916 0.05418882683861529\n",
            "after loss data in training 0.054162122309207916 0.05418877577832962\n",
            "yyy epoch 523\n",
            "before loss data in training 0.06329521536827087 0.05418877577832962\n",
            "after loss data in training 0.06329521536827087 0.05420615447983714\n",
            "yyy epoch 524\n",
            "before loss data in training 0.057099781930446625 0.05420615447983714\n",
            "after loss data in training 0.057099781930446625 0.054211666151171636\n",
            "yyy epoch 525\n",
            "before loss data in training 0.05604926124215126 0.054211666151171636\n",
            "after loss data in training 0.05604926124215126 0.05421515967796057\n",
            "yyy epoch 526\n",
            "before loss data in training 0.0458848737180233 0.05421515967796057\n",
            "after loss data in training 0.0458848737180233 0.05419935268372919\n",
            "yyy epoch 527\n",
            "before loss data in training 0.042100872844457626 0.05419935268372919\n",
            "after loss data in training 0.042100872844457626 0.05417643889615481\n",
            "yyy epoch 528\n",
            "before loss data in training 0.05213458836078644 0.05417643889615481\n",
            "after loss data in training 0.05213458836078644 0.054172579065275096\n",
            "yyy epoch 529\n",
            "before loss data in training 0.07143962383270264 0.054172579065275096\n",
            "after loss data in training 0.07143962383270264 0.05420515839502496\n",
            "yyy epoch 530\n",
            "before loss data in training 0.058171797543764114 0.05420515839502496\n",
            "after loss data in training 0.058171797543764114 0.05421262852524857\n",
            "yyy epoch 531\n",
            "before loss data in training 0.0444200225174427 0.05421262852524857\n",
            "after loss data in training 0.0444200225174427 0.05419422137109856\n",
            "yyy epoch 532\n",
            "before loss data in training 0.0627235397696495 0.05419422137109856\n",
            "after loss data in training 0.0627235397696495 0.05421022384464181\n",
            "yyy epoch 533\n",
            "before loss data in training 0.05916063115000725 0.05421022384464181\n",
            "after loss data in training 0.05916063115000725 0.05421949427030729\n",
            "yyy epoch 534\n",
            "before loss data in training 0.06486611813306808 0.05421949427030729\n",
            "after loss data in training 0.06486611813306808 0.05423939450182647\n",
            "yyy epoch 535\n",
            "before loss data in training 0.04723979905247688 0.05423939450182647\n",
            "after loss data in training 0.04723979905247688 0.05422633555509261\n",
            "yyy epoch 536\n",
            "before loss data in training 0.05439634248614311 0.05422633555509261\n",
            "after loss data in training 0.05439634248614311 0.05422665214155639\n",
            "yyy epoch 537\n",
            "before loss data in training 0.04709111899137497 0.05422665214155639\n",
            "after loss data in training 0.04709111899137497 0.05421338906878654\n",
            "yyy epoch 538\n",
            "before loss data in training 0.06316598504781723 0.05421338906878654\n",
            "after loss data in training 0.06316598504781723 0.05422999870882185\n",
            "yyy epoch 539\n",
            "before loss data in training 0.059249963611364365 0.05422999870882185\n",
            "after loss data in training 0.059249963611364365 0.054239294940122855\n",
            "yyy epoch 540\n",
            "before loss data in training 0.058541323989629745 0.054239294940122855\n",
            "after loss data in training 0.058541323989629745 0.05424724693466908\n",
            "yyy epoch 541\n",
            "before loss data in training 0.044758908450603485 0.05424724693466908\n",
            "after loss data in training 0.044758908450603485 0.05422974077510438\n",
            "yyy epoch 542\n",
            "before loss data in training 0.06590498238801956 0.05422974077510438\n",
            "after loss data in training 0.06590498238801956 0.054251242140874026\n",
            "yyy epoch 543\n",
            "before loss data in training 0.04344341531395912 0.054251242140874026\n",
            "after loss data in training 0.04344341531395912 0.05423137481214808\n",
            "yyy epoch 544\n",
            "before loss data in training 0.05164629593491554 0.05423137481214808\n",
            "after loss data in training 0.05164629593491554 0.05422663154815315\n",
            "yyy epoch 545\n",
            "before loss data in training 0.05120328441262245 0.05422663154815315\n",
            "after loss data in training 0.05120328441262245 0.054221094282337165\n",
            "yyy epoch 546\n",
            "before loss data in training 0.03805789723992348 0.054221094282337165\n",
            "after loss data in training 0.03805789723992348 0.054191545476043906\n",
            "yyy epoch 547\n",
            "before loss data in training 0.04969063401222229 0.054191545476043906\n",
            "after loss data in training 0.04969063401222229 0.05418333213395664\n",
            "yyy epoch 548\n",
            "before loss data in training 0.04090036079287529 0.05418333213395664\n",
            "after loss data in training 0.04090036079287529 0.05415913728634082\n",
            "yyy epoch 549\n",
            "before loss data in training 0.0437934584915638 0.05415913728634082\n",
            "after loss data in training 0.0437934584915638 0.05414029059762305\n",
            "yyy epoch 550\n",
            "before loss data in training 0.06209493428468704 0.05414029059762305\n",
            "after loss data in training 0.06209493428468704 0.05415472733752698\n",
            "yyy epoch 551\n",
            "before loss data in training 0.04740782827138901 0.05415472733752698\n",
            "after loss data in training 0.04740782827138901 0.05414250469429122\n",
            "yyy epoch 552\n",
            "before loss data in training 0.044454775750637054 0.05414250469429122\n",
            "after loss data in training 0.044454775750637054 0.05412498619710559\n",
            "yyy epoch 553\n",
            "before loss data in training 0.06651512533426285 0.05412498619710559\n",
            "after loss data in training 0.06651512533426285 0.05414735106919432\n",
            "yyy epoch 554\n",
            "before loss data in training 0.05359400436282158 0.05414735106919432\n",
            "after loss data in training 0.05359400436282158 0.05414635404810176\n",
            "yyy epoch 555\n",
            "before loss data in training 0.06857423484325409 0.05414635404810176\n",
            "after loss data in training 0.06857423484325409 0.05417230347399232\n",
            "yyy epoch 556\n",
            "before loss data in training 0.04408708214759827 0.05417230347399232\n",
            "after loss data in training 0.04408708214759827 0.05415419715204189\n",
            "yyy epoch 557\n",
            "before loss data in training 0.04474614933133125 0.05415419715204189\n",
            "after loss data in training 0.04474614933133125 0.05413733685128792\n",
            "yyy epoch 558\n",
            "before loss data in training 0.053828608244657516 0.05413733685128792\n",
            "after loss data in training 0.053828608244657516 0.05413678456397732\n",
            "yyy epoch 559\n",
            "before loss data in training 0.057757291942834854 0.05413678456397732\n",
            "after loss data in training 0.057757291942834854 0.05414324975572528\n",
            "yyy epoch 560\n",
            "before loss data in training 0.061043061316013336 0.05414324975572528\n",
            "after loss data in training 0.061043061316013336 0.054155548885066254\n",
            "yyy epoch 561\n",
            "before loss data in training 0.052746448665857315 0.054155548885066254\n",
            "after loss data in training 0.052746448665857315 0.054153041589302536\n",
            "yyy epoch 562\n",
            "before loss data in training 0.05356918275356293 0.054153041589302536\n",
            "after loss data in training 0.05356918275356293 0.05415200453986072\n",
            "yyy epoch 563\n",
            "before loss data in training 0.056639548391103745 0.05415200453986072\n",
            "after loss data in training 0.056639548391103745 0.05415641507860406\n",
            "yyy epoch 564\n",
            "before loss data in training 0.05584406480193138 0.05415641507860406\n",
            "after loss data in training 0.05584406480193138 0.05415940206926482\n",
            "yyy epoch 565\n",
            "before loss data in training 0.06523038446903229 0.05415940206926482\n",
            "after loss data in training 0.06523038446903229 0.05417896210884038\n",
            "yyy epoch 566\n",
            "before loss data in training 0.03917780518531799 0.05417896210884038\n",
            "after loss data in training 0.03917780518531799 0.05415250504195586\n",
            "yyy epoch 567\n",
            "before loss data in training 0.06392072141170502 0.05415250504195586\n",
            "after loss data in training 0.06392072141170502 0.05416970260598711\n",
            "yyy epoch 568\n",
            "before loss data in training 0.06588006764650345 0.05416970260598711\n",
            "after loss data in training 0.06588006764650345 0.054190283212385205\n",
            "yyy epoch 569\n",
            "before loss data in training 0.03970988467335701 0.054190283212385205\n",
            "after loss data in training 0.03970988467335701 0.054164879004422\n",
            "yyy epoch 570\n",
            "before loss data in training 0.04137676954269409 0.054164879004422\n",
            "after loss data in training 0.04137676954269409 0.05414248301587256\n",
            "yyy epoch 571\n",
            "before loss data in training 0.045345913618803024 0.05414248301587256\n",
            "after loss data in training 0.045345913618803024 0.054127104398045515\n",
            "yyy epoch 572\n",
            "before loss data in training 0.042024362832307816 0.054127104398045515\n",
            "after loss data in training 0.042024362832307816 0.054105982685016304\n",
            "yyy epoch 573\n",
            "before loss data in training 0.04605096951127052 0.054105982685016304\n",
            "after loss data in training 0.04605096951127052 0.05409194956102023\n",
            "yyy epoch 574\n",
            "before loss data in training 0.04866200312972069 0.05409194956102023\n",
            "after loss data in training 0.04866200312972069 0.05408250617592232\n",
            "yyy epoch 575\n",
            "before loss data in training 0.03646763041615486 0.05408250617592232\n",
            "after loss data in training 0.03646763041615486 0.05405192479439495\n",
            "yyy epoch 576\n",
            "before loss data in training 0.05937819555401802 0.05405192479439495\n",
            "after loss data in training 0.05937819555401802 0.05406115576624871\n",
            "yyy epoch 577\n",
            "before loss data in training 0.04062625393271446 0.05406115576624871\n",
            "after loss data in training 0.04062625393271446 0.05403791199145021\n",
            "yyy epoch 578\n",
            "before loss data in training 0.037655819207429886 0.05403791199145021\n",
            "after loss data in training 0.037655819207429886 0.054009618221529616\n",
            "yyy epoch 579\n",
            "before loss data in training 0.044793691486120224 0.054009618221529616\n",
            "after loss data in training 0.044793691486120224 0.05399372869267546\n",
            "yyy epoch 580\n",
            "before loss data in training 0.05019448325037956 0.05399372869267546\n",
            "after loss data in training 0.05019448325037956 0.053987189543893546\n",
            "yyy epoch 581\n",
            "before loss data in training 0.031768884509801865 0.053987189543893546\n",
            "after loss data in training 0.031768884509801865 0.053949013762048026\n",
            "yyy epoch 582\n",
            "before loss data in training 0.03971227630972862 0.053949013762048026\n",
            "after loss data in training 0.03971227630972862 0.05392459397224988\n",
            "yyy epoch 583\n",
            "before loss data in training 0.05331715568900108 0.05392459397224988\n",
            "after loss data in training 0.05331715568900108 0.05392355383820322\n",
            "yyy epoch 584\n",
            "before loss data in training 0.04150916263461113 0.05392355383820322\n",
            "after loss data in training 0.04150916263461113 0.05390233265665862\n",
            "yyy epoch 585\n",
            "before loss data in training 0.05506999045610428 0.05390233265665862\n",
            "after loss data in training 0.05506999045610428 0.05390432524676007\n",
            "yyy epoch 586\n",
            "before loss data in training 0.055249109864234924 0.05390432524676007\n",
            "after loss data in training 0.055249109864234924 0.053906616191593926\n",
            "yyy epoch 587\n",
            "before loss data in training 0.04567890986800194 0.053906616191593926\n",
            "after loss data in training 0.04567890986800194 0.05389262349376469\n",
            "yyy epoch 588\n",
            "before loss data in training 0.049415163695812225 0.05389262349376469\n",
            "after loss data in training 0.049415163695812225 0.05388502169444728\n",
            "yyy epoch 589\n",
            "before loss data in training 0.05229481682181358 0.05388502169444728\n",
            "after loss data in training 0.05229481682181358 0.053882326431951295\n",
            "yyy epoch 590\n",
            "before loss data in training 0.05901392921805382 0.053882326431951295\n",
            "after loss data in training 0.05901392921805382 0.053891009346987005\n",
            "yyy epoch 591\n",
            "before loss data in training 0.0354403555393219 0.053891009346987005\n",
            "after loss data in training 0.0354403555393219 0.053859842702041624\n",
            "yyy epoch 592\n",
            "before loss data in training 0.045435141772031784 0.053859842702041624\n",
            "after loss data in training 0.045435141772031784 0.053845635786476685\n",
            "yyy epoch 593\n",
            "before loss data in training 0.03837099298834801 0.053845635786476685\n",
            "after loss data in training 0.03837099298834801 0.05381958419927445\n",
            "yyy epoch 594\n",
            "before loss data in training 0.0455327145755291 0.05381958419927445\n",
            "after loss data in training 0.0455327145755291 0.053805656687301764\n",
            "yyy epoch 595\n",
            "before loss data in training 0.04161415994167328 0.053805656687301764\n",
            "after loss data in training 0.04161415994167328 0.053785201155849366\n",
            "yyy epoch 596\n",
            "before loss data in training 0.052805300801992416 0.053785201155849366\n",
            "after loss data in training 0.052805300801992416 0.0537835597817223\n",
            "yyy epoch 597\n",
            "before loss data in training 0.044347602874040604 0.0537835597817223\n",
            "after loss data in training 0.044347602874040604 0.053767780589568985\n",
            "yyy epoch 598\n",
            "before loss data in training 0.04489381983876228 0.053767780589568985\n",
            "after loss data in training 0.04489381983876228 0.05375296596394159\n",
            "yyy epoch 599\n",
            "before loss data in training 0.06333362311124802 0.05375296596394159\n",
            "after loss data in training 0.06333362311124802 0.05376893372585377\n",
            "yyy epoch 600\n",
            "before loss data in training 0.043121736496686935 0.05376893372585377\n",
            "after loss data in training 0.043121736496686935 0.05375121792347579\n",
            "yyy epoch 601\n",
            "before loss data in training 0.051775529980659485 0.05375121792347579\n",
            "after loss data in training 0.051775529980659485 0.053747936049816626\n",
            "yyy epoch 602\n",
            "before loss data in training 0.054983653128147125 0.053747936049816626\n",
            "after loss data in training 0.054983653128147125 0.05374998533187024\n",
            "yyy epoch 603\n",
            "before loss data in training 0.054929036647081375 0.05374998533187024\n",
            "after loss data in training 0.054929036647081375 0.053751937403584166\n",
            "yyy epoch 604\n",
            "before loss data in training 0.060270629823207855 0.053751937403584166\n",
            "after loss data in training 0.060270629823207855 0.05376271210179842\n",
            "yyy epoch 605\n",
            "before loss data in training 0.05846571549773216 0.05376271210179842\n",
            "after loss data in training 0.05846571549773216 0.05377047283347488\n",
            "yyy epoch 606\n",
            "before loss data in training 0.04151733219623566 0.05377047283347488\n",
            "after loss data in training 0.04151733219623566 0.053750286440332806\n",
            "yyy epoch 607\n",
            "before loss data in training 0.048527125269174576 0.053750286440332806\n",
            "after loss data in training 0.048527125269174576 0.05374169571472235\n",
            "yyy epoch 608\n",
            "before loss data in training 0.04339691251516342 0.05374169571472235\n",
            "after loss data in training 0.04339691251516342 0.053724709207005505\n",
            "yyy epoch 609\n",
            "before loss data in training 0.05046144127845764 0.053724709207005505\n",
            "after loss data in training 0.05046144127845764 0.053719359587450506\n",
            "yyy epoch 610\n",
            "before loss data in training 0.05190480127930641 0.053719359587450506\n",
            "after loss data in training 0.05190480127930641 0.05371638977025223\n",
            "yyy epoch 611\n",
            "before loss data in training 0.03790301829576492 0.05371638977025223\n",
            "after loss data in training 0.03790301829576492 0.05369055092797366\n",
            "yyy epoch 612\n",
            "before loss data in training 0.04573732987046242 0.05369055092797366\n",
            "after loss data in training 0.04573732987046242 0.05367757666849974\n",
            "yyy epoch 613\n",
            "before loss data in training 0.046165406703948975 0.05367757666849974\n",
            "after loss data in training 0.046165406703948975 0.05366534186399721\n",
            "yyy epoch 614\n",
            "before loss data in training 0.05555891990661621 0.05366534186399721\n",
            "after loss data in training 0.05555891990661621 0.0536684208526844\n",
            "yyy epoch 615\n",
            "before loss data in training 0.0537315271794796 0.0536684208526844\n",
            "after loss data in training 0.0537315271794796 0.053668523298020104\n",
            "yyy epoch 616\n",
            "before loss data in training 0.03455614298582077 0.053668523298020104\n",
            "after loss data in training 0.03455614298582077 0.05363754699281394\n",
            "yyy epoch 617\n",
            "before loss data in training 0.04902700334787369 0.05363754699281394\n",
            "after loss data in training 0.04902700334787369 0.05363008656620401\n",
            "yyy epoch 618\n",
            "before loss data in training 0.04526837170124054 0.05363008656620401\n",
            "after loss data in training 0.04526837170124054 0.053616578141543324\n",
            "yyy epoch 619\n",
            "before loss data in training 0.03579817712306976 0.053616578141543324\n",
            "after loss data in training 0.03579817712306976 0.05358783878506192\n",
            "yyy epoch 620\n",
            "before loss data in training 0.04284685105085373 0.05358783878506192\n",
            "after loss data in training 0.04284685105085373 0.053570542508517296\n",
            "yyy epoch 621\n",
            "before loss data in training 0.05118401348590851 0.053570542508517296\n",
            "after loss data in training 0.05118401348590851 0.053566705645136894\n",
            "yyy epoch 622\n",
            "before loss data in training 0.052175603806972504 0.053566705645136894\n",
            "after loss data in training 0.052175603806972504 0.05356447273688944\n",
            "yyy epoch 623\n",
            "before loss data in training 0.03564855456352234 0.05356447273688944\n",
            "after loss data in training 0.03564855456352234 0.053535761329560325\n",
            "yyy epoch 624\n",
            "before loss data in training 0.0442904457449913 0.053535761329560325\n",
            "after loss data in training 0.0442904457449913 0.053520968824625016\n",
            "yyy epoch 625\n",
            "before loss data in training 0.04776386171579361 0.053520968824625016\n",
            "after loss data in training 0.04776386171579361 0.05351177216790164\n",
            "yyy epoch 626\n",
            "before loss data in training 0.05443798005580902 0.05351177216790164\n",
            "after loss data in training 0.05443798005580902 0.053513249373464494\n",
            "yyy epoch 627\n",
            "before loss data in training 0.041160788387060165 0.053513249373464494\n",
            "after loss data in training 0.041160788387060165 0.05349357984960079\n",
            "yyy epoch 628\n",
            "before loss data in training 0.06702258437871933 0.05349357984960079\n",
            "after loss data in training 0.06702258437871933 0.05351508860083946\n",
            "yyy epoch 629\n",
            "before loss data in training 0.058027416467666626 0.05351508860083946\n",
            "after loss data in training 0.058027416467666626 0.0535222510260249\n",
            "yyy epoch 630\n",
            "before loss data in training 0.03746737912297249 0.0535222510260249\n",
            "after loss data in training 0.03746737912297249 0.053496807488936066\n",
            "yyy epoch 631\n",
            "before loss data in training 0.0443711057305336 0.053496807488936066\n",
            "after loss data in training 0.0443711057305336 0.05348236808741961\n",
            "yyy epoch 632\n",
            "before loss data in training 0.04984363541007042 0.05348236808741961\n",
            "after loss data in training 0.04984363541007042 0.053476619694564395\n",
            "yyy epoch 633\n",
            "before loss data in training 0.05833059176802635 0.053476619694564395\n",
            "after loss data in training 0.05833059176802635 0.053484275801935786\n",
            "yyy epoch 634\n",
            "before loss data in training 0.0645090714097023 0.053484275801935786\n",
            "after loss data in training 0.0645090714097023 0.053501637684782666\n",
            "yyy epoch 635\n",
            "before loss data in training 0.051169004291296005 0.053501637684782666\n",
            "after loss data in training 0.051169004291296005 0.05349797002221429\n",
            "yyy epoch 636\n",
            "before loss data in training 0.03355089947581291 0.05349797002221429\n",
            "after loss data in training 0.03355089947581291 0.05346665593972386\n",
            "yyy epoch 637\n",
            "before loss data in training 0.04915446788072586 0.05346665593972386\n",
            "after loss data in training 0.04915446788072586 0.05345989702427089\n",
            "yyy epoch 638\n",
            "before loss data in training 0.04797647148370743 0.05345989702427089\n",
            "after loss data in training 0.04797647148370743 0.05345131576364403\n",
            "yyy epoch 639\n",
            "before loss data in training 0.04970938712358475 0.05345131576364403\n",
            "after loss data in training 0.04970938712358475 0.053445469000143936\n",
            "yyy epoch 640\n",
            "before loss data in training 0.045377928763628006 0.053445469000143936\n",
            "after loss data in training 0.045377928763628006 0.053432883133940325\n",
            "yyy epoch 641\n",
            "before loss data in training 0.04463542252779007 0.053432883133940325\n",
            "after loss data in training 0.04463542252779007 0.053419179924273426\n",
            "yyy epoch 642\n",
            "before loss data in training 0.05899958685040474 0.053419179924273426\n",
            "after loss data in training 0.05899958685040474 0.053427858628668654\n",
            "yyy epoch 643\n",
            "before loss data in training 0.04766194149851799 0.053427858628668654\n",
            "after loss data in training 0.04766194149851799 0.053418905341199474\n",
            "yyy epoch 644\n",
            "before loss data in training 0.04817128926515579 0.053418905341199474\n",
            "after loss data in training 0.04817128926515579 0.05341076950232189\n",
            "yyy epoch 645\n",
            "before loss data in training 0.04226577654480934 0.05341076950232189\n",
            "after loss data in training 0.04226577654480934 0.05339351719124215\n",
            "yyy epoch 646\n",
            "before loss data in training 0.03337298333644867 0.05339351719124215\n",
            "after loss data in training 0.03337298333644867 0.05336257355313582\n",
            "yyy epoch 647\n",
            "before loss data in training 0.04658335819840431 0.05336257355313582\n",
            "after loss data in training 0.04658335819840431 0.05335211180104518\n",
            "yyy epoch 648\n",
            "before loss data in training 0.05876244604587555 0.05335211180104518\n",
            "after loss data in training 0.05876244604587555 0.05336044821744708\n",
            "yyy epoch 649\n",
            "before loss data in training 0.06036587804555893 0.05336044821744708\n",
            "after loss data in training 0.06036587804555893 0.05337122580179802\n",
            "yyy epoch 650\n",
            "before loss data in training 0.0689132958650589 0.05337122580179802\n",
            "after loss data in training 0.0689132958650589 0.05339509994936063\n",
            "yyy epoch 651\n",
            "before loss data in training 0.06558389961719513 0.05339509994936063\n",
            "after loss data in training 0.06558389961719513 0.053413794427378786\n",
            "yyy epoch 652\n",
            "before loss data in training 0.057986173778772354 0.053413794427378786\n",
            "after loss data in training 0.057986173778772354 0.05342079653970864\n",
            "yyy epoch 653\n",
            "before loss data in training 0.04960544779896736 0.05342079653970864\n",
            "after loss data in training 0.04960544779896736 0.05341496267313258\n",
            "yyy epoch 654\n",
            "before loss data in training 0.0559060163795948 0.05341496267313258\n",
            "after loss data in training 0.0559060163795948 0.053418765808562294\n",
            "yyy epoch 655\n",
            "before loss data in training 0.038857318460941315 0.053418765808562294\n",
            "after loss data in training 0.038857318460941315 0.05339656848028848\n",
            "yyy epoch 656\n",
            "before loss data in training 0.05311940237879753 0.05339656848028848\n",
            "after loss data in training 0.05311940237879753 0.05339614661407616\n",
            "yyy epoch 657\n",
            "before loss data in training 0.05248510092496872 0.05339614661407616\n",
            "after loss data in training 0.05248510092496872 0.05339476204615959\n",
            "yyy epoch 658\n",
            "before loss data in training 0.06501881778240204 0.05339476204615959\n",
            "after loss data in training 0.06501881778240204 0.053412400977474066\n",
            "yyy epoch 659\n",
            "before loss data in training 0.0306923296302557 0.053412400977474066\n",
            "after loss data in training 0.0306923296302557 0.05337797662694798\n",
            "yyy epoch 660\n",
            "before loss data in training 0.04452348127961159 0.05337797662694798\n",
            "after loss data in training 0.04452348127961159 0.053364581021278785\n",
            "yyy epoch 661\n",
            "before loss data in training 0.043924637138843536 0.053364581021278785\n",
            "after loss data in training 0.043924637138843536 0.053350321287317405\n",
            "yyy epoch 662\n",
            "before loss data in training 0.04231953248381615 0.053350321287317405\n",
            "after loss data in training 0.04231953248381615 0.05333368359681439\n",
            "yyy epoch 663\n",
            "before loss data in training 0.05687534064054489 0.05333368359681439\n",
            "after loss data in training 0.05687534064054489 0.05333901741766338\n",
            "yyy epoch 664\n",
            "before loss data in training 0.0669446736574173 0.05333901741766338\n",
            "after loss data in training 0.0669446736574173 0.053359477051106614\n",
            "yyy epoch 665\n",
            "before loss data in training 0.0654541552066803 0.053359477051106614\n",
            "after loss data in training 0.0654541552066803 0.053377637228517384\n",
            "yyy epoch 666\n",
            "before loss data in training 0.05814910680055618 0.053377637228517384\n",
            "after loss data in training 0.05814910680055618 0.05338479085606167\n",
            "yyy epoch 667\n",
            "before loss data in training 0.04804668575525284 0.05338479085606167\n",
            "after loss data in training 0.04804668575525284 0.053376799680761054\n",
            "yyy epoch 668\n",
            "before loss data in training 0.039923135191202164 0.053376799680761054\n",
            "after loss data in training 0.039923135191202164 0.05335668956941642\n",
            "yyy epoch 669\n",
            "before loss data in training 0.04699813574552536 0.05335668956941642\n",
            "after loss data in training 0.04699813574552536 0.0533471991905748\n",
            "yyy epoch 670\n",
            "before loss data in training 0.043985240161418915 0.0533471991905748\n",
            "after loss data in training 0.043985240161418915 0.05333324694164908\n",
            "yyy epoch 671\n",
            "before loss data in training 0.06730560213327408 0.05333324694164908\n",
            "after loss data in training 0.06730560213327408 0.05335403913687471\n",
            "yyy epoch 672\n",
            "before loss data in training 0.03646136447787285 0.05335403913687471\n",
            "after loss data in training 0.03646136447787285 0.05332893858017486\n",
            "yyy epoch 673\n",
            "before loss data in training 0.06142731383442879 0.05332893858017486\n",
            "after loss data in training 0.06142731383442879 0.053340953973727165\n",
            "yyy epoch 674\n",
            "before loss data in training 0.041054751724004745 0.053340953973727165\n",
            "after loss data in training 0.041054751724004745 0.053322752192616465\n",
            "yyy epoch 675\n",
            "before loss data in training 0.055156536400318146 0.053322752192616465\n",
            "after loss data in training 0.055156536400318146 0.05332546489114857\n",
            "yyy epoch 676\n",
            "before loss data in training 0.05229245871305466 0.05332546489114857\n",
            "after loss data in training 0.05229245871305466 0.053323939032687576\n",
            "yyy epoch 677\n",
            "before loss data in training 0.05658778175711632 0.053323939032687576\n",
            "after loss data in training 0.05658778175711632 0.05332875296000974\n",
            "yyy epoch 678\n",
            "before loss data in training 0.057967644184827805 0.05332875296000974\n",
            "after loss data in training 0.057967644184827805 0.05333558490584894\n",
            "yyy epoch 679\n",
            "before loss data in training 0.05691090226173401 0.05333558490584894\n",
            "after loss data in training 0.05691090226173401 0.05334084272548995\n",
            "yyy epoch 680\n",
            "before loss data in training 0.06762135773897171 0.05334084272548995\n",
            "after loss data in training 0.06762135773897171 0.05336181264474617\n",
            "yyy epoch 681\n",
            "before loss data in training 0.07316549122333527 0.05336181264474617\n",
            "after loss data in training 0.07316549122333527 0.05339085029662093\n",
            "yyy epoch 682\n",
            "before loss data in training 0.04038038104772568 0.05339085029662093\n",
            "after loss data in training 0.04038038104772568 0.05337180129332826\n",
            "yyy epoch 683\n",
            "before loss data in training 0.035855330526828766 0.05337180129332826\n",
            "after loss data in training 0.035855330526828766 0.05334619241793864\n",
            "yyy epoch 684\n",
            "before loss data in training 0.051671553403139114 0.05334619241793864\n",
            "after loss data in training 0.051671553403139114 0.05334374768944988\n",
            "yyy epoch 685\n",
            "before loss data in training 0.06063893064856529 0.05334374768944988\n",
            "after loss data in training 0.06063893064856529 0.053354382066941305\n",
            "yyy epoch 686\n",
            "before loss data in training 0.04809320345520973 0.053354382066941305\n",
            "after loss data in training 0.04809320345520973 0.05334672387391113\n",
            "yyy epoch 687\n",
            "before loss data in training 0.047598619014024734 0.05334672387391113\n",
            "after loss data in training 0.047598619014024734 0.05333836907033571\n",
            "yyy epoch 688\n",
            "before loss data in training 0.046153392642736435 0.05333836907033571\n",
            "after loss data in training 0.046153392642736435 0.05332794094779929\n",
            "yyy epoch 689\n",
            "before loss data in training 0.063471220433712 0.05332794094779929\n",
            "after loss data in training 0.063471220433712 0.053342641352851335\n",
            "yyy epoch 690\n",
            "before loss data in training 0.04864082485437393 0.053342641352851335\n",
            "after loss data in training 0.04864082485437393 0.05333583698744109\n",
            "yyy epoch 691\n",
            "before loss data in training 0.06325828284025192 0.05333583698744109\n",
            "after loss data in training 0.06325828284025192 0.053350175782026076\n",
            "yyy epoch 692\n",
            "before loss data in training 0.04392591491341591 0.053350175782026076\n",
            "after loss data in training 0.04392591491341591 0.05333657655999345\n",
            "yyy epoch 693\n",
            "before loss data in training 0.0508364737033844 0.05333657655999345\n",
            "after loss data in training 0.0508364737033844 0.05333297410630958\n",
            "yyy epoch 694\n",
            "before loss data in training 0.053211189806461334 0.05333297410630958\n",
            "after loss data in training 0.053211189806461334 0.05333279887710116\n",
            "yyy epoch 695\n",
            "before loss data in training 0.04096674174070358 0.05333279887710116\n",
            "after loss data in training 0.04096674174070358 0.05331503155362933\n",
            "yyy epoch 696\n",
            "before loss data in training 0.05325254425406456 0.05331503155362933\n",
            "after loss data in training 0.05325254425406456 0.053314941901836554\n",
            "yyy epoch 697\n",
            "before loss data in training 0.04235871508717537 0.053314941901836554\n",
            "after loss data in training 0.04235871508717537 0.05329924530181555\n",
            "yyy epoch 698\n",
            "before loss data in training 0.062491174787282944 0.05329924530181555\n",
            "after loss data in training 0.062491174787282944 0.05331239541552866\n",
            "yyy epoch 699\n",
            "before loss data in training 0.040149811655282974 0.05331239541552866\n",
            "after loss data in training 0.040149811655282974 0.05329359172444259\n",
            "yyy epoch 700\n",
            "before loss data in training 0.04611063748598099 0.05329359172444259\n",
            "after loss data in training 0.04611063748598099 0.05328334499942339\n",
            "yyy epoch 701\n",
            "before loss data in training 0.046918898820877075 0.05328334499942339\n",
            "after loss data in training 0.046918898820877075 0.053274278836775886\n",
            "yyy epoch 702\n",
            "before loss data in training 0.04863763973116875 0.053274278836775886\n",
            "after loss data in training 0.04863763973116875 0.05326768333306948\n",
            "yyy epoch 703\n",
            "before loss data in training 0.0393952950835228 0.05326768333306948\n",
            "after loss data in training 0.0393952950835228 0.0532479782361241\n",
            "yyy epoch 704\n",
            "before loss data in training 0.04343126714229584 0.0532479782361241\n",
            "after loss data in training 0.04343126714229584 0.053234053823225054\n",
            "yyy epoch 705\n",
            "before loss data in training 0.05698011443018913 0.053234053823225054\n",
            "after loss data in training 0.05698011443018913 0.05323935985807911\n",
            "yyy epoch 706\n",
            "before loss data in training 0.05222128704190254 0.05323935985807911\n",
            "after loss data in training 0.05222128704190254 0.0532379198682401\n",
            "yyy epoch 707\n",
            "before loss data in training 0.0580330528318882 0.0532379198682401\n",
            "after loss data in training 0.0580330528318882 0.05324469265491192\n",
            "yyy epoch 708\n",
            "before loss data in training 0.03330380469560623 0.05324469265491192\n",
            "after loss data in training 0.03330380469560623 0.053216567284024324\n",
            "yyy epoch 709\n",
            "before loss data in training 0.06366769224405289 0.053216567284024324\n",
            "after loss data in training 0.06366769224405289 0.053231287178334225\n",
            "yyy epoch 710\n",
            "before loss data in training 0.055481936782598495 0.053231287178334225\n",
            "after loss data in training 0.055481936782598495 0.053234452648945\n",
            "yyy epoch 711\n",
            "before loss data in training 0.0606861487030983 0.053234452648945\n",
            "after loss data in training 0.0606861487030983 0.0532449185141896\n",
            "yyy epoch 712\n",
            "before loss data in training 0.058215875178575516 0.0532449185141896\n",
            "after loss data in training 0.058215875178575516 0.05325189040291946\n",
            "yyy epoch 713\n",
            "before loss data in training 0.05167783424258232 0.05325189040291946\n",
            "after loss data in training 0.05167783424258232 0.053249685842470806\n",
            "yyy epoch 714\n",
            "before loss data in training 0.05668751522898674 0.053249685842470806\n",
            "after loss data in training 0.05668751522898674 0.05325449399545894\n",
            "yyy epoch 715\n",
            "before loss data in training 0.03242257982492447 0.05325449399545894\n",
            "after loss data in training 0.03242257982492447 0.053225399143265455\n",
            "yyy epoch 716\n",
            "before loss data in training 0.06456322968006134 0.053225399143265455\n",
            "after loss data in training 0.06456322968006134 0.05324121201709641\n",
            "yyy epoch 717\n",
            "before loss data in training 0.04232077673077583 0.05324121201709641\n",
            "after loss data in training 0.04232077673077583 0.053226002497199026\n",
            "yyy epoch 718\n",
            "before loss data in training 0.05197708308696747 0.053226002497199026\n",
            "after loss data in training 0.05197708308696747 0.05322426547437534\n",
            "yyy epoch 719\n",
            "before loss data in training 0.05740620568394661 0.05322426547437534\n",
            "after loss data in training 0.05740620568394661 0.053230073724666406\n",
            "yyy epoch 720\n",
            "before loss data in training 0.0564911812543869 0.053230073724666406\n",
            "after loss data in training 0.0564911812543869 0.05323459675868821\n",
            "yyy epoch 721\n",
            "before loss data in training 0.04813159629702568 0.05323459675868821\n",
            "after loss data in training 0.04813159629702568 0.05322752889101278\n",
            "yyy epoch 722\n",
            "before loss data in training 0.06680257618427277 0.05322752889101278\n",
            "after loss data in training 0.06680257618427277 0.05324630489003527\n",
            "yyy epoch 723\n",
            "before loss data in training 0.05138757824897766 0.05324630489003527\n",
            "after loss data in training 0.05138757824897766 0.053243737588044855\n",
            "yyy epoch 724\n",
            "before loss data in training 0.060799695551395416 0.053243737588044855\n",
            "after loss data in training 0.060799695551395416 0.05325415959902879\n",
            "yyy epoch 725\n",
            "before loss data in training 0.06098442152142525 0.05325415959902879\n",
            "after loss data in training 0.06098442152142525 0.05326480734272355\n",
            "yyy epoch 726\n",
            "before loss data in training 0.06612897664308548 0.05326480734272355\n",
            "after loss data in training 0.06612897664308548 0.053282502211087185\n",
            "yyy epoch 727\n",
            "before loss data in training 0.05031179264187813 0.053282502211087185\n",
            "after loss data in training 0.05031179264187813 0.053278421566074534\n",
            "yyy epoch 728\n",
            "before loss data in training 0.05165915563702583 0.053278421566074534\n",
            "after loss data in training 0.05165915563702583 0.05327620035080835\n",
            "yyy epoch 729\n",
            "before loss data in training 0.055191028863191605 0.05327620035080835\n",
            "after loss data in training 0.055191028863191605 0.05327882340356504\n",
            "yyy epoch 730\n",
            "before loss data in training 0.037838537245988846 0.05327882340356504\n",
            "after loss data in training 0.037838537245988846 0.053257701261078615\n",
            "yyy epoch 731\n",
            "before loss data in training 0.04189758375287056 0.053257701261078615\n",
            "after loss data in training 0.04189758375287056 0.053242181974865216\n",
            "yyy epoch 732\n",
            "before loss data in training 0.05297752469778061 0.053242181974865216\n",
            "after loss data in training 0.05297752469778061 0.05324182091445992\n",
            "yyy epoch 733\n",
            "before loss data in training 0.044521134346723557 0.05324182091445992\n",
            "after loss data in training 0.044521134346723557 0.053229939870089706\n",
            "yyy epoch 734\n",
            "before loss data in training 0.04941735044121742 0.053229939870089706\n",
            "after loss data in training 0.04941735044121742 0.05322475267358784\n",
            "yyy epoch 735\n",
            "before loss data in training 0.052273865789175034 0.05322475267358784\n",
            "after loss data in training 0.052273865789175034 0.05322346070771228\n",
            "yyy epoch 736\n",
            "before loss data in training 0.05009537935256958 0.05322346070771228\n",
            "after loss data in training 0.05009537935256958 0.05321921636394682\n",
            "yyy epoch 737\n",
            "before loss data in training 0.06466440111398697 0.05321921636394682\n",
            "after loss data in training 0.06466440111398697 0.053234724744366926\n",
            "yyy epoch 738\n",
            "before loss data in training 0.044480789452791214 0.053234724744366926\n",
            "after loss data in training 0.044480789452791214 0.053222879094445986\n",
            "yyy epoch 739\n",
            "before loss data in training 0.04502052441239357 0.053222879094445986\n",
            "after loss data in training 0.04502052441239357 0.05321179483136213\n",
            "yyy epoch 740\n",
            "before loss data in training 0.04228033125400543 0.05321179483136213\n",
            "after loss data in training 0.04228033125400543 0.05319704251884208\n",
            "yyy epoch 741\n",
            "before loss data in training 0.053772274404764175 0.05319704251884208\n",
            "after loss data in training 0.053772274404764175 0.05319781776397135\n",
            "yyy epoch 742\n",
            "before loss data in training 0.04622293636202812 0.05319781776397135\n",
            "after loss data in training 0.04622293636202812 0.053188430305826075\n",
            "yyy epoch 743\n",
            "before loss data in training 0.05196521431207657 0.053188430305826075\n",
            "after loss data in training 0.05196521431207657 0.05318678619830759\n",
            "yyy epoch 744\n",
            "before loss data in training 0.05886746197938919 0.05318678619830759\n",
            "after loss data in training 0.05886746197938919 0.05319441126647012\n",
            "yyy epoch 745\n",
            "before loss data in training 0.06388381868600845 0.05319441126647012\n",
            "after loss data in training 0.06388381868600845 0.05320874023083948\n",
            "yyy epoch 746\n",
            "before loss data in training 0.052952155470848083 0.05320874023083948\n",
            "after loss data in training 0.052952155470848083 0.05320839674387831\n",
            "yyy epoch 747\n",
            "before loss data in training 0.04734785854816437 0.05320839674387831\n",
            "after loss data in training 0.04734785854816437 0.05320056179976639\n",
            "yyy epoch 748\n",
            "before loss data in training 0.046725619584321976 0.05320056179976639\n",
            "after loss data in training 0.046725619584321976 0.05319191701710225\n",
            "yyy epoch 749\n",
            "before loss data in training 0.06022735685110092 0.05319191701710225\n",
            "after loss data in training 0.06022735685110092 0.05320129760354758\n",
            "yyy epoch 750\n",
            "before loss data in training 0.05714961141347885 0.05320129760354758\n",
            "after loss data in training 0.05714961141347885 0.053206555012082775\n",
            "yyy epoch 751\n",
            "before loss data in training 0.03984297439455986 0.053206555012082775\n",
            "after loss data in training 0.03984297439455986 0.05318878429317649\n",
            "yyy epoch 752\n",
            "before loss data in training 0.056819844990968704 0.05318878429317649\n",
            "after loss data in training 0.056819844990968704 0.053193606418937174\n",
            "yyy epoch 753\n",
            "before loss data in training 0.04758761078119278 0.053193606418937174\n",
            "after loss data in training 0.04758761078119278 0.05318617141146006\n",
            "yyy epoch 754\n",
            "before loss data in training 0.03674467280507088 0.05318617141146006\n",
            "after loss data in training 0.03674467280507088 0.05316439459211385\n",
            "yyy epoch 755\n",
            "before loss data in training 0.0633249506354332 0.05316439459211385\n",
            "after loss data in training 0.0633249506354332 0.05317783448106004\n",
            "yyy epoch 756\n",
            "before loss data in training 0.04713298752903938 0.05317783448106004\n",
            "after loss data in training 0.04713298752903938 0.05316984921428062\n",
            "yyy epoch 757\n",
            "before loss data in training 0.0517253577709198 0.05316984921428062\n",
            "after loss data in training 0.0517253577709198 0.053167943552745844\n",
            "yyy epoch 758\n",
            "before loss data in training 0.04993438720703125 0.053167943552745844\n",
            "after loss data in training 0.04993438720703125 0.053163683267705375\n",
            "yyy epoch 759\n",
            "before loss data in training 0.04885268583893776 0.053163683267705375\n",
            "after loss data in training 0.04885268583893776 0.05315801090266752\n",
            "yyy epoch 760\n",
            "before loss data in training 0.03944944590330124 0.05315801090266752\n",
            "after loss data in training 0.03944944590330124 0.05313999701961974\n",
            "yyy epoch 761\n",
            "before loss data in training 0.05349033325910568 0.05313999701961974\n",
            "after loss data in training 0.05349033325910568 0.053140456778464205\n",
            "yyy epoch 762\n",
            "before loss data in training 0.053646087646484375 0.053140456778464205\n",
            "after loss data in training 0.053646087646484375 0.05314111946636463\n",
            "yyy epoch 763\n",
            "before loss data in training 0.06340616941452026 0.05314111946636463\n",
            "after loss data in training 0.06340616941452026 0.05315455539561614\n",
            "yyy epoch 764\n",
            "before loss data in training 0.06128886342048645 0.05315455539561614\n",
            "after loss data in training 0.06128886342048645 0.05316518847800159\n",
            "yyy epoch 765\n",
            "before loss data in training 0.0361238569021225 0.05316518847800159\n",
            "after loss data in training 0.0361238569021225 0.05314294130884248\n",
            "yyy epoch 766\n",
            "before loss data in training 0.05149468034505844 0.05314294130884248\n",
            "after loss data in training 0.05149468034505844 0.05314079233757288\n",
            "yyy epoch 767\n",
            "before loss data in training 0.03959740698337555 0.05314079233757288\n",
            "after loss data in training 0.03959740698337555 0.05312315772122627\n",
            "yyy epoch 768\n",
            "before loss data in training 0.05521358549594879 0.05312315772122627\n",
            "after loss data in training 0.05521358549594879 0.053125876092844894\n",
            "yyy epoch 769\n",
            "before loss data in training 0.05263776704668999 0.053125876092844894\n",
            "after loss data in training 0.05263776704668999 0.053125242184992746\n",
            "yyy epoch 770\n",
            "before loss data in training 0.04560738056898117 0.053125242184992746\n",
            "after loss data in training 0.04560738056898117 0.053115491391716466\n",
            "yyy epoch 771\n",
            "before loss data in training 0.0461387075483799 0.053115491391716466\n",
            "after loss data in training 0.0461387075483799 0.053106454106945304\n",
            "yyy epoch 772\n",
            "before loss data in training 0.0642554983496666 0.053106454106945304\n",
            "after loss data in training 0.0642554983496666 0.053120877191347275\n",
            "yyy epoch 773\n",
            "before loss data in training 0.04455127567052841 0.053120877191347275\n",
            "after loss data in training 0.04455127567052841 0.05310980535475707\n",
            "yyy epoch 774\n",
            "before loss data in training 0.05250070244073868 0.05310980535475707\n",
            "after loss data in training 0.05250070244073868 0.05310901941551317\n",
            "yyy epoch 775\n",
            "before loss data in training 0.05730708688497543 0.05310901941551317\n",
            "after loss data in training 0.05730708688497543 0.05311442929627279\n",
            "yyy epoch 776\n",
            "before loss data in training 0.03741762042045593 0.05311442929627279\n",
            "after loss data in training 0.03741762042045593 0.0530942274830478\n",
            "yyy epoch 777\n",
            "before loss data in training 0.05088192597031593 0.0530942274830478\n",
            "after loss data in training 0.05088192597031593 0.05309138390783863\n",
            "yyy epoch 778\n",
            "before loss data in training 0.057557448744773865 0.05309138390783863\n",
            "after loss data in training 0.057557448744773865 0.053097116982083734\n",
            "yyy epoch 779\n",
            "before loss data in training 0.04874377325177193 0.053097116982083734\n",
            "after loss data in training 0.04874377325177193 0.05309153577217308\n",
            "yyy epoch 780\n",
            "before loss data in training 0.041819263249635696 0.05309153577217308\n",
            "after loss data in training 0.041819263249635696 0.053077102644743454\n",
            "yyy epoch 781\n",
            "before loss data in training 0.06001749634742737 0.053077102644743454\n",
            "after loss data in training 0.06001749634742737 0.05308597782850648\n",
            "yyy epoch 782\n",
            "before loss data in training 0.03755804896354675 0.05308597782850648\n",
            "after loss data in training 0.03755804896354675 0.053066146501731304\n",
            "yyy epoch 783\n",
            "before loss data in training 0.04054046422243118 0.053066146501731304\n",
            "after loss data in training 0.04054046422243118 0.05305016986617097\n",
            "yyy epoch 784\n",
            "before loss data in training 0.047852545976638794 0.05305016986617097\n",
            "after loss data in training 0.047852545976638794 0.05304354868924163\n",
            "yyy epoch 785\n",
            "before loss data in training 0.03743366152048111 0.05304354868924163\n",
            "after loss data in training 0.03743366152048111 0.053023688781902244\n",
            "yyy epoch 786\n",
            "before loss data in training 0.05485621467232704 0.053023688781902244\n",
            "after loss data in training 0.05485621467232704 0.05302601727731574\n",
            "yyy epoch 787\n",
            "before loss data in training 0.042814090847969055 0.05302601727731574\n",
            "after loss data in training 0.042814090847969055 0.05301305797981657\n",
            "yyy epoch 788\n",
            "before loss data in training 0.04695815220475197 0.05301305797981657\n",
            "after loss data in training 0.04695815220475197 0.053005383828010406\n",
            "yyy epoch 789\n",
            "before loss data in training 0.03744179755449295 0.053005383828010406\n",
            "after loss data in training 0.03744179755449295 0.05298568308589203\n",
            "yyy epoch 790\n",
            "before loss data in training 0.045998696237802505 0.05298568308589203\n",
            "after loss data in training 0.045998696237802505 0.052976849979889384\n",
            "yyy epoch 791\n",
            "before loss data in training 0.056167345494031906 0.052976849979889384\n",
            "after loss data in training 0.056167345494031906 0.05298087838331633\n",
            "yyy epoch 792\n",
            "before loss data in training 0.060721833258867264 0.05298087838331633\n",
            "after loss data in training 0.060721833258867264 0.052990639990977806\n",
            "yyy epoch 793\n",
            "before loss data in training 0.04846568778157234 0.052990639990977806\n",
            "after loss data in training 0.04846568778157234 0.052984941058724144\n",
            "yyy epoch 794\n",
            "before loss data in training 0.050999611616134644 0.052984941058724144\n",
            "after loss data in training 0.050999611616134644 0.05298244378898504\n",
            "yyy epoch 795\n",
            "before loss data in training 0.05554484948515892 0.05298244378898504\n",
            "after loss data in training 0.05554484948515892 0.05298566289161843\n",
            "yyy epoch 796\n",
            "before loss data in training 0.0487091988325119 0.05298566289161843\n",
            "after loss data in training 0.0487091988325119 0.05298029719016409\n",
            "yyy epoch 797\n",
            "before loss data in training 0.050916992127895355 0.05298029719016409\n",
            "after loss data in training 0.050916992127895355 0.05297771159484796\n",
            "yyy epoch 798\n",
            "before loss data in training 0.045431021600961685 0.05297771159484796\n",
            "after loss data in training 0.045431021600961685 0.05296826642589441\n",
            "yyy epoch 799\n",
            "before loss data in training 0.05012919008731842 0.05296826642589441\n",
            "after loss data in training 0.05012919008731842 0.05296471758047119\n",
            "yyy epoch 800\n",
            "before loss data in training 0.0533643402159214 0.05296471758047119\n",
            "after loss data in training 0.0533643402159214 0.05296521648513468\n",
            "yyy epoch 801\n",
            "before loss data in training 0.0552489198744297 0.05296521648513468\n",
            "after loss data in training 0.0552489198744297 0.052968063995595144\n",
            "yyy epoch 802\n",
            "before loss data in training 0.04730162397027016 0.052968063995595144\n",
            "after loss data in training 0.04730162397027016 0.05296100740776784\n",
            "yyy epoch 803\n",
            "before loss data in training 0.047479525208473206 0.05296100740776784\n",
            "after loss data in training 0.047479525208473206 0.05295418964383837\n",
            "yyy epoch 804\n",
            "before loss data in training 0.05389406904578209 0.05295418964383837\n",
            "after loss data in training 0.05389406904578209 0.05295535719589047\n",
            "yyy epoch 805\n",
            "before loss data in training 0.04409521073102951 0.05295535719589047\n",
            "after loss data in training 0.04409521073102951 0.05294436445834102\n",
            "yyy epoch 806\n",
            "before loss data in training 0.04181147366762161 0.05294436445834102\n",
            "after loss data in training 0.04181147366762161 0.05293056905463504\n",
            "yyy epoch 807\n",
            "before loss data in training 0.05613405257463455 0.05293056905463504\n",
            "after loss data in training 0.05613405257463455 0.05293453376196178\n",
            "yyy epoch 808\n",
            "before loss data in training 0.04775798320770264 0.05293453376196178\n",
            "after loss data in training 0.04775798320770264 0.0529281350591753\n",
            "yyy epoch 809\n",
            "before loss data in training 0.034376781433820724 0.0529281350591753\n",
            "after loss data in training 0.034376781433820724 0.05290523215346498\n",
            "yyy epoch 810\n",
            "before loss data in training 0.05744216591119766 0.05290523215346498\n",
            "after loss data in training 0.05744216591119766 0.05291082639977538\n",
            "yyy epoch 811\n",
            "before loss data in training 0.0511256568133831 0.05291082639977538\n",
            "after loss data in training 0.0511256568133831 0.052908627915063074\n",
            "yyy epoch 812\n",
            "before loss data in training 0.036084968596696854 0.052908627915063074\n",
            "after loss data in training 0.036084968596696854 0.0528879346071684\n",
            "yyy epoch 813\n",
            "before loss data in training 0.04821505770087242 0.0528879346071684\n",
            "after loss data in training 0.04821505770087242 0.052882193972148384\n",
            "yyy epoch 814\n",
            "before loss data in training 0.054703112691640854 0.052882193972148384\n",
            "after loss data in training 0.054703112691640854 0.05288442822824592\n",
            "yyy epoch 815\n",
            "before loss data in training 0.042112935334444046 0.05288442822824592\n",
            "after loss data in training 0.042112935334444046 0.05287122786930744\n",
            "yyy epoch 816\n",
            "before loss data in training 0.050540272146463394 0.05287122786930744\n",
            "after loss data in training 0.050540272146463394 0.052868374802327214\n",
            "yyy epoch 817\n",
            "before loss data in training 0.05486218258738518 0.052868374802327214\n",
            "after loss data in training 0.05486218258738518 0.05287081222015736\n",
            "yyy epoch 818\n",
            "before loss data in training 0.05054516717791557 0.05287081222015736\n",
            "after loss data in training 0.05054516717791557 0.05286797260472117\n",
            "yyy epoch 819\n",
            "before loss data in training 0.04711102321743965 0.05286797260472117\n",
            "after loss data in training 0.04711102321743965 0.05286095193473668\n",
            "yyy epoch 820\n",
            "before loss data in training 0.0521387942135334 0.05286095193473668\n",
            "after loss data in training 0.0521387942135334 0.05286007232728089\n",
            "yyy epoch 821\n",
            "before loss data in training 0.04994858428835869 0.05286007232728089\n",
            "after loss data in training 0.04994858428835869 0.05285653037102916\n",
            "yyy epoch 822\n",
            "before loss data in training 0.05471736937761307 0.05285653037102916\n",
            "after loss data in training 0.05471736937761307 0.05285879141477956\n",
            "yyy epoch 823\n",
            "before loss data in training 0.04749881848692894 0.05285879141477956\n",
            "after loss data in training 0.04749881848692894 0.05285228659326518\n",
            "yyy epoch 824\n",
            "before loss data in training 0.05177097022533417 0.05285228659326518\n",
            "after loss data in training 0.05177097022533417 0.05285097590675859\n",
            "yyy epoch 825\n",
            "before loss data in training 0.057458266615867615 0.05285097590675859\n",
            "after loss data in training 0.057458266615867615 0.05285655374054686\n",
            "yyy epoch 826\n",
            "before loss data in training 0.04458249360322952 0.05285655374054686\n",
            "after loss data in training 0.04458249360322952 0.05284654883107005\n",
            "yyy epoch 827\n",
            "before loss data in training 0.05190296843647957 0.05284654883107005\n",
            "after loss data in training 0.05190296843647957 0.05284540924122151\n",
            "yyy epoch 828\n",
            "before loss data in training 0.05720535293221474 0.05284540924122151\n",
            "after loss data in training 0.05720535293221474 0.05285066852191028\n",
            "yyy epoch 829\n",
            "before loss data in training 0.06165853142738342 0.05285066852191028\n",
            "after loss data in training 0.06165853142738342 0.05286128040492893\n",
            "yyy epoch 830\n",
            "before loss data in training 0.06364676356315613 0.05286128040492893\n",
            "after loss data in training 0.06364676356315613 0.05287425932569695\n",
            "yyy epoch 831\n",
            "before loss data in training 0.03578868508338928 0.05287425932569695\n",
            "after loss data in training 0.03578868508338928 0.05285372377973264\n",
            "yyy epoch 832\n",
            "before loss data in training 0.042377620935440063 0.05285372377973264\n",
            "after loss data in training 0.042377620935440063 0.052841147425777904\n",
            "yyy epoch 833\n",
            "before loss data in training 0.045827466994524 0.052841147425777904\n",
            "after loss data in training 0.045827466994524 0.052832737737011415\n",
            "yyy epoch 834\n",
            "before loss data in training 0.04442713037133217 0.052832737737011415\n",
            "after loss data in training 0.04442713037133217 0.052822671141363894\n",
            "yyy epoch 835\n",
            "before loss data in training 0.04613802582025528 0.052822671141363894\n",
            "after loss data in training 0.04613802582025528 0.05281467515413769\n",
            "yyy epoch 836\n",
            "before loss data in training 0.06107636168599129 0.05281467515413769\n",
            "after loss data in training 0.06107636168599129 0.05282454574736571\n",
            "yyy epoch 837\n",
            "before loss data in training 0.04739207401871681 0.05282454574736571\n",
            "after loss data in training 0.04739207401871681 0.05281806308420503\n",
            "yyy epoch 838\n",
            "before loss data in training 0.04902584105730057 0.05281806308420503\n",
            "after loss data in training 0.04902584105730057 0.05281354315330288\n",
            "yyy epoch 839\n",
            "before loss data in training 0.05587279424071312 0.05281354315330288\n",
            "after loss data in training 0.05587279424071312 0.052817185118883135\n",
            "yyy epoch 840\n",
            "before loss data in training 0.05769619718194008 0.052817185118883135\n",
            "after loss data in training 0.05769619718194008 0.052822986560099615\n",
            "yyy epoch 841\n",
            "before loss data in training 0.0498809851706028 0.052822986560099615\n",
            "after loss data in training 0.0498809851706028 0.05281949249669166\n",
            "yyy epoch 842\n",
            "before loss data in training 0.04687664657831192 0.05281949249669166\n",
            "after loss data in training 0.04687664657831192 0.05281244285740533\n",
            "yyy epoch 843\n",
            "before loss data in training 0.045352376997470856 0.05281244285740533\n",
            "after loss data in training 0.045352376997470856 0.05280360391681299\n",
            "yyy epoch 844\n",
            "before loss data in training 0.05674045532941818 0.05280360391681299\n",
            "after loss data in training 0.05674045532941818 0.05280826291256755\n",
            "yyy epoch 845\n",
            "before loss data in training 0.04246646538376808 0.05280826291256755\n",
            "after loss data in training 0.04246646538376808 0.052796038565606794\n",
            "yyy epoch 846\n",
            "before loss data in training 0.03596631810069084 0.052796038565606794\n",
            "after loss data in training 0.03596631810069084 0.05277616876576628\n",
            "yyy epoch 847\n",
            "before loss data in training 0.05369613319635391 0.05277616876576628\n",
            "after loss data in training 0.05369613319635391 0.052777253629481595\n",
            "yyy epoch 848\n",
            "before loss data in training 0.05424245446920395 0.052777253629481595\n",
            "after loss data in training 0.05424245446920395 0.05277897942552367\n",
            "yyy epoch 849\n",
            "before loss data in training 0.05321769416332245 0.05277897942552367\n",
            "after loss data in training 0.05321769416332245 0.052779495560509315\n",
            "yyy epoch 850\n",
            "before loss data in training 0.03909677639603615 0.052779495560509315\n",
            "after loss data in training 0.03909677639603615 0.05276341715961099\n",
            "yyy epoch 851\n",
            "before loss data in training 0.059744540601968765 0.05276341715961099\n",
            "after loss data in training 0.059744540601968765 0.05277161096646822\n",
            "yyy epoch 852\n",
            "before loss data in training 0.035219427198171616 0.05277161096646822\n",
            "after loss data in training 0.035219427198171616 0.05275103396322286\n",
            "yyy epoch 853\n",
            "before loss data in training 0.04439322650432587 0.05275103396322286\n",
            "after loss data in training 0.04439322650432587 0.05274124730343492\n",
            "yyy epoch 854\n",
            "before loss data in training 0.04063979908823967 0.05274124730343492\n",
            "after loss data in training 0.04063979908823967 0.05272709356283235\n",
            "yyy epoch 855\n",
            "before loss data in training 0.04189760982990265 0.05272709356283235\n",
            "after loss data in training 0.04189760982990265 0.05271444229678921\n",
            "yyy epoch 856\n",
            "before loss data in training 0.05840946361422539 0.05271444229678921\n",
            "after loss data in training 0.05840946361422539 0.05272108759587607\n",
            "yyy epoch 857\n",
            "before loss data in training 0.056415166705846786 0.05272108759587607\n",
            "after loss data in training 0.056415166705846786 0.052725393049384194\n",
            "yyy epoch 858\n",
            "before loss data in training 0.04026374965906143 0.052725393049384194\n",
            "after loss data in training 0.04026374965906143 0.052710885897591035\n",
            "yyy epoch 859\n",
            "before loss data in training 0.041283875703811646 0.052710885897591035\n",
            "after loss data in training 0.041283875703811646 0.052697598676435475\n",
            "yyy epoch 860\n",
            "before loss data in training 0.0483192577958107 0.052697598676435475\n",
            "after loss data in training 0.0483192577958107 0.052692513495389455\n",
            "yyy epoch 861\n",
            "before loss data in training 0.05996161326766014 0.052692513495389455\n",
            "after loss data in training 0.05996161326766014 0.052700946325751716\n",
            "yyy epoch 862\n",
            "before loss data in training 0.04092121124267578 0.052700946325751716\n",
            "after loss data in training 0.04092121124267578 0.052687296574786394\n",
            "yyy epoch 863\n",
            "before loss data in training 0.04197074845433235 0.052687296574786394\n",
            "after loss data in training 0.04197074845433235 0.05267489316260994\n",
            "yyy epoch 864\n",
            "before loss data in training 0.05780346691608429 0.05267489316260994\n",
            "after loss data in training 0.05780346691608429 0.052680822149608175\n",
            "yyy epoch 865\n",
            "before loss data in training 0.04614789038896561 0.052680822149608175\n",
            "after loss data in training 0.04614789038896561 0.052673278348498886\n",
            "yyy epoch 866\n",
            "before loss data in training 0.07165117561817169 0.052673278348498886\n",
            "after loss data in training 0.07165117561817169 0.05269516750336587\n",
            "yyy epoch 867\n",
            "before loss data in training 0.04845532029867172 0.05269516750336587\n",
            "after loss data in training 0.04845532029867172 0.0526902828867706\n",
            "yyy epoch 868\n",
            "before loss data in training 0.04838069900870323 0.0526902828867706\n",
            "after loss data in training 0.04838069900870323 0.05268532364180159\n",
            "yyy epoch 869\n",
            "before loss data in training 0.0624704547226429 0.05268532364180159\n",
            "after loss data in training 0.0624704547226429 0.052696570918906005\n",
            "yyy epoch 870\n",
            "before loss data in training 0.042064130306243896 0.052696570918906005\n",
            "after loss data in training 0.042064130306243896 0.0526843637540235\n",
            "yyy epoch 871\n",
            "before loss data in training 0.051319580525159836 0.0526843637540235\n",
            "after loss data in training 0.051319580525159836 0.05268279863564178\n",
            "yyy epoch 872\n",
            "before loss data in training 0.03711976110935211 0.05268279863564178\n",
            "after loss data in training 0.03711976110935211 0.05266497155943755\n",
            "yyy epoch 873\n",
            "before loss data in training 0.06177932769060135 0.05266497155943755\n",
            "after loss data in training 0.06177932769060135 0.05267539988453041\n",
            "yyy epoch 874\n",
            "before loss data in training 0.03446168825030327 0.05267539988453041\n",
            "after loss data in training 0.03446168825030327 0.0526545842140913\n",
            "yyy epoch 875\n",
            "before loss data in training 0.04677439481019974 0.0526545842140913\n",
            "after loss data in training 0.04677439481019974 0.05264787166910968\n",
            "yyy epoch 876\n",
            "before loss data in training 0.04733176901936531 0.05264787166910968\n",
            "after loss data in training 0.04733176901936531 0.05264180997851704\n",
            "yyy epoch 877\n",
            "before loss data in training 0.043693602085113525 0.05264180997851704\n",
            "after loss data in training 0.043693602085113525 0.05263161839777285\n",
            "yyy epoch 878\n",
            "before loss data in training 0.0734882801771164 0.05263161839777285\n",
            "after loss data in training 0.0734882801771164 0.05265534611310771\n",
            "yyy epoch 879\n",
            "before loss data in training 0.042236339300870895 0.05265534611310771\n",
            "after loss data in training 0.042236339300870895 0.052643506332639256\n",
            "yyy epoch 880\n",
            "before loss data in training 0.05412739887833595 0.052643506332639256\n",
            "after loss data in training 0.05412739887833595 0.05264519066015991\n",
            "yyy epoch 881\n",
            "before loss data in training 0.040944088250398636 0.05264519066015991\n",
            "after loss data in training 0.040944088250398636 0.05263192410413977\n",
            "yyy epoch 882\n",
            "before loss data in training 0.05313486233353615 0.05263192410413977\n",
            "after loss data in training 0.05313486233353615 0.05263249368310851\n",
            "yyy epoch 883\n",
            "before loss data in training 0.06290321052074432 0.05263249368310851\n",
            "after loss data in training 0.06290321052074432 0.052644112141069635\n",
            "yyy epoch 884\n",
            "before loss data in training 0.06890183687210083 0.052644112141069635\n",
            "after loss data in training 0.06890183687210083 0.05266248245150018\n",
            "yyy epoch 885\n",
            "before loss data in training 0.06291352957487106 0.05266248245150018\n",
            "after loss data in training 0.06291352957487106 0.052674052482113466\n",
            "yyy epoch 886\n",
            "before loss data in training 0.053863588720560074 0.052674052482113466\n",
            "after loss data in training 0.053863588720560074 0.0526753935601726\n",
            "yyy epoch 887\n",
            "before loss data in training 0.03680625557899475 0.0526753935601726\n",
            "after loss data in training 0.03680625557899475 0.05265752290929289\n",
            "yyy epoch 888\n",
            "before loss data in training 0.04575551673769951 0.05265752290929289\n",
            "after loss data in training 0.04575551673769951 0.05264975912282316\n",
            "yyy epoch 889\n",
            "before loss data in training 0.039528269320726395 0.05264975912282316\n",
            "after loss data in training 0.039528269320726395 0.05263501587585451\n",
            "yyy epoch 890\n",
            "before loss data in training 0.05513054504990578 0.05263501587585451\n",
            "after loss data in training 0.05513054504990578 0.05263781669423167\n",
            "yyy epoch 891\n",
            "before loss data in training 0.057338617742061615 0.05263781669423167\n",
            "after loss data in training 0.057338617742061615 0.05264308665056332\n",
            "yyy epoch 892\n",
            "before loss data in training 0.05085228383541107 0.05264308665056332\n",
            "after loss data in training 0.05085228383541107 0.05264108127227087\n",
            "yyy epoch 893\n",
            "before loss data in training 0.057547178119421005 0.05264108127227087\n",
            "after loss data in training 0.057547178119421005 0.05264656907635046\n",
            "yyy epoch 894\n",
            "before loss data in training 0.06423656642436981 0.05264656907635046\n",
            "after loss data in training 0.06423656642436981 0.052659518794057746\n",
            "yyy epoch 895\n",
            "before loss data in training 0.03863214701414108 0.052659518794057746\n",
            "after loss data in training 0.03863214701414108 0.05264386324519623\n",
            "yyy epoch 896\n",
            "before loss data in training 0.06941322237253189 0.05264386324519623\n",
            "after loss data in training 0.06941322237253189 0.05266255818290787\n",
            "yyy epoch 897\n",
            "before loss data in training 0.05538162961602211 0.05266255818290787\n",
            "after loss data in training 0.05538162961602211 0.05266558610209842\n",
            "yyy epoch 898\n",
            "before loss data in training 0.03848426789045334 0.05266558610209842\n",
            "after loss data in training 0.03848426789045334 0.05264981155458825\n",
            "yyy epoch 899\n",
            "before loss data in training 0.04399867728352547 0.05264981155458825\n",
            "after loss data in training 0.04399867728352547 0.05264019918317596\n",
            "yyy epoch 900\n",
            "before loss data in training 0.057696741074323654 0.05264019918317596\n",
            "after loss data in training 0.057696741074323654 0.052645811327339274\n",
            "yyy epoch 901\n",
            "before loss data in training 0.04616832733154297 0.052645811327339274\n",
            "after loss data in training 0.04616832733154297 0.0526386300812242\n",
            "yyy epoch 902\n",
            "before loss data in training 0.05682762712240219 0.0526386300812242\n",
            "after loss data in training 0.05682762712240219 0.05264326905912141\n",
            "yyy epoch 903\n",
            "before loss data in training 0.06302047520875931 0.05264326905912141\n",
            "after loss data in training 0.06302047520875931 0.05265474826946393\n",
            "yyy epoch 904\n",
            "before loss data in training 0.06341991573572159 0.05265474826946393\n",
            "after loss data in training 0.06341991573572159 0.05266664348213383\n",
            "yyy epoch 905\n",
            "before loss data in training 0.06862279772758484 0.05266664348213383\n",
            "after loss data in training 0.06862279772758484 0.052684255131411374\n",
            "yyy epoch 906\n",
            "before loss data in training 0.054119259119033813 0.052684255131411374\n",
            "after loss data in training 0.054119259119033813 0.052685837274727386\n",
            "yyy epoch 907\n",
            "before loss data in training 0.05703973397612572 0.052685837274727386\n",
            "after loss data in training 0.05703973397612572 0.05269063231514743\n",
            "yyy epoch 908\n",
            "before loss data in training 0.046787988394498825 0.05269063231514743\n",
            "after loss data in training 0.046787988394498825 0.05268413875747895\n",
            "yyy epoch 909\n",
            "before loss data in training 0.05318785831332207 0.05268413875747895\n",
            "after loss data in training 0.05318785831332207 0.052684692295452404\n",
            "yyy epoch 910\n",
            "before loss data in training 0.0408477783203125 0.052684692295452404\n",
            "after loss data in training 0.0408477783203125 0.05267169897605049\n",
            "yyy epoch 911\n",
            "before loss data in training 0.048896871507167816 0.05267169897605049\n",
            "after loss data in training 0.048896871507167816 0.05266755991084338\n",
            "yyy epoch 912\n",
            "before loss data in training 0.05869802087545395 0.05266755991084338\n",
            "after loss data in training 0.05869802087545395 0.052674165015952484\n",
            "yyy epoch 913\n",
            "before loss data in training 0.039452385157346725 0.052674165015952484\n",
            "after loss data in training 0.039452385157346725 0.05265969917365642\n",
            "yyy epoch 914\n",
            "before loss data in training 0.04880718141794205 0.05265969917365642\n",
            "after loss data in training 0.04880718141794205 0.052655488771737605\n",
            "yyy epoch 915\n",
            "before loss data in training 0.052443839609622955 0.052655488771737605\n",
            "after loss data in training 0.052443839609622955 0.05265525771370036\n",
            "yyy epoch 916\n",
            "before loss data in training 0.05463451147079468 0.05265525771370036\n",
            "after loss data in training 0.05463451147079468 0.05265741611474408\n",
            "yyy epoch 917\n",
            "before loss data in training 0.050401970744132996 0.05265741611474408\n",
            "after loss data in training 0.050401970744132996 0.05265495920257566\n",
            "yyy epoch 918\n",
            "before loss data in training 0.052123066037893295 0.05265495920257566\n",
            "after loss data in training 0.052123066037893295 0.052654380428729435\n",
            "yyy epoch 919\n",
            "before loss data in training 0.0413968488574028 0.052654380428729435\n",
            "after loss data in training 0.0413968488574028 0.052642143981369294\n",
            "yyy epoch 920\n",
            "before loss data in training 0.05924789980053902 0.052642143981369294\n",
            "after loss data in training 0.05924789980053902 0.05264931635468001\n",
            "yyy epoch 921\n",
            "before loss data in training 0.07793895155191422 0.05264931635468001\n",
            "after loss data in training 0.07793895155191422 0.052676745460100004\n",
            "yyy epoch 922\n",
            "before loss data in training 0.058207884430885315 0.052676745460100004\n",
            "after loss data in training 0.058207884430885315 0.05268273802669891\n",
            "yyy epoch 923\n",
            "before loss data in training 0.06072629988193512 0.05268273802669891\n",
            "after loss data in training 0.06072629988193512 0.05269144318022189\n",
            "yyy epoch 924\n",
            "before loss data in training 0.039938393980264664 0.05269144318022189\n",
            "after loss data in training 0.039938393980264664 0.05267765610000572\n",
            "yyy epoch 925\n",
            "before loss data in training 0.051714543253183365 0.05267765610000572\n",
            "after loss data in training 0.051714543253183365 0.05267661602133745\n",
            "yyy epoch 926\n",
            "before loss data in training 0.05532417073845863 0.05267661602133745\n",
            "after loss data in training 0.05532417073845863 0.052679472067418485\n",
            "yyy epoch 927\n",
            "before loss data in training 0.050778377801179886 0.052679472067418485\n",
            "after loss data in training 0.050778377801179886 0.052677423474459174\n",
            "yyy epoch 928\n",
            "before loss data in training 0.05725652351975441 0.052677423474459174\n",
            "after loss data in training 0.05725652351975441 0.052682352538017084\n",
            "yyy epoch 929\n",
            "before loss data in training 0.04770416393876076 0.052682352538017084\n",
            "after loss data in training 0.04770416393876076 0.052676999647050145\n",
            "yyy epoch 930\n",
            "before loss data in training 0.04996607080101967 0.052676999647050145\n",
            "after loss data in training 0.04996607080101967 0.052674087800813806\n",
            "yyy epoch 931\n",
            "before loss data in training 0.045025840401649475 0.052674087800813806\n",
            "after loss data in training 0.045025840401649475 0.05266588152678037\n",
            "yyy epoch 932\n",
            "before loss data in training 0.046470414847135544 0.05266588152678037\n",
            "after loss data in training 0.046470414847135544 0.052659241155205185\n",
            "yyy epoch 933\n",
            "before loss data in training 0.06657523661851883 0.052659241155205185\n",
            "after loss data in training 0.06657523661851883 0.052674140507949635\n",
            "yyy epoch 934\n",
            "before loss data in training 0.06411010771989822 0.052674140507949635\n",
            "after loss data in training 0.06411010771989822 0.05268637148892498\n",
            "yyy epoch 935\n",
            "before loss data in training 0.05567410588264465 0.05268637148892498\n",
            "after loss data in training 0.05567410588264465 0.052689563512849896\n",
            "yyy epoch 936\n",
            "before loss data in training 0.05462215095758438 0.052689563512849896\n",
            "after loss data in training 0.05462215095758438 0.05269162603947181\n",
            "yyy epoch 937\n",
            "before loss data in training 0.04234736040234566 0.05269162603947181\n",
            "after loss data in training 0.04234736040234566 0.05268059803772648\n",
            "yyy epoch 938\n",
            "before loss data in training 0.03789028897881508 0.05268059803772648\n",
            "after loss data in training 0.03789028897881508 0.05266484690986821\n",
            "yyy epoch 939\n",
            "before loss data in training 0.059245310723781586 0.05266484690986821\n",
            "after loss data in training 0.059245310723781586 0.052671847403287265\n",
            "yyy epoch 940\n",
            "before loss data in training 0.047528065741062164 0.052671847403287265\n",
            "after loss data in training 0.047528065741062164 0.05266638111034122\n",
            "yyy epoch 941\n",
            "before loss data in training 0.05722089856863022 0.05266638111034122\n",
            "after loss data in training 0.05722089856863022 0.05267121605456446\n",
            "yyy epoch 942\n",
            "before loss data in training 0.05025128647685051 0.05267121605456446\n",
            "after loss data in training 0.05025128647685051 0.052668649851406756\n",
            "yyy epoch 943\n",
            "before loss data in training 0.04211549088358879 0.052668649851406756\n",
            "after loss data in training 0.04211549088358879 0.052657470657584915\n",
            "yyy epoch 944\n",
            "before loss data in training 0.045066870748996735 0.052657470657584915\n",
            "after loss data in training 0.045066870748996735 0.052649438276729264\n",
            "yyy epoch 945\n",
            "before loss data in training 0.03595216944813728 0.052649438276729264\n",
            "after loss data in training 0.03595216944813728 0.05263178788684703\n",
            "yyy epoch 946\n",
            "before loss data in training 0.04857410117983818 0.05263178788684703\n",
            "after loss data in training 0.04857410117983818 0.05262750310679739\n",
            "yyy epoch 947\n",
            "before loss data in training 0.031086811795830727 0.05262750310679739\n",
            "after loss data in training 0.031086811795830727 0.05260478085857907\n",
            "yyy epoch 948\n",
            "before loss data in training 0.03874151036143303 0.05260478085857907\n",
            "after loss data in training 0.03874151036143303 0.05259017256511527\n",
            "yyy epoch 949\n",
            "before loss data in training 0.06433514505624771 0.05259017256511527\n",
            "after loss data in training 0.06433514505624771 0.05260253569405331\n",
            "yyy epoch 950\n",
            "before loss data in training 0.0539553239941597 0.05260253569405331\n",
            "after loss data in training 0.0539553239941597 0.05260395818437939\n",
            "yyy epoch 951\n",
            "before loss data in training 0.04119617119431496 0.05260395818437939\n",
            "after loss data in training 0.04119617119431496 0.05259197521485202\n",
            "yyy epoch 952\n",
            "before loss data in training 0.044409871101379395 0.05259197521485202\n",
            "after loss data in training 0.044409871101379395 0.0525833895861915\n",
            "yyy epoch 953\n",
            "before loss data in training 0.04090501740574837 0.0525833895861915\n",
            "after loss data in training 0.04090501740574837 0.0525711481059185\n",
            "yyy epoch 954\n",
            "before loss data in training 0.03446932137012482 0.0525711481059185\n",
            "after loss data in training 0.03446932137012482 0.052552193313525\n",
            "yyy epoch 955\n",
            "before loss data in training 0.04634882137179375 0.052552193313525\n",
            "after loss data in training 0.04634882137179375 0.05254570443074076\n",
            "yyy epoch 956\n",
            "before loss data in training 0.04818381369113922 0.05254570443074076\n",
            "after loss data in training 0.04818381369113922 0.05254114655118005\n",
            "yyy epoch 957\n",
            "before loss data in training 0.05051582306623459 0.05254114655118005\n",
            "after loss data in training 0.05051582306623459 0.05253903243480745\n",
            "yyy epoch 958\n",
            "before loss data in training 0.05682382732629776 0.05253903243480745\n",
            "after loss data in training 0.05682382732629776 0.05254350041696751\n",
            "yyy epoch 959\n",
            "before loss data in training 0.050606440752744675 0.05254350041696751\n",
            "after loss data in training 0.050606440752744675 0.05254148264648394\n",
            "yyy epoch 960\n",
            "before loss data in training 0.04873653128743172 0.05254148264648394\n",
            "after loss data in training 0.04873653128743172 0.0525375232798252\n",
            "yyy epoch 961\n",
            "before loss data in training 0.05838822200894356 0.0525375232798252\n",
            "after loss data in training 0.05838822200894356 0.05254360508723593\n",
            "yyy epoch 962\n",
            "before loss data in training 0.05016208812594414 0.05254360508723593\n",
            "after loss data in training 0.05016208812594414 0.05254113206858453\n",
            "yyy epoch 963\n",
            "before loss data in training 0.05380700156092644 0.05254113206858453\n",
            "after loss data in training 0.05380700156092644 0.052542445211211444\n",
            "yyy epoch 964\n",
            "before loss data in training 0.054874248802661896 0.052542445211211444\n",
            "after loss data in training 0.054874248802661896 0.05254486158799015\n",
            "yyy epoch 965\n",
            "before loss data in training 0.04928147792816162 0.05254486158799015\n",
            "after loss data in training 0.04928147792816162 0.052541483344035876\n",
            "yyy epoch 966\n",
            "before loss data in training 0.03608841076493263 0.052541483344035876\n",
            "after loss data in training 0.03608841076493263 0.05252446879121364\n",
            "yyy epoch 967\n",
            "before loss data in training 0.057566601783037186 0.05252446879121364\n",
            "after loss data in training 0.057566601783037186 0.052529677606287836\n",
            "yyy epoch 968\n",
            "before loss data in training 0.055133577436208725 0.052529677606287836\n",
            "after loss data in training 0.055133577436208725 0.052532364809414686\n",
            "yyy epoch 969\n",
            "before loss data in training 0.04125019162893295 0.052532364809414686\n",
            "after loss data in training 0.04125019162893295 0.052520733703043054\n",
            "yyy epoch 970\n",
            "before loss data in training 0.04679378867149353 0.052520733703043054\n",
            "after loss data in training 0.04679378867149353 0.052514835716398825\n",
            "yyy epoch 971\n",
            "before loss data in training 0.04574029892683029 0.052514835716398825\n",
            "after loss data in training 0.04574029892683029 0.052507866028343717\n",
            "yyy epoch 972\n",
            "before loss data in training 0.04839705675840378 0.052507866028343717\n",
            "after loss data in training 0.04839705675840378 0.052503641147285195\n",
            "yyy epoch 973\n",
            "before loss data in training 0.03008122928440571 0.052503641147285195\n",
            "after loss data in training 0.03008122928440571 0.052480620190547125\n",
            "yyy epoch 974\n",
            "before loss data in training 0.05050255358219147 0.052480620190547125\n",
            "after loss data in training 0.05050255358219147 0.05247859140428215\n",
            "yyy epoch 975\n",
            "before loss data in training 0.04647299274802208 0.05247859140428215\n",
            "after loss data in training 0.04647299274802208 0.0524724381269704\n",
            "yyy epoch 976\n",
            "before loss data in training 0.04606118053197861 0.0524724381269704\n",
            "after loss data in training 0.04606118053197861 0.052465875939053316\n",
            "yyy epoch 977\n",
            "before loss data in training 0.044441696256399155 0.052465875939053316\n",
            "after loss data in training 0.044441696256399155 0.052457671256351215\n",
            "yyy epoch 978\n",
            "before loss data in training 0.05008574202656746 0.052457671256351215\n",
            "after loss data in training 0.05008574202656746 0.05245524844814919\n",
            "yyy epoch 979\n",
            "before loss data in training 0.05291806906461716 0.05245524844814919\n",
            "after loss data in training 0.05291806906461716 0.05245572071408436\n",
            "yyy epoch 980\n",
            "before loss data in training 0.04533779248595238 0.05245572071408436\n",
            "after loss data in training 0.04533779248595238 0.05244846492588035\n",
            "yyy epoch 981\n",
            "before loss data in training 0.055401694029569626 0.05244846492588035\n",
            "after loss data in training 0.055401694029569626 0.05245147228749307\n",
            "yyy epoch 982\n",
            "before loss data in training 0.04377586767077446 0.05245147228749307\n",
            "after loss data in training 0.04377586767077446 0.052442646646987764\n",
            "yyy epoch 983\n",
            "before loss data in training 0.036917272955179214 0.052442646646987764\n",
            "after loss data in training 0.036917272955179214 0.05242686882819528\n",
            "yyy epoch 984\n",
            "before loss data in training 0.04726707562804222 0.05242686882819528\n",
            "after loss data in training 0.04726707562804222 0.05242163045946416\n",
            "yyy epoch 985\n",
            "before loss data in training 0.042690932750701904 0.05242163045946416\n",
            "after loss data in training 0.042690932750701904 0.052411761597690565\n",
            "yyy epoch 986\n",
            "before loss data in training 0.07690342515707016 0.052411761597690565\n",
            "after loss data in training 0.07690342515707016 0.052436575846484264\n",
            "yyy epoch 987\n",
            "before loss data in training 0.056364934891462326 0.052436575846484264\n",
            "after loss data in training 0.056364934891462326 0.052440551918392135\n",
            "yyy epoch 988\n",
            "before loss data in training 0.06853532046079636 0.052440551918392135\n",
            "after loss data in training 0.06853532046079636 0.0524568256985159\n",
            "yyy epoch 989\n",
            "before loss data in training 0.051086269319057465 0.0524568256985159\n",
            "after loss data in training 0.051086269319057465 0.052455441298132614\n",
            "yyy epoch 990\n",
            "before loss data in training 0.05963204428553581 0.052455441298132614\n",
            "after loss data in training 0.05963204428553581 0.052462683077131006\n",
            "yyy epoch 991\n",
            "before loss data in training 0.06140446290373802 0.052462683077131006\n",
            "after loss data in training 0.06140446290373802 0.05247169696808525\n",
            "yyy epoch 992\n",
            "before loss data in training 0.05589580535888672 0.05247169696808525\n",
            "after loss data in training 0.05589580535888672 0.05247514521419885\n",
            "yyy epoch 993\n",
            "before loss data in training 0.054888010025024414 0.05247514521419885\n",
            "after loss data in training 0.054888010025024414 0.052477572643586\n",
            "yyy epoch 994\n",
            "before loss data in training 0.04025782644748688 0.052477572643586\n",
            "after loss data in training 0.04025782644748688 0.052465291491630116\n",
            "yyy epoch 995\n",
            "before loss data in training 0.051749054342508316 0.052465291491630116\n",
            "after loss data in training 0.051749054342508316 0.05246457237802658\n",
            "yyy epoch 996\n",
            "before loss data in training 0.046288613229990005 0.05246457237802658\n",
            "after loss data in training 0.046288613229990005 0.05245837783525022\n",
            "yyy epoch 997\n",
            "before loss data in training 0.04704952985048294 0.05245837783525022\n",
            "after loss data in training 0.04704952985048294 0.05245295814789073\n",
            "yyy epoch 998\n",
            "before loss data in training 0.05620504170656204 0.05245295814789073\n",
            "after loss data in training 0.05620504170656204 0.052456713987288794\n",
            "yyy epoch 999\n",
            "before loss data in training 0.04234105348587036 0.052456713987288794\n",
            "after loss data in training 0.04234105348587036 0.05244659832678738\n",
            "yyy epoch 1000\n",
            "before loss data in training 0.046374205499887466 0.05244659832678738\n",
            "after loss data in training 0.046374205499887466 0.05244053200028698\n",
            "yyy epoch 1001\n",
            "before loss data in training 0.04759891703724861 0.05244053200028698\n",
            "after loss data in training 0.04759891703724861 0.05243570004922606\n",
            "yyy epoch 1002\n",
            "before loss data in training 0.04923079535365105 0.05243570004922606\n",
            "after loss data in training 0.04923079535365105 0.052432504730486704\n",
            "yyy epoch 1003\n",
            "before loss data in training 0.042204730212688446 0.052432504730486704\n",
            "after loss data in training 0.042204730212688446 0.05242231770407455\n",
            "yyy epoch 1004\n",
            "before loss data in training 0.03864221274852753 0.05242231770407455\n",
            "after loss data in training 0.03864221274852753 0.0524086061568551\n",
            "yyy epoch 1005\n",
            "before loss data in training 0.03138040751218796 0.0524086061568551\n",
            "after loss data in training 0.03138040751218796 0.05238770337490215\n",
            "yyy epoch 1006\n",
            "before loss data in training 0.05217083916068077 0.05238770337490215\n",
            "after loss data in training 0.05217083916068077 0.05238748801818495\n",
            "yyy epoch 1007\n",
            "before loss data in training 0.040639232844114304 0.05238748801818495\n",
            "after loss data in training 0.040639232844114304 0.05237583300313131\n",
            "yyy epoch 1008\n",
            "before loss data in training 0.03995922952890396 0.05237583300313131\n",
            "after loss data in training 0.03995922952890396 0.05236352715231444\n",
            "yyy epoch 1009\n",
            "before loss data in training 0.0684150904417038 0.05236352715231444\n",
            "after loss data in training 0.0684150904417038 0.052379419789234626\n",
            "yyy epoch 1010\n",
            "before loss data in training 0.05275306478142738 0.052379419789234626\n",
            "after loss data in training 0.05275306478142738 0.052379789368851036\n",
            "yyy epoch 1011\n",
            "before loss data in training 0.05559242144227028 0.052379789368851036\n",
            "after loss data in training 0.05559242144227028 0.05238296390647299\n",
            "yyy epoch 1012\n",
            "before loss data in training 0.04553471505641937 0.05238296390647299\n",
            "after loss data in training 0.04553471505641937 0.052376203542356455\n",
            "yyy epoch 1013\n",
            "before loss data in training 0.04968736320734024 0.052376203542356455\n",
            "after loss data in training 0.04968736320734024 0.05237355182604973\n",
            "yyy epoch 1014\n",
            "before loss data in training 0.06285520642995834 0.05237355182604973\n",
            "after loss data in training 0.06285520642995834 0.05238387857935407\n",
            "yyy epoch 1015\n",
            "before loss data in training 0.06316065788269043 0.05238387857935407\n",
            "after loss data in training 0.06316065788269043 0.05239448564559752\n",
            "yyy epoch 1016\n",
            "before loss data in training 0.05401693284511566 0.05239448564559752\n",
            "after loss data in training 0.05401693284511566 0.05239608097224405\n",
            "yyy epoch 1017\n",
            "before loss data in training 0.06213611364364624 0.05239608097224405\n",
            "after loss data in training 0.06213611364364624 0.05240564878429847\n",
            "yyy epoch 1018\n",
            "before loss data in training 0.0374336801469326 0.05240564878429847\n",
            "after loss data in training 0.0374336801469326 0.05239095597896249\n",
            "yyy epoch 1019\n",
            "before loss data in training 0.04515368491411209 0.05239095597896249\n",
            "after loss data in training 0.04515368491411209 0.05238386061517342\n",
            "yyy epoch 1020\n",
            "before loss data in training 0.057926300913095474 0.05238386061517342\n",
            "after loss data in training 0.057926300913095474 0.05238928905816845\n",
            "yyy epoch 1021\n",
            "before loss data in training 0.0308273583650589 0.05238928905816845\n",
            "after loss data in training 0.0308273583650589 0.05236819127862529\n",
            "yyy epoch 1022\n",
            "before loss data in training 0.04319503903388977 0.05236819127862529\n",
            "after loss data in training 0.04319503903388977 0.05235922436538508\n",
            "yyy epoch 1023\n",
            "before loss data in training 0.04823432117700577 0.05235922436538508\n",
            "after loss data in training 0.04823432117700577 0.05235519613961518\n",
            "yyy epoch 1024\n",
            "before loss data in training 0.050166621804237366 0.05235519613961518\n",
            "after loss data in training 0.050166621804237366 0.05235306094514164\n",
            "yyy epoch 1025\n",
            "before loss data in training 0.039408303797245026 0.05235306094514164\n",
            "after loss data in training 0.039408303797245026 0.05234044422277527\n",
            "yyy epoch 1026\n",
            "before loss data in training 0.0547579750418663 0.05234044422277527\n",
            "after loss data in training 0.0547579750418663 0.05234279819630895\n",
            "yyy epoch 1027\n",
            "before loss data in training 0.057929955422878265 0.05234279819630895\n",
            "after loss data in training 0.057929955422878265 0.05234823317415581\n",
            "yyy epoch 1028\n",
            "before loss data in training 0.031736601144075394 0.05234823317415581\n",
            "after loss data in training 0.031736601144075394 0.0523282024336018\n",
            "yyy epoch 1029\n",
            "before loss data in training 0.03479905053973198 0.0523282024336018\n",
            "after loss data in training 0.03479905053973198 0.05231118383953008\n",
            "yyy epoch 1030\n",
            "before loss data in training 0.03151785582304001 0.05231118383953008\n",
            "after loss data in training 0.03151785582304001 0.05229101572312223\n",
            "yyy epoch 1031\n",
            "before loss data in training 0.040042273700237274 0.05229101572312223\n",
            "after loss data in training 0.040042273700237274 0.052279146787053546\n",
            "yyy epoch 1032\n",
            "before loss data in training 0.045310985296964645 0.052279146787053546\n",
            "after loss data in training 0.045310985296964645 0.05227240122897989\n",
            "yyy epoch 1033\n",
            "before loss data in training 0.04675060138106346 0.05227240122897989\n",
            "after loss data in training 0.04675060138106346 0.05226706099701865\n",
            "yyy epoch 1034\n",
            "before loss data in training 0.05570891126990318 0.05226706099701865\n",
            "after loss data in training 0.05570891126990318 0.05227038645621951\n",
            "yyy epoch 1035\n",
            "before loss data in training 0.048526667058467865 0.05227038645621951\n",
            "after loss data in training 0.048526667058467865 0.0522667728274572\n",
            "yyy epoch 1036\n",
            "before loss data in training 0.04671517387032509 0.0522667728274572\n",
            "after loss data in training 0.04671517387032509 0.052261419308694296\n",
            "yyy epoch 1037\n",
            "before loss data in training 0.049082327634096146 0.052261419308694296\n",
            "after loss data in training 0.049082327634096146 0.052258356599951906\n",
            "yyy epoch 1038\n",
            "before loss data in training 0.03862352296710014 0.052258356599951906\n",
            "after loss data in training 0.03862352296710014 0.052245233564694106\n",
            "yyy epoch 1039\n",
            "before loss data in training 0.038655396550893784 0.052245233564694106\n",
            "after loss data in training 0.038655396550893784 0.0522321664137193\n",
            "yyy epoch 1040\n",
            "before loss data in training 0.04343736916780472 0.0522321664137193\n",
            "after loss data in training 0.04343736916780472 0.05222371800137932\n",
            "yyy epoch 1041\n",
            "before loss data in training 0.05032949149608612 0.05222371800137932\n",
            "after loss data in training 0.05032949149608612 0.052221900125654475\n",
            "yyy epoch 1042\n",
            "before loss data in training 0.042094018310308456 0.052221900125654475\n",
            "after loss data in training 0.042094018310308456 0.0522121897883435\n",
            "yyy epoch 1043\n",
            "before loss data in training 0.05572805553674698 0.0522121897883435\n",
            "after loss data in training 0.05572805553674698 0.05221555747584197\n",
            "yyy epoch 1044\n",
            "before loss data in training 0.050259362906217575 0.05221555747584197\n",
            "after loss data in training 0.050259362906217575 0.05221368551931602\n",
            "yyy epoch 1045\n",
            "before loss data in training 0.03976857662200928 0.05221368551931602\n",
            "after loss data in training 0.03976857662200928 0.05220178770966276\n",
            "yyy epoch 1046\n",
            "before loss data in training 0.04910406097769737 0.05220178770966276\n",
            "after loss data in training 0.04910406097769737 0.052198829040386766\n",
            "yyy epoch 1047\n",
            "before loss data in training 0.0549638569355011 0.052198829040386766\n",
            "after loss data in training 0.0549638569355011 0.05220146742578287\n",
            "yyy epoch 1048\n",
            "before loss data in training 0.04945141449570656 0.05220146742578287\n",
            "after loss data in training 0.04945141449570656 0.052198845830997286\n",
            "yyy epoch 1049\n",
            "before loss data in training 0.04493061825633049 0.052198845830997286\n",
            "after loss data in training 0.04493061825633049 0.0521919237094976\n",
            "yyy epoch 1050\n",
            "before loss data in training 0.03389524295926094 0.0521919237094976\n",
            "after loss data in training 0.03389524295926094 0.05217451487909776\n",
            "yyy epoch 1051\n",
            "before loss data in training 0.05205043405294418 0.05217451487909776\n",
            "after loss data in training 0.05205043405294418 0.05217439693154438\n",
            "yyy epoch 1052\n",
            "before loss data in training 0.04124994948506355 0.05217439693154438\n",
            "after loss data in training 0.04124994948506355 0.052164022337578116\n",
            "yyy epoch 1053\n",
            "before loss data in training 0.046539057046175 0.052164022337578116\n",
            "after loss data in training 0.046539057046175 0.05215868555836426\n",
            "yyy epoch 1054\n",
            "before loss data in training 0.05003586411476135 0.05215868555836426\n",
            "after loss data in training 0.05003586411476135 0.052156673405337146\n",
            "yyy epoch 1055\n",
            "before loss data in training 0.04717273637652397 0.052156673405337146\n",
            "after loss data in training 0.04717273637652397 0.052151953767999255\n",
            "yyy epoch 1056\n",
            "before loss data in training 0.05016399174928665 0.052151953767999255\n",
            "after loss data in training 0.05016399174928665 0.05215007300923037\n",
            "yyy epoch 1057\n",
            "before loss data in training 0.04875064641237259 0.05215007300923037\n",
            "after loss data in training 0.04875064641237259 0.052146859940613306\n",
            "yyy epoch 1058\n",
            "before loss data in training 0.04105232283473015 0.052146859940613306\n",
            "after loss data in training 0.04105232283473015 0.05213638351275128\n",
            "yyy epoch 1059\n",
            "before loss data in training 0.05035538971424103 0.05213638351275128\n",
            "after loss data in training 0.05035538971424103 0.0521347033299225\n",
            "yyy epoch 1060\n",
            "before loss data in training 0.040108442306518555 0.0521347033299225\n",
            "after loss data in training 0.040108442306518555 0.05212336849389667\n",
            "yyy epoch 1061\n",
            "before loss data in training 0.05642518773674965 0.05212336849389667\n",
            "after loss data in training 0.05642518773674965 0.052127419171149825\n",
            "yyy epoch 1062\n",
            "before loss data in training 0.05150661617517471 0.052127419171149825\n",
            "after loss data in training 0.05150661617517471 0.05212683516080554\n",
            "yyy epoch 1063\n",
            "before loss data in training 0.03307163342833519 0.05212683516080554\n",
            "after loss data in training 0.03307163342833519 0.05210892613662089\n",
            "yyy epoch 1064\n",
            "before loss data in training 0.060326553881168365 0.05210892613662089\n",
            "after loss data in training 0.060326553881168365 0.052116642219010134\n",
            "yyy epoch 1065\n",
            "before loss data in training 0.06233544275164604 0.052116642219010134\n",
            "after loss data in training 0.06233544275164604 0.0521262283358325\n",
            "yyy epoch 1066\n",
            "before loss data in training 0.05230772867798805 0.0521262283358325\n",
            "after loss data in training 0.05230772867798805 0.05212639843924595\n",
            "yyy epoch 1067\n",
            "before loss data in training 0.031827542930841446 0.05212639843924595\n",
            "after loss data in training 0.031827542930841446 0.05210739202023059\n",
            "yyy epoch 1068\n",
            "before loss data in training 0.05191619694232941 0.05210739202023059\n",
            "after loss data in training 0.05191619694232941 0.0521072131660885\n",
            "yyy epoch 1069\n",
            "before loss data in training 0.05683236941695213 0.0521072131660885\n",
            "after loss data in training 0.05683236941695213 0.052111629199967806\n",
            "yyy epoch 1070\n",
            "before loss data in training 0.05052049085497856 0.052111629199967806\n",
            "after loss data in training 0.05052049085497856 0.0521101435432498\n",
            "yyy epoch 1071\n",
            "before loss data in training 0.04751070588827133 0.0521101435432498\n",
            "after loss data in training 0.04751070588827133 0.05210585302304926\n",
            "yyy epoch 1072\n",
            "before loss data in training 0.047441087663173676 0.05210585302304926\n",
            "after loss data in training 0.047441087663173676 0.052101505618240423\n",
            "yyy epoch 1073\n",
            "before loss data in training 0.055759068578481674 0.052101505618240423\n",
            "after loss data in training 0.055759068578481674 0.05210491117034493\n",
            "yyy epoch 1074\n",
            "before loss data in training 0.04278029501438141 0.05210491117034493\n",
            "after loss data in training 0.04278029501438141 0.0520962371088045\n",
            "yyy epoch 1075\n",
            "before loss data in training 0.04930232837796211 0.0520962371088045\n",
            "after loss data in training 0.04930232837796211 0.05209364053935204\n",
            "yyy epoch 1076\n",
            "before loss data in training 0.03986719250679016 0.05209364053935204\n",
            "after loss data in training 0.03986719250679016 0.05208228821991605\n",
            "yyy epoch 1077\n",
            "before loss data in training 0.04110762104392052 0.05208228821991605\n",
            "after loss data in training 0.04110762104392052 0.05207210763812013\n",
            "yyy epoch 1078\n",
            "before loss data in training 0.06665447354316711 0.05207210763812013\n",
            "after loss data in training 0.06665447354316711 0.05208562234238802\n",
            "yyy epoch 1079\n",
            "before loss data in training 0.05455992743372917 0.05208562234238802\n",
            "after loss data in training 0.05455992743372917 0.05208791336562074\n",
            "yyy epoch 1080\n",
            "before loss data in training 0.052129317075014114 0.05208791336562074\n",
            "after loss data in training 0.052129317075014114 0.05208795166692453\n",
            "yyy epoch 1081\n",
            "before loss data in training 0.05325513705611229 0.05208795166692453\n",
            "after loss data in training 0.05325513705611229 0.0520890303964894\n",
            "yyy epoch 1082\n",
            "before loss data in training 0.04550936445593834 0.0520890303964894\n",
            "after loss data in training 0.04550936445593834 0.052082954989342074\n",
            "yyy epoch 1083\n",
            "before loss data in training 0.041591793298721313 0.052082954989342074\n",
            "after loss data in training 0.041591793298721313 0.05207327679590054\n",
            "yyy epoch 1084\n",
            "before loss data in training 0.056893445551395416 0.05207327679590054\n",
            "after loss data in training 0.056893445551395416 0.05207771934774892\n",
            "yyy epoch 1085\n",
            "before loss data in training 0.0574788823723793 0.05207771934774892\n",
            "after loss data in training 0.0574788823723793 0.0520826927943646\n",
            "yyy epoch 1086\n",
            "before loss data in training 0.052583929151296616 0.0520826927943646\n",
            "after loss data in training 0.052583929151296616 0.05208315391336822\n",
            "yyy epoch 1087\n",
            "before loss data in training 0.041056402027606964 0.05208315391336822\n",
            "after loss data in training 0.041056402027606964 0.05207301903112028\n",
            "yyy epoch 1088\n",
            "before loss data in training 0.04035428166389465 0.05207301903112028\n",
            "after loss data in training 0.04035428166389465 0.05206225802343688\n",
            "yyy epoch 1089\n",
            "before loss data in training 0.036442309617996216 0.05206225802343688\n",
            "after loss data in training 0.036442309617996216 0.05204792779554198\n",
            "yyy epoch 1090\n",
            "before loss data in training 0.033105283975601196 0.05204792779554198\n",
            "after loss data in training 0.033105283975601196 0.05203056515226064\n",
            "yyy epoch 1091\n",
            "before loss data in training 0.05499443784356117 0.05203056515226064\n",
            "after loss data in training 0.05499443784356117 0.05203327932139187\n",
            "yyy epoch 1092\n",
            "before loss data in training 0.0432891808450222 0.05203327932139187\n",
            "after loss data in training 0.0432891808450222 0.052025279231294554\n",
            "yyy epoch 1093\n",
            "before loss data in training 0.053618885576725006 0.052025279231294554\n",
            "after loss data in training 0.053618885576725006 0.05202673590985528\n",
            "yyy epoch 1094\n",
            "before loss data in training 0.04544822499155998 0.05202673590985528\n",
            "after loss data in training 0.04544822499155998 0.05202072813732716\n",
            "yyy epoch 1095\n",
            "before loss data in training 0.03880841284990311 0.05202072813732716\n",
            "after loss data in training 0.03880841284990311 0.052008673105130604\n",
            "yyy epoch 1096\n",
            "before loss data in training 0.05973796918988228 0.052008673105130604\n",
            "after loss data in training 0.05973796918988228 0.05201571895388608\n",
            "yyy epoch 1097\n",
            "before loss data in training 0.050014253705739975 0.05201571895388608\n",
            "after loss data in training 0.050014253705739975 0.05201389612579123\n",
            "yyy epoch 1098\n",
            "before loss data in training 0.043108582496643066 0.05201389612579123\n",
            "after loss data in training 0.043108582496643066 0.052005793019668255\n",
            "yyy epoch 1099\n",
            "before loss data in training 0.051335468888282776 0.052005793019668255\n",
            "after loss data in training 0.051335468888282776 0.05200518363409427\n",
            "yyy epoch 1100\n",
            "before loss data in training 0.042025573551654816 0.05200518363409427\n",
            "after loss data in training 0.042025573551654816 0.05199611950141267\n",
            "yyy epoch 1101\n",
            "before loss data in training 0.04333041235804558 0.05199611950141267\n",
            "after loss data in training 0.04333041235804558 0.05198825588331524\n",
            "yyy epoch 1102\n",
            "before loss data in training 0.03965918719768524 0.05198825588331524\n",
            "after loss data in training 0.03965918719768524 0.051977078123854104\n",
            "yyy epoch 1103\n",
            "before loss data in training 0.038511116057634354 0.051977078123854104\n",
            "after loss data in training 0.038511116057634354 0.0519648806944463\n",
            "yyy epoch 1104\n",
            "before loss data in training 0.05825699865818024 0.0519648806944463\n",
            "after loss data in training 0.05825699865818024 0.051970574918847864\n",
            "yyy epoch 1105\n",
            "before loss data in training 0.04197138920426369 0.051970574918847864\n",
            "after loss data in training 0.04197138920426369 0.05196153406377139\n",
            "yyy epoch 1106\n",
            "before loss data in training 0.04137521609663963 0.05196153406377139\n",
            "after loss data in training 0.04137521609663963 0.051951970994243715\n",
            "yyy epoch 1107\n",
            "before loss data in training 0.0630795806646347 0.051951970994243715\n",
            "after loss data in training 0.0630795806646347 0.05196201396326031\n",
            "yyy epoch 1108\n",
            "before loss data in training 0.05348031967878342 0.05196201396326031\n",
            "after loss data in training 0.05348031967878342 0.051963383039649425\n",
            "yyy epoch 1109\n",
            "before loss data in training 0.055055972188711166 0.051963383039649425\n",
            "after loss data in training 0.055055972188711166 0.05196616915599993\n",
            "yyy epoch 1110\n",
            "before loss data in training 0.04874113202095032 0.05196616915599993\n",
            "after loss data in training 0.04874113202095032 0.05196326633229601\n",
            "yyy epoch 1111\n",
            "before loss data in training 0.03546896204352379 0.05196326633229601\n",
            "after loss data in training 0.03546896204352379 0.05194843332484208\n",
            "yyy epoch 1112\n",
            "before loss data in training 0.042334653437137604 0.05194843332484208\n",
            "after loss data in training 0.042334653437137604 0.05193979560706337\n",
            "yyy epoch 1113\n",
            "before loss data in training 0.0445256382226944 0.05193979560706337\n",
            "after loss data in training 0.0445256382226944 0.051933140169554956\n",
            "yyy epoch 1114\n",
            "before loss data in training 0.041694846004247665 0.051933140169554956\n",
            "after loss data in training 0.041694846004247665 0.0519239578429493\n",
            "yyy epoch 1115\n",
            "before loss data in training 0.054499126970767975 0.0519239578429493\n",
            "after loss data in training 0.054499126970767975 0.05192626534216777\n",
            "yyy epoch 1116\n",
            "before loss data in training 0.05013908073306084 0.05192626534216777\n",
            "after loss data in training 0.05013908073306084 0.05192466535594655\n",
            "yyy epoch 1117\n",
            "before loss data in training 0.03951519727706909 0.05192466535594655\n",
            "after loss data in training 0.03951519727706909 0.05191356565283486\n",
            "yyy epoch 1118\n",
            "before loss data in training 0.07096721231937408 0.05191356565283486\n",
            "after loss data in training 0.07096721231937408 0.05193059304038315\n",
            "yyy epoch 1119\n",
            "before loss data in training 0.03765568509697914 0.05193059304038315\n",
            "after loss data in training 0.03765568509697914 0.051917847586862256\n",
            "yyy epoch 1120\n",
            "before loss data in training 0.051763564348220825 0.051917847586862256\n",
            "after loss data in training 0.051763564348220825 0.051917709956854546\n",
            "yyy epoch 1121\n",
            "before loss data in training 0.05896603316068649 0.051917709956854546\n",
            "after loss data in training 0.05896603316068649 0.0519239918848437\n",
            "yyy epoch 1122\n",
            "before loss data in training 0.04007171839475632 0.0519239918848437\n",
            "after loss data in training 0.04007171839475632 0.05191343776775546\n",
            "yyy epoch 1123\n",
            "before loss data in training 0.056069325655698776 0.05191343776775546\n",
            "after loss data in training 0.056069325655698776 0.051917135176908435\n",
            "yyy epoch 1124\n",
            "before loss data in training 0.04320722445845604 0.051917135176908435\n",
            "after loss data in training 0.04320722445845604 0.05190939303404759\n",
            "yyy epoch 1125\n",
            "before loss data in training 0.035208530724048615 0.05190939303404759\n",
            "after loss data in training 0.035208530724048615 0.051894561007129296\n",
            "yyy epoch 1126\n",
            "before loss data in training 0.045731279999017715 0.051894561007129296\n",
            "after loss data in training 0.045731279999017715 0.05188909225734393\n",
            "yyy epoch 1127\n",
            "before loss data in training 0.04189379885792732 0.05188909225734393\n",
            "after loss data in training 0.04189379885792732 0.05188023118163523\n",
            "yyy epoch 1128\n",
            "before loss data in training 0.04263456165790558 0.05188023118163523\n",
            "after loss data in training 0.04263456165790558 0.05187204192607833\n",
            "yyy epoch 1129\n",
            "before loss data in training 0.059656769037246704 0.05187204192607833\n",
            "after loss data in training 0.059656769037246704 0.051878931065114764\n",
            "yyy epoch 1130\n",
            "before loss data in training 0.04953606054186821 0.051878931065114764\n",
            "after loss data in training 0.04953606054186821 0.05187685956155752\n",
            "yyy epoch 1131\n",
            "before loss data in training 0.04123157635331154 0.05187685956155752\n",
            "after loss data in training 0.04123157635331154 0.051867455601126204\n",
            "yyy epoch 1132\n",
            "before loss data in training 0.04230188950896263 0.051867455601126204\n",
            "after loss data in training 0.04230188950896263 0.05185901291260708\n",
            "yyy epoch 1133\n",
            "before loss data in training 0.058232296258211136 0.05185901291260708\n",
            "after loss data in training 0.058232296258211136 0.0518646330919242\n",
            "yyy epoch 1134\n",
            "before loss data in training 0.034401074051856995 0.0518646330919242\n",
            "after loss data in training 0.034401074051856995 0.051849246696294184\n",
            "yyy epoch 1135\n",
            "before loss data in training 0.031776588410139084 0.051849246696294184\n",
            "after loss data in training 0.031776588410139084 0.05183157710273243\n",
            "yyy epoch 1136\n",
            "before loss data in training 0.0739622488617897 0.05183157710273243\n",
            "after loss data in training 0.0739622488617897 0.0518510411939893\n",
            "yyy epoch 1137\n",
            "before loss data in training 0.06487923860549927 0.0518510411939893\n",
            "after loss data in training 0.06487923860549927 0.05186248952211892\n",
            "yyy epoch 1138\n",
            "before loss data in training 0.04865037277340889 0.05186248952211892\n",
            "after loss data in training 0.04865037277340889 0.0518596694020586\n",
            "yyy epoch 1139\n",
            "before loss data in training 0.05164961889386177 0.0518596694020586\n",
            "after loss data in training 0.05164961889386177 0.051859485147226846\n",
            "yyy epoch 1140\n",
            "before loss data in training 0.05457122623920441 0.051859485147226846\n",
            "after loss data in training 0.05457122623920441 0.05186186178271499\n",
            "yyy epoch 1141\n",
            "before loss data in training 0.053208254277706146 0.05186186178271499\n",
            "after loss data in training 0.053208254277706146 0.05186304076038136\n",
            "yyy epoch 1142\n",
            "before loss data in training 0.05723608657717705 0.05186304076038136\n",
            "after loss data in training 0.05723608657717705 0.051867741587867625\n",
            "yyy epoch 1143\n",
            "before loss data in training 0.04851047322154045 0.051867741587867625\n",
            "after loss data in training 0.04851047322154045 0.05186480691272223\n",
            "yyy epoch 1144\n",
            "before loss data in training 0.048968300223350525 0.05186480691272223\n",
            "after loss data in training 0.048968300223350525 0.05186227721255684\n",
            "yyy epoch 1145\n",
            "before loss data in training 0.06355331093072891 0.05186227721255684\n",
            "after loss data in training 0.06355331093072891 0.05187247881265995\n",
            "yyy epoch 1146\n",
            "before loss data in training 0.03829795494675636 0.05187247881265995\n",
            "after loss data in training 0.03829795494675636 0.05186064400545341\n",
            "yyy epoch 1147\n",
            "before loss data in training 0.050711847841739655 0.05186064400545341\n",
            "after loss data in training 0.050711847841739655 0.05185964331193101\n",
            "yyy epoch 1148\n",
            "before loss data in training 0.04964739829301834 0.05185964331193101\n",
            "after loss data in training 0.04964739829301834 0.051857717946379306\n",
            "yyy epoch 1149\n",
            "before loss data in training 0.036805905401706696 0.051857717946379306\n",
            "after loss data in training 0.036805905401706696 0.05184462941373177\n",
            "yyy epoch 1150\n",
            "before loss data in training 0.052630119025707245 0.05184462941373177\n",
            "after loss data in training 0.052630119025707245 0.05184531185474999\n",
            "yyy epoch 1151\n",
            "before loss data in training 0.03613865748047829 0.05184531185474999\n",
            "after loss data in training 0.03613865748047829 0.05183167760616121\n",
            "yyy epoch 1152\n",
            "before loss data in training 0.042625900357961655 0.05183167760616121\n",
            "after loss data in training 0.042625900357961655 0.05182369341080284\n",
            "yyy epoch 1153\n",
            "before loss data in training 0.04749192297458649 0.05182369341080284\n",
            "after loss data in training 0.04749192297458649 0.05181993971025153\n",
            "yyy epoch 1154\n",
            "before loss data in training 0.05668100342154503 0.05181993971025153\n",
            "after loss data in training 0.05668100342154503 0.05182414842342148\n",
            "yyy epoch 1155\n",
            "before loss data in training 0.04139703884720802 0.05182414842342148\n",
            "after loss data in training 0.04139703884720802 0.051815128432438594\n",
            "yyy epoch 1156\n",
            "before loss data in training 0.05563154071569443 0.051815128432438594\n",
            "after loss data in training 0.05563154071569443 0.05181842697373786\n",
            "yyy epoch 1157\n",
            "before loss data in training 0.04940391331911087 0.05181842697373786\n",
            "after loss data in training 0.04940391331911087 0.051816341901497254\n",
            "yyy epoch 1158\n",
            "before loss data in training 0.04622023552656174 0.051816341901497254\n",
            "after loss data in training 0.04622023552656174 0.05181151350945676\n",
            "yyy epoch 1159\n",
            "before loss data in training 0.05754364654421806 0.05181151350945676\n",
            "after loss data in training 0.05754364654421806 0.05181645500345224\n",
            "yyy epoch 1160\n",
            "before loss data in training 0.046067580580711365 0.05181645500345224\n",
            "after loss data in training 0.046067580580711365 0.05181150334589605\n",
            "yyy epoch 1161\n",
            "before loss data in training 0.046647388488054276 0.05181150334589605\n",
            "after loss data in training 0.046647388488054276 0.051807059185088954\n",
            "yyy epoch 1162\n",
            "before loss data in training 0.06391779333353043 0.051807059185088954\n",
            "after loss data in training 0.06391779333353043 0.05181747254205236\n",
            "############# Epoch 3: Training End     #############\n",
            "############# Epoch 3: Validation Start   #############\n",
            "############# Epoch 3: Validation End     #############\n",
            "before cal avg train loss 0.05181747254205236\n",
            "Epoch: 3 \tAvgerage Training Loss: 0.000045 \tAverage Validation Loss: 0.000182\n",
            "Validation loss decreased (0.000214 --> 0.000182).  Saving model ...\n",
            "############# Epoch 3  Done   #############\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trained_model = train_model(EPOCHS, train_data_loader, val_data_loader, model, optimizer, ckpt_path, best_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Testing"
      ],
      "metadata": {
        "id": "l0zGbonHRSkB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2eBBWG4g3hn",
        "outputId": "30b533be-e089-4e4a-a77a-c8697b3e177b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score = 0.52813273295201\n",
            "F1 Score (Micro) = 0.7244023873581346\n",
            "F1 Score (Macro) = 0.5576325850379967\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "val_preds = (np.array(val_outputs) > 0.5).astype(int)\n",
        "\n",
        "accuracy = metrics.accuracy_score(val_targets, val_preds)\n",
        "f1_score_micro = metrics.f1_score(val_targets, val_preds, average='micro')\n",
        "f1_score_macro = metrics.f1_score(val_targets, val_preds, average='macro')\n",
        "print(f\"Accuracy Score = {accuracy}\")\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sseZvlPqg3ho",
        "outputId": "2e872230-d6f0-4c20-e243-40f4b8729a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[24207     9]\n",
            "  [  352    83]]\n",
            "\n",
            " [[23630   169]\n",
            "  [  164   688]]\n",
            "\n",
            " [[23125    80]\n",
            "  [  710   736]]\n",
            "\n",
            " [[24247     5]\n",
            "  [  313    86]]\n",
            "\n",
            " [[23149   128]\n",
            "  [  592   782]]\n",
            "\n",
            " [[24352     5]\n",
            "  [  217    77]]\n",
            "\n",
            " [[23627    61]\n",
            "  [  195   768]]\n",
            "\n",
            " [[22702   371]\n",
            "  [  239  1339]]\n",
            "\n",
            " [[24049    50]\n",
            "  [  188   364]]\n",
            "\n",
            " [[24155    19]\n",
            "  [  204   273]]\n",
            "\n",
            " [[23829    72]\n",
            "  [   80   670]]\n",
            "\n",
            " [[24283     2]\n",
            "  [  308    58]]\n",
            "\n",
            " [[23951    46]\n",
            "  [  348   306]]\n",
            "\n",
            " [[23008   137]\n",
            "  [  201  1305]]\n",
            "\n",
            " [[22958   199]\n",
            "  [  376  1118]]\n",
            "\n",
            " [[24073    35]\n",
            "  [   28   515]]\n",
            "\n",
            " [[24007    56]\n",
            "  [  407   181]]\n",
            "\n",
            " [[24257    13]\n",
            "  [  331    50]]\n",
            "\n",
            " [[24200    43]\n",
            "  [  287   121]]\n",
            "\n",
            " [[24013    20]\n",
            "  [  543    75]]\n",
            "\n",
            " [[23303   124]\n",
            "  [   81  1143]]\n",
            "\n",
            " [[24120    36]\n",
            "  [  358   137]]\n",
            "\n",
            " [[24072    24]\n",
            "  [  405   150]]\n",
            "\n",
            " [[23881    68]\n",
            "  [  395   307]]\n",
            "\n",
            " [[24035    22]\n",
            "  [  525    69]]\n",
            "\n",
            " [[24137    10]\n",
            "  [  450    54]]\n",
            "\n",
            " [[24440    19]\n",
            "  [   80   112]]\n",
            "\n",
            " [[23264    88]\n",
            "  [  153  1146]]\n",
            "\n",
            " [[24229     2]\n",
            "  [  107   313]]\n",
            "\n",
            " [[21495   441]\n",
            "  [  250  2465]]\n",
            "\n",
            " [[23382    84]\n",
            "  [  127  1058]]\n",
            "\n",
            " [[24315     0]\n",
            "  [  336     0]]\n",
            "\n",
            " [[20735   475]\n",
            "  [  273  3168]]\n",
            "\n",
            " [[23085   117]\n",
            "  [  314  1135]]\n",
            "\n",
            " [[24403     8]\n",
            "  [  166    74]]\n",
            "\n",
            " [[24336     0]\n",
            "  [  315     0]]\n",
            "\n",
            " [[24231     0]\n",
            "  [  420     0]]\n",
            "\n",
            " [[24555     0]\n",
            "  [   96     0]]\n",
            "\n",
            " [[18300   513]\n",
            "  [ 3339  2499]]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.19      0.31       435\n",
            "           1       0.80      0.81      0.81       852\n",
            "           2       0.90      0.51      0.65      1446\n",
            "           3       0.95      0.22      0.35       399\n",
            "           4       0.86      0.57      0.68      1374\n",
            "           5       0.94      0.26      0.41       294\n",
            "           6       0.93      0.80      0.86       963\n",
            "           7       0.78      0.85      0.81      1578\n",
            "           8       0.88      0.66      0.75       552\n",
            "           9       0.93      0.57      0.71       477\n",
            "          10       0.90      0.89      0.90       750\n",
            "          11       0.97      0.16      0.27       366\n",
            "          12       0.87      0.47      0.61       654\n",
            "          13       0.90      0.87      0.89      1506\n",
            "          14       0.85      0.75      0.80      1494\n",
            "          15       0.94      0.95      0.94       543\n",
            "          16       0.76      0.31      0.44       588\n",
            "          17       0.79      0.13      0.23       381\n",
            "          18       0.74      0.30      0.42       408\n",
            "          19       0.79      0.12      0.21       618\n",
            "          20       0.90      0.93      0.92      1224\n",
            "          21       0.79      0.28      0.41       495\n",
            "          22       0.86      0.27      0.41       555\n",
            "          23       0.82      0.44      0.57       702\n",
            "          24       0.76      0.12      0.20       594\n",
            "          25       0.84      0.11      0.19       504\n",
            "          26       0.85      0.58      0.69       192\n",
            "          27       0.93      0.88      0.90      1299\n",
            "          28       0.99      0.75      0.85       420\n",
            "          29       0.85      0.91      0.88      2715\n",
            "          30       0.93      0.89      0.91      1185\n",
            "          31       0.00      0.00      0.00       336\n",
            "          32       0.87      0.92      0.89      3441\n",
            "          33       0.91      0.78      0.84      1449\n",
            "          34       0.90      0.31      0.46       240\n",
            "          35       0.00      0.00      0.00       315\n",
            "          36       0.00      0.00      0.00       420\n",
            "          37       0.00      0.00      0.00        96\n",
            "          38       0.83      0.43      0.56      5838\n",
            "\n",
            "   micro avg       0.87      0.62      0.72     37698\n",
            "   macro avg       0.78      0.49      0.56     37698\n",
            "weighted avg       0.84      0.62      0.68     37698\n",
            " samples avg       0.77      0.68      0.70     37698\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix as mcm, classification_report\n",
        "cm_labels = target_list\n",
        "cm = mcm(val_targets, val_preds)\n",
        "print(cm)\n",
        "\n",
        "print(classification_report(val_targets, val_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpcvKv-lg3hp"
      },
      "outputs": [],
      "source": [
        "#Testing\n",
        "test_targets=[]\n",
        "test_outputs=[]\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, data in tqdm.tqdm(enumerate(test_data_loader)):\n",
        "        \n",
        "        input_ids = data['input_ids'].to(device, dtype=torch.long)\n",
        "        attention_mask = data['attention_mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "        \n",
        "        output = model(input_ids, attention_mask, token_type_ids)\n",
        "        \n",
        "        test_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "        test_outputs.extend(torch.sigmoid(output).cpu().detach().numpy().tolist())   \n",
        "        \n",
        "test_preds = (np.array(test_outputs) > 0.5).astype(int)\n",
        "\n",
        "accuracy = metrics.accuracy_score(test_targets, test_preds)\n",
        "f1_score_micro = metrics.f1_score(test_targets, test_preds, average='micro')\n",
        "f1_score_macro = metrics.f1_score(test_targets, test_preds, average='macro')\n",
        "print(f\"Accuracy Score = {accuracy}\")\n",
        "print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "\n",
        "from sklearn.metrics import multilabel_confusion_matrix as mcm, classification_report\n",
        "cm_labels = target_list\n",
        "cm = mcm(test_targets, test_preds)\n",
        "print(cm)\n",
        "\n",
        "print(classification_report(test_targets, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "example = 'км ну потому что очень часто заказываю всегда товар хороший и быстрая доставка даже с другого города жалоб нету вам спасибо kaspi shopping'\n",
        "\n",
        "encodings = tokenizer.encode_plus(\n",
        "    example,\n",
        "    None,\n",
        "    add_special_tokens=True,\n",
        "    max_length=MAX_LEN,\n",
        "    padding='max_length',\n",
        "    return_token_type_ids=True,\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt'\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    input_ids = encodings['input_ids'].to(device, dtype=torch.long)\n",
        "    attention_mask = encodings['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = encodings['token_type_ids'].to(device, dtype=torch.long)\n",
        "    output = model(input_ids, attention_mask, token_type_ids)\n",
        "    final_output = torch.sigmoid(output).cpu().detach().numpy().tolist()\n",
        "    #print(train_df.columns[1:].to_list()[int(np.argmax(final_output, axis=1))])"
      ],
      "metadata": {
        "id": "w7gq2wDAEU58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_preds = (np.array(final_output) > 0.3).astype(int)\n",
        "\n",
        "import json\n",
        "labels_dict = json.load(open(labels_path, 'r', encoding='utf-8'))\n",
        "pred_target_df = pd.DataFrame({'Preds':final_preds[0], \"Target\": list(labels_dict.values())})\n",
        "list(pred_target_df[pred_target_df.Preds==1].iloc[:,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU7WFpqMEhO-",
        "outputId": "3e8fa64b-9622-4fc0-f5a7-17cb242462eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Позитив -> Магазин на Kaspi.kz -> Получение -> Нравится доставка...']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bert.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}